{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lukekolbe/AL-in-CreditScoring/blob/main/AL_CreditScoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVXsYRfkWNca"
      },
      "source": [
        "# Prep runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNnbXTVF-blo",
        "outputId": "f42ba8c7-79cf-40a0-eebb-6cdbea4800a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v9_ZaFTE7yD"
      },
      "source": [
        "#Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dpSw3SZlRhoI"
      },
      "outputs": [],
      "source": [
        "#!pip freeze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "appqqcKF-J7n",
        "outputId": "d0d632b3-f11c-4af5-bba0-1f4030900f2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: imbalanced-learn 0.9.0\n",
            "Uninstalling imbalanced-learn-0.9.0:\n",
            "  Successfully uninstalled imbalanced-learn-0.9.0\n",
            "Collecting imbalanced-learn\n",
            "  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (3.1.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from imbalanced-learn) (1.21.5)\n",
            "Installing collected packages: imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.9.0\n",
            "Requirement already satisfied: scikit-plot in /usr/local/lib/python3.7/dist-packages (0.3.7)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.0.2)\n",
            "Requirement already satisfied: joblib>=0.10 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.1.0)\n",
            "Requirement already satisfied: scipy>=0.9 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (1.4.1)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-plot) (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=1.4.0->scikit-plot) (3.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=1.4.0->scikit-plot) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->scikit-plot) (3.1.0)\n",
            "Collecting git+https://github.com/NUAA-AL/alipy.git\n",
            "  Cloning https://github.com/NUAA-AL/alipy.git to /tmp/pip-req-build-pv6d44co\n",
            "  Running command git clone -q https://github.com/NUAA-AL/alipy.git /tmp/pip-req-build-pv6d44co\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from alipy==1.2.5) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from alipy==1.2.5) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from alipy==1.2.5) (1.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from alipy==1.2.5) (3.2.2)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from alipy==1.2.5) (3.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->alipy==1.2.5) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->alipy==1.2.5) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->alipy==1.2.5) (1.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->alipy==1.2.5) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->alipy==1.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->alipy==1.2.5) (1.15.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->alipy==1.2.5) (4.11.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->alipy==1.2.5) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->alipy==1.2.5) (3.7.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->alipy==1.2.5) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->alipy==1.2.5) (1.1.0)\n",
            "Requirement already satisfied: hmeasure in /usr/local/lib/python3.7/dist-packages (0.1.6)\n",
            "Requirement already satisfied: numpy>=1.16.1 in /usr/local/lib/python3.7/dist-packages (from hmeasure) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.1 in /usr/local/lib/python3.7/dist-packages (from hmeasure) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from hmeasure) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.1->hmeasure) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.1->hmeasure) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "### FOR RUNNING ON COLAB:\n",
        "#update scikit-learn and imbalanced-learn to recent version\n",
        "#!pip install sklearn-pandas-transformers\n",
        "#!pip uninstall sklearn-pandas-transformers -y\n",
        "#!pip uninstall sklearn_transformers_pandas -y\n",
        "\n",
        "#!pip uninstall scikit-learn -y\n",
        "#pip install -U scikit-learn==0.24.2 #specific version, because skopt does not work with sklearn 1.0.0 \n",
        "#!pip install -U scikit-learn==1.0.1\n",
        "\n",
        "!pip uninstall imbalanced-learn -y\n",
        "!pip install -U imbalanced-learn\n",
        "\n",
        "#!pip install scikit-optimize\n",
        "!pip install scikit-plot\n",
        "\n",
        "!pip install git+https://github.com/NUAA-AL/alipy.git\n",
        "\n",
        "!pip install hmeasure\n",
        "#!pip install git+https://github.com/jundongl/scikit-feature\n",
        "#!pip install git+https://github.com/lukekolbe/scikit-feature\n",
        "\n",
        "#!pip install cvxpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1G7lnfTo-J7t"
      },
      "outputs": [],
      "source": [
        "############ LIBRARIES\n",
        "\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import multiprocessing\n",
        "import pickle\n",
        "import re\n",
        "import copy\n",
        "import gc\n",
        "import sys\n",
        "import json\n",
        "\n",
        "gc.enable()\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_columns', None)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import cm\n",
        "plt.style.use('default')\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from itertools import cycle\n",
        "\n",
        "import scikitplot as skplt\n",
        "\n",
        "import scipy.stats\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "#from sklearn.model_selection import StratifiedKFold  ##### what is this used for?\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report,confusion_matrix, roc_auc_score,roc_curve\n",
        "from sklearn.metrics import average_precision_score, brier_score_loss, f1_score, fbeta_score, precision_score, recall_score, balanced_accuracy_score\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# FOR CROSS VALIDATED HYPERPARAMETER TUNING\n",
        "# use imblearn pipeline instead of sklearn pipeline to skip AL sampling process in the prediction phase\n",
        "from imblearn.pipeline import Pipeline\n",
        "from imblearn import FunctionSampler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "#from skopt import BayesSearchCV #not compatible with latest sklearn, no option to use nested Parameter grid --> not applicable\n",
        "\n",
        "from hmeasure import h_score\n",
        "\n",
        "#from skfeature.function.statistical_based.CFS import cfs\n",
        "#from skfeature.function.information_theoretical_based.FCBF import fcbf\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#from alipy import ToolBox\n",
        "from alipy import query_strategy\n",
        "from alipy.index import IndexCollection\n",
        "from alipy import data_manipulate\n",
        "\n",
        "#from imblearn.under_sampling import RandomUnderSampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9clhtuie-zY6"
      },
      "outputs": [],
      "source": [
        "os.chdir('/gdrive/My Drive/ACTIVE LEARNING THESIS/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "4hTm9ni8CnhY",
        "outputId": "69a59373-5141-4290-ff06-9a75fd8dd858"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'from pathlib import Path\\nfor dir in [\\'model_indices\\', \\'model_cost\\', \\'model_results\\']:\\n  for dat in [\\'bene1\\',\\'bene1_nobins\\',\\'bene2\\',\\'gmsc\\',\\'australian\\',\\'german\\',\\'thomas\\',\\'hmeq\\',\\'lendingclub\\',\\'pakdd\\', \\'uk\\']:\\n    Path(f\"{dir}/{dat}\").mkdir(parents=True, exist_ok=True)'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "# RUN THIS ONCE TO CREATE DIRECTORIES WHERE RESULTS ARE SAVED\n",
        "'''from pathlib import Path\n",
        "for dir in ['model_indices', 'model_cost', 'model_results']:\n",
        "  for dat in ['bene1','bene1_nobins','bene2','gmsc','australian','german','thomas','hmeq','lendingclub','pakdd', 'uk']:\n",
        "    Path(f\"{dir}/{dat}\").mkdir(parents=True, exist_ok=True)'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "SwT100xf-J7u"
      },
      "outputs": [],
      "source": [
        "############ RANDOMNESS\n",
        "# seed function\n",
        "def seed_everything(seed):\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "# set seed\n",
        "seed = 30\n",
        "seed_everything(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP-kbRQHWTbB"
      },
      "source": [
        "# Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "a9qj2-Zl-J7v"
      },
      "outputs": [],
      "source": [
        "############ DATA IMPORT\n",
        "\n",
        "def data_loader(dataset):\n",
        "  \n",
        "  #df = pd.read_csv('//home//RDC//kolbeluk1//AL_THESIS//prepared_data//{}.csv'.format(dataset)) #Linux path\n",
        "  #df = pd.read_csv('C:\\\\Users\\\\kolbeluk1\\\\AL_THESIS\\\\prepared_data\\\\{}.csv'.format(dataset))\n",
        "  df = pd.read_csv('/gdrive/My Drive/ACTIVE LEARNING THESIS/prepared_data/{}.csv'.format(dataset))\n",
        "\n",
        "  # remove NA\n",
        "  df = df.dropna()\n",
        "  df.reset_index(drop = True, inplace = True)\n",
        "\n",
        "  #print(df)\n",
        "  # extract label\n",
        "  df['BAD'][df['BAD']=='BAD']  = 1\n",
        "  df['BAD'][df['BAD']=='GOOD'] = 0\n",
        "  df['BAD'] = df['BAD'].astype('int')\n",
        "\n",
        "\n",
        "  y_temp = df['BAD']\n",
        "  del df['BAD']\n",
        "\n",
        "  #one hot encoding\n",
        "  df = pd.get_dummies(df)\n",
        "\n",
        "  #print(df.describe())\n",
        "  #print(df.dtypes)\n",
        "\n",
        "  #transform to numpy array >> same location for df and X\n",
        "  X = df.to_numpy()\n",
        "  y = y_temp.to_numpy()\n",
        "\n",
        "  print(\"X type: \", type(X), \"X shape: \", X.shape,\"y shape: \", y.shape, \"y mean: \", np.mean(y))\n",
        "  print (id(X), id(df))\n",
        "\n",
        "  return X,y\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-kViyqDgoSA"
      },
      "source": [
        "# Define Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Ag_qAThO2orW"
      },
      "outputs": [],
      "source": [
        "def confusion_getter(y, accept_index, reject_index = None):\n",
        "\n",
        "  # this function computes misclassification cost that accrues from selecting training data over the course of the experiment\n",
        "  # each AL model will pick different instances to keep, so the accepted instances (i.e. credits) become increasingly different from another with each generation\n",
        "  # Keeping an instance means they are effectively classified as  a \"good risk (p=0), different misclassification cost for different algorightms is the result\n",
        "\n",
        "  if reject_index is not None: \n",
        "    index = np.concatenate((accept_index, reject_index)) # indices of all accepted and rejected cases used to subset y with\n",
        "\n",
        "    #create vector of ones that has the same length as the index vector of all accepted and rejected cases combined\n",
        "    #for all accepted cases, set this to zero, so that the cost performance can be computed via the confusion matrix\n",
        "    prediction_scores = np.concatenate((np.zeros(len(accept_index)), np.ones(len(reject_index))))\n",
        "\n",
        "  else:\n",
        "    # first generation of any model will always land here, the oracle will always land here (no cases are ever rejected in the oracle)\n",
        "    index = accept_index\n",
        "    prediction_scores = np.zeros(len(index))\n",
        "\n",
        "  #compute internal confusion matrix, which is different than the one for test prediction\n",
        "  confusion_mat = confusion_matrix(y[index], prediction_scores)\n",
        "  #print(\" tn, fp, fn, tp: \", confusion_mat.ravel())\n",
        "\n",
        "  #tn, fp, fn, tp = confusion_mat.ravel()\n",
        "\n",
        "  return confusion_mat\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gOY7hOQXzu2h"
      },
      "outputs": [],
      "source": [
        "def cost_function(y_true, prediction, indices_dict, cost_dict, cost_matrix, gen=0, oracle=False):\n",
        "\n",
        "\n",
        "  # ---------------- EXTERNAL cost (Test prediction)\n",
        "  test_idx = indices_dict['test_idx']\n",
        "  confusion_mat = confusion_matrix(y_true[test_idx], prediction)\n",
        "\n",
        "  ext_cost = np.sum(cost_matrix * confusion_mat)\n",
        "  cost_dict['external_cost'].loc[cost_dict['generation'] == gen] = ext_cost\n",
        "\n",
        "  # ---------------- external cost per loan (eCPL)\n",
        "\n",
        "  ext_cpl = ext_cost / len(test_idx)\n",
        "  cost_dict['external_cpl'].loc[cost_dict['generation'] == gen] = ext_cpl\n",
        "\n",
        "\n",
        "\n",
        "  #############\n",
        "\n",
        "  # ---------------- INITIAL COST\n",
        "  # misclassification cost when using just the initial sample\n",
        "  initial_conf_mat = confusion_getter(y_true, indices_dict['accept_idx'][0])\n",
        "  initial_cost = np.sum(initial_conf_mat * cost_matrix)\n",
        "\n",
        "  #############\n",
        "\n",
        "  # ---------------- GEN INTERNAL COST\n",
        "  # internal classification cost caused by this generation's samples\n",
        "\n",
        "  accept_idx = indices_dict['gen_accept_idx'][gen]\n",
        "\n",
        "  if not oracle:\n",
        "    reject_idx = indices_dict['gen_AL_reject_idx'][gen]\n",
        "    gen_int_conf_mat = confusion_getter(y_true, accept_idx, reject_idx)\n",
        "  else:\n",
        "    gen_int_conf_mat = confusion_getter(y_true, accept_idx)\n",
        "\n",
        "  gen_int_cost = np.sum(cost_matrix * gen_int_conf_mat)\n",
        "  cost_dict['gen_internal_cost'].loc[cost_dict['generation'] == gen] = gen_int_cost\n",
        "\n",
        "  # ---------------- GEN internal cost per loan (CPL)\n",
        "\n",
        "  gen_int_cpl = gen_int_cost / np.sum(gen_int_conf_mat)\n",
        "  cost_dict['gen_internal_cpl'].loc[cost_dict['generation'] == gen] = gen_int_cpl\n",
        "\n",
        "\n",
        "\n",
        "  # ---------------- TOTAL INTERNAL COST\n",
        "  # all accepted cases\n",
        "\n",
        "  accept_idx = indices_dict['accept_idx'][gen]\n",
        "\n",
        "  if not oracle:\n",
        "    reject_idx = indices_dict['total_reject_idx'][gen]\n",
        "    total_int_conf_mat = confusion_getter(y_true, accept_idx, reject_idx)\n",
        "  else:\n",
        "    total_int_conf_mat = confusion_getter(y_true, accept_idx)\n",
        "\n",
        "  total_int_cost = np.sum(cost_matrix * total_int_conf_mat)\n",
        "  cost_dict['total_internal_cost'].loc[cost_dict['generation'] == gen] = total_int_cost\n",
        "\n",
        "  # ---------------- TOTAL internal cost per loan (CPL)\n",
        "  total_int_cpl = total_int_cost / np.sum(total_int_conf_mat)\n",
        "  cost_dict['total_internal_cpl'].loc[cost_dict['generation'] == gen] = total_int_cpl\n",
        "\n",
        "\n",
        "\n",
        "  # ---------------- MODEL INTERNAL COST\n",
        "  # excluding initial sample\n",
        "  # compute initial cost, subtract from total (since all models will incur this cost)\n",
        "  #model_int_cost = total_int_cost - cost_dict['total_internal_cost'].loc[cost_dict['generation'] == 0] #subtract cost of initial gen\n",
        "  model_int_cost = total_int_cost - initial_cost\n",
        "  cost_dict['model_internal_cost'].loc[cost_dict['generation'] == gen] = model_int_cost\n",
        "\n",
        "  # ---------------- MODEL internal cost per loan (CPL)\n",
        "  #not including initial starter sample\n",
        "\n",
        "  model_int_cpl = model_int_cost / (np.sum(total_int_conf_mat) - np.sum(initial_conf_mat))\n",
        "  cost_dict['model_internal_cpl'].loc[cost_dict['generation'] == gen] = model_int_cpl\n",
        "\n",
        "\n",
        "\n",
        "  return cost_dict\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LeGu98mZkqt_"
      },
      "outputs": [],
      "source": [
        "# prediction analyser\n",
        "\n",
        "#define a function that does certain steps of analysis inside the loop, to be repeated at various stages of the loop\n",
        "def prediction_analyser(y, prediction_raw, prediction_thresholded, results=None, gen=0, severity_ratio=None):\n",
        "\n",
        "  ##########################################\n",
        "  # metrics based on probability (no threshold)\n",
        "  results['roc_auc'].loc[results['generation'] == gen] = roc_auc_score(y, prediction_raw[:,1]) #ROC AUC takes non-thresholded inputs\n",
        "  results['brier'].loc[results['generation'] == gen] = brier_score_loss(y, prediction_raw[:,1])\n",
        "  results['h-measure'].loc[results['generation'] == gen] = h_score(y, prediction_raw[:,1], severity_ratio = severity_ratio)\n",
        "  results['average_precision'].loc[results['generation'] == gen] = average_precision_score(y, prediction_raw[:,1])\n",
        "  \n",
        "  # metrics based on thresholded 0/1 predictions\n",
        "  results['f1'].loc[results['generation'] == gen] = f1_score(y, prediction_thresholded)\n",
        "  results['precision'].loc[results['generation'] == gen] = precision_score(y, prediction_thresholded)\n",
        "  results['recall'].loc[results['generation'] == gen] = recall_score(y, prediction_thresholded)\n",
        "  results['balanced_accuracy'].loc[results['generation'] == gen] = balanced_accuracy_score(y, prediction_thresholded)\n",
        "\n",
        "  # ecm : external confusion matrix (test prediction)\n",
        "  ecm = confusion_matrix(y_true=y, y_pred = prediction_thresholded) ##tn, fp, fn, tp\n",
        "\n",
        "  tn, fp, fn, tp = ecm.ravel()\n",
        "  \n",
        "  results['tn'].loc[results['generation'] == gen] = tn\n",
        "  results['fp'].loc[results['generation'] == gen] = fp\n",
        "  results['fn'].loc[results['generation'] == gen] = fn\n",
        "  results['tp'].loc[results['generation'] == gen] = tp\n",
        "  results['fnr'].loc[results['generation'] == gen]  = fn/(fn+tp)\n",
        "  results['fpr'].loc[results['generation'] == gen]  = fp/(tn+fp)\n",
        "  results['pcc'].loc[results['generation'] == gen]  = (tp+tn)/(fp+fn+tn+fp) # == accuracy\n",
        "\n",
        "\n",
        "\n",
        "  ##########################################\n",
        "  # CREATE PLOTS\n",
        "\n",
        "  '''skplt.metrics.plot_calibration_curve(y,[prediction_raw], ['LR'], title = 'Calibration plot of test prediction')\n",
        "  plt.show()'''\n",
        "\n",
        "  '''skplt.metrics.plot_roc_curve(y, prediction_raw, 'ROC curve of test prediction')\n",
        "  plt.show()'''\n",
        "  #np.set_printoptions(precision=2)\n",
        "\n",
        "  '''disp = ConfusionMatrixDisplay.from_predictions(\n",
        "        y,\n",
        "        prediction_thresholded,\n",
        "    )\n",
        "  disp.ax_.set_title('confusion matrix from test prediction')\n",
        "  plt.show()'''\n",
        "\n",
        "\n",
        "  return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "SSGEqP_YYCOu"
      },
      "outputs": [],
      "source": [
        "# get strategy\n",
        "\n",
        "def strategy_getter(X, y, strategy_name=\"QueryInstanceRandom\", train_idx = None, **kwargs):\n",
        "    \"\"\"Return the query strategy object from alipy package\"\"\"\n",
        "    \n",
        "    try:\n",
        "        exec(\"from alipy.query_strategy import \" + strategy_name)\n",
        "    except:\n",
        "        raise KeyError(\"Strategy \" + strategy_name + \" is not implemented in ALiPy.\")\n",
        "    strategy = None\n",
        "    \n",
        "    if train_idx is not None:\n",
        "      strategy = eval(strategy_name + \"(X=X, y=y, train_idx = train_idx, **kwargs)\")\n",
        "    else:\n",
        "      strategy = eval(strategy_name + \"(X=X, y=y, **kwargs)\")\n",
        "          \n",
        "    # print(strategy)\n",
        "    return strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KvGBeAPboDIF"
      },
      "outputs": [],
      "source": [
        "# loader function that unpacks tuning results and extracts parameters for different model steps\n",
        "\n",
        "def param_getter(tuned=False, dataset=None):\n",
        "  if tuned:\n",
        "    filename = f'{dataset}_tuned-params'\n",
        "\n",
        "    with open(filename, 'r') as f:\n",
        "      param_list = [json.loads(line) for line in f if line.startswith('{')]\n",
        "\n",
        "    param_dict = {}\n",
        "    for i in range(len(param_list)):\n",
        "      strategy_short = list(param_list[i].keys())[0]\n",
        "      param_dict[strategy_short] = param_list[i][list(param_list[i].keys())[0]]\n",
        "\n",
        "    #find cases where some models are not tuned, establish base parameters\n",
        "    for key, name in [('oracle', 'Oracle'),\n",
        "                      ('score', 'Score'),\n",
        "                      ('eer', 'QueryExpectedErrorReduction'), \n",
        "                      ('quire', 'QueryInstanceQUIRE'), \n",
        "                      ('bmdr','QueryInstanceBMDR'),\n",
        "                      ('spal', 'QueryInstanceSPAL')]:\n",
        "      try:\n",
        "        param_dict[key]\n",
        "      except KeyError:\n",
        "        param_dict[key] = {'AL':{'strategy_name':name}, 'CLF':{}}\n",
        "\n",
        "    # transfer tuned classifier to all models (clf is tuned separately, not in combination with AL model)\n",
        "    for key in param_dict.keys():\n",
        "      param_dict[key]['CLF']=param_dict['random']['CLF']\n",
        "\n",
        "    for key in ['bmdr', 'spal']:\n",
        "      param_dict[key]['AL']['rho'] = 10\n",
        "\n",
        "\n",
        "  else:\n",
        "    param_dict = {'oracle': {'AL':{'strategy_name': 'Oracle'}, 'CLF':{}}\n",
        "                  ,'score': {'AL':{'strategy_name': 'Score'}, 'CLF':{}}\n",
        "                  ,'random': {'AL':{'strategy_name': 'QueryInstanceRandom'}, 'CLF':{}}\n",
        "                  ,'unc': {'AL':{'strategy_name': 'QueryInstanceUncertainty'}, 'CLF':{}}\n",
        "                  ,'qbc': {'AL':{'strategy_name': 'QueryInstanceQBC'}, 'CLF':{}}\n",
        "                  ,'eer': {'AL':{'strategy_name': 'QueryExpectedErrorReduction'}, 'CLF':{}}\n",
        "                  ,'dw': {'AL':{'strategy_name': 'QueryInstanceDensityWeighted'}, 'CLF':{}}\n",
        "                  ,'density':{'AL':{'strategy_name': 'QueryInstanceGraphDensity'}, 'CLF':{}}\n",
        "                  ,'cors' :  {'AL':{'strategy_name': 'QueryInstanceCoresetGreedy'}, 'CLF':{}}\n",
        "                  ,'quire': {'AL':{'strategy_name': 'QueryInstanceQUIRE'}, 'CLF':{}}\n",
        "                  ,'bmdr': {'AL':{'strategy_name': 'QueryInstanceBMDR'}, 'CLF':{}}\n",
        "                  ,'spal': {'AL':{'strategy_name': 'QueryInstanceSPAL'}, 'CLF':{}}\n",
        "                  ,'lal': {'AL':{'strategy_name': 'QueryInstanceLAL'}, 'CLF':{}}\n",
        "                  }\n",
        "  \n",
        "  return param_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Iy2fFs8sGn6Q"
      },
      "outputs": [],
      "source": [
        "\n",
        "def loop_param_setter(X, max_gens = 10, total_rounds=5, init_ratio = 0.1, AL_acc_rate = 0.5, weights=True, cost_mat = None, dataset = None):\n",
        "\n",
        "  #max_gens = 10 per round (outer loop) this parameter sets the number of generations of applicants (inner loop)\n",
        "  #total_rounds = 5 #number of loop iterations, important for Stratified Kfold splitting of data\n",
        "  #init_ratio = 0.1\n",
        "\n",
        "  init_size, sample_size  = split_sample_calc(X, init_ratio, total_rounds, max_gens)\n",
        "\n",
        "  LOOP_params = {'rounds': total_rounds,\n",
        "            'max_gens': max_gens,\n",
        "            'init_sample': init_size, # initial labaled sampel; is defined at the beginning\n",
        "            'sample_size': sample_size, # number of applicants for each iteration\n",
        "            'score_acc_rate': 1-AL_acc_rate, # score_accept_rate: share OF APPLICANTS that are accepted by score (total share is dependent on threshold, too!)\n",
        "            'AL_acc_rate': AL_acc_rate, # AL_accept_rate: share OF APPLICANTS that are accepted by AL\n",
        "\n",
        "            'weights': weights, #sample weights\n",
        "            'w_factor': np.round((1-AL_acc_rate) / AL_acc_rate, 4),\n",
        "            \n",
        "            'cost_matrix': cost_mat, #[[0, 1],[5, 0]] #TN, FP, FN, TP\n",
        "            'do_thres': 'tuned', # do threshold; either 'tuned', \"mean\", \"roc\", \"youden\" or 'none'\n",
        "            'tuned_threshold': tuned_thresholds[dataset],\n",
        "            'dataset_name': dataset\n",
        "            }\n",
        "\n",
        "  return LOOP_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "s2hCEon2GFlS"
      },
      "outputs": [],
      "source": [
        "# function for calculating split and sample sizes\n",
        "# ensure that max_gens are possible with the data\n",
        "\n",
        "def split_sample_calc(X, init_size=0.1, total_rounds=5, max_gens=10):\n",
        "  #size of the initially labeled sample\n",
        "  #the term (len(X)-100) is used to leave some buffer so that the loop will be able to query a stratified sample in the last generation without issues due to a missing class.\n",
        "  #factor (1-1/total_rounds) is used because one of k folds is used as test set, therefore not being available for training.\n",
        "  available_data = int((len(X)-100) * (1-1/total_rounds))\n",
        "  print(\"available_data\", available_data)\n",
        "  s_initial = int(init_size*available_data)\n",
        "\n",
        "  #sample size to draw in each generation\n",
        "  #s_size = int(((len(X)-100) * (1-1/total_rounds) * (1-init_size) / (max_gens)))\n",
        "  s_size = int((available_data - s_initial) / max_gens)\n",
        "\n",
        "### these values are aribitrarily chosen! NEEDS REASONING\n",
        "  if s_initial > 500:\n",
        "    s_initial = 500\n",
        "\n",
        "  if s_size > s_initial:\n",
        "    s_size = s_initial\n",
        "  print(s_initial, s_size)\n",
        "  return s_initial, s_size"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plots"
      ],
      "metadata": {
        "id": "uB4IsIfEhApo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "5vaGz5qTG2DH"
      },
      "outputs": [],
      "source": [
        "def stats_plotter(stats):  # plot size\n",
        "  fig = plt.figure(figsize = (30, 45))\n",
        "\n",
        "  plt.subplot(7, 2, 1)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['roc_auc'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('ROC AUC score',        fontsize = 15)\n",
        "    #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "    plt.legend(prop = {'size': 12}, loc='upper center', bbox_to_anchor=(0.7, 0.4),\n",
        "          fancybox=True, shadow=True, ncol=3)\n",
        "\n",
        "  plt.subplot(7, 2, 2)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['h-measure'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('h-measure',        fontsize = 15)\n",
        "    #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "\n",
        "  # Brier-score\n",
        "  plt.subplot(7, 2, 3)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats)))) \n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['brier'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('Brier score',        fontsize = 15)\n",
        "    #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "\n",
        "  # f1-score\n",
        "  plt.subplot(7, 2, 4)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['average_precision'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('Average Precision', fontsize = 15)\n",
        "\n",
        "  plt.subplot(7, 2, 5)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['balanced_accuracy'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('Balanced Accuracy', fontsize = 15)\n",
        "\n",
        "  # h-measure\n",
        "  plt.subplot(7, 2, 6)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['f1'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('F1 score',        fontsize = 15)\n",
        "    #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "  #handles, labels = ax.get_legend_handles_labels()\n",
        "  #fig.legend(handles, labels, loc='upper center')\n",
        " \n",
        "\n",
        "  plt.subplot(7, 2, 7)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['pcc'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('Precentage of correctly classified', fontsize = 15)\n",
        " \n",
        "  '''plt.subplot(7, 2, 8)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['recall'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('Recall score', fontsize = 15)'''\n",
        "\n",
        "  # FNR\n",
        "  plt.subplot(7, 2, 9)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['fnr'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('False Negative Rate (FNR)\\nof test-prediction',        fontsize = 15)\n",
        "    #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "  #handles, labels = ax.get_legend_handles_labels()\n",
        "  #fig.legend(handles, labels, loc='upper center')\n",
        "\n",
        "  # FPR\n",
        "  plt.subplot(7, 2, 10)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['fpr'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('False Positve Rate (FPR)\\nof test-prediction', fontsize = 15)\n",
        "    #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "  #handles, labels = ax.get_legend_handles_labels()\n",
        "  #fig.legend(handles, labels, loc='upper center')\n",
        "\n",
        "  # Bad Ratio Accepted Instances\n",
        "  plt.subplot(7, 2, 11)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    c=next(color)\n",
        "    l=next(lines)\n",
        "    # absoute values\n",
        "    plt.plot(stats[\"{}\".format(key)]['average']['bad_ratio_accepts'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "    #plt.xlabel('Generation', fontsize = 15)\n",
        "    plt.ylabel('Bad ratio of accepted cases', fontsize = 15)\n",
        "    #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "    #plt.legend(prop = {'size': 12}, loc='upper center', bbox_to_anchor=(0.6, 0.4),\n",
        "    #      fancybox=True, shadow=True, ncol=3)\n",
        "    \n",
        "\n",
        "  # Bad Ratio Rejected Instances\n",
        "  plt.subplot(7, 2, 12)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    if key not in [\"oracle\"]:\n",
        "      c=next(color)\n",
        "      l=next(lines)\n",
        "      # absoute values\n",
        "      plt.plot(stats[\"{}\".format(key)]['average']['bad_ratio_rejects'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "      #plt.xlabel('Generation', fontsize = 15)\n",
        "      plt.ylabel('Bad ratio of rejected cases',        fontsize = 15)\n",
        "      #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "      #plt.legend(prop = {'size': 12}, loc='upper center', bbox_to_anchor=(0.6, 0.4),\n",
        "      #      fancybox=True, shadow=True, ncol=3)\n",
        "    else:\n",
        "      c=next(color) #skip one color because we don't need a curve for oracle and want to maintain color scheme\n",
        "      l=next(lines)\n",
        "\n",
        "\n",
        "  # Bad Ratio SCORE Instances\n",
        "  plt.subplot(7, 2, 13)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    if key not in [\"oracle\"]:\n",
        "      c=next(color)\n",
        "      l=next(lines)\n",
        "      # absoute values\n",
        "      plt.plot(stats[\"{}\".format(key)]['average']['bad_ratio_AL_selects'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "      #plt.xlabel('Generation', fontsize = 15)\n",
        "      plt.ylabel('Bad ratio of AL-selected cases',        fontsize = 15)\n",
        "      #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "      #plt.legend(prop = {'size': 12}, loc='upper center', bbox_to_anchor=(0.6, 0.4),\n",
        "      #      fancybox=True, shadow=True, ncol=3)\n",
        "    else:\n",
        "      c=next(color) #skip one color because we don't need a curve for oracle and want to maintain color scheme\n",
        "      l=next(lines)\n",
        "\n",
        "  # Bad Ratio SCORE Instances\n",
        "  plt.subplot(7, 2, 14)\n",
        "  color=iter(cm.rainbow(np.linspace(0,1,len(stats))))\n",
        "  lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "  for key in stats.keys():\n",
        "    if key not in [\"oracle\"]:\n",
        "      c=next(color)\n",
        "      l=next(lines)\n",
        "      # absoute values\n",
        "      plt.plot(stats[\"{}\".format(key)]['average']['bad_ratio_score_accepts'], c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "      #plt.xlabel('Generation', fontsize = 15)\n",
        "      plt.ylabel('Bad ratio of score-selected cases',        fontsize = 15)\n",
        "      #plt.legend(prop = {'size': 12}, bbox_to_anchor=(1, 1), loc='upper left')\n",
        "      #plt.legend(prop = {'size': 12}, loc='upper center', bbox_to_anchor=(0.6, 0.4),\n",
        "      #      fancybox=True, shadow=True, ncol=3)\n",
        "    else:\n",
        "      c=next(color) #skip one color because we don't need a curve for oracle and want to maintain color scheme\n",
        "      l=next(lines)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BHd-N3ERZj_1"
      },
      "outputs": [],
      "source": [
        "\n",
        "############ COST PLOTS\n",
        "def cost_plotter(cost, cost_metrics):  # plot size\n",
        "\n",
        "  if len(cost_metrics)%2 > 0:\n",
        "    plotrows = (len(cost_metrics) + 1) / 2\n",
        "\n",
        "  else:\n",
        "    plotrows = (len(cost_metrics)) / 2\n",
        "\n",
        "  fig = plt.figure(figsize = (20, 20))\n",
        "\n",
        "\n",
        "  for q in cost_metrics:\n",
        "\n",
        "    # External cost\n",
        "    plt.subplot(plotrows, 2, (cost_metrics.index(q) + 1))\n",
        "    color=iter(cm.rainbow(np.linspace(0,1,len(cost))))\n",
        "    lines = cycle([\"-\",\"--\",\"-.\",\":\"])\n",
        "    \n",
        "    for key in cost.keys():\n",
        "      #if key not in [\"score\"]:\n",
        "        c=next(color)\n",
        "        l=next(lines)\n",
        "        # absoute values\n",
        "        plt.plot((cost[\"{}\".format(key)]['average'][q]), c=c, ls=l, label = AL_models[key][0],  linewidth = 2)\n",
        "        plt.xlabel('Generation', fontsize = 15)\n",
        "        plt.ylabel(f'{q}', fontsize = 15)\n",
        "\n",
        "        if cost_metrics.index(q) < 1:\n",
        "          plt.legend(prop = {'size': 10}, loc='upper right', bbox_to_anchor=(0.9, 0.9),\n",
        "          fancybox=True, shadow=True, ncol=3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLH8eLKXAYgs"
      },
      "source": [
        "# Define AL Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VKdLPoRr-J7z"
      },
      "outputs": [],
      "source": [
        "def AL_loop(data, y, kf_indices, key,  AL_models, rounds = 5, classifier=None, iteration=0, max_gens=10, sample_size=500, init_sample=1000, score_acc_rate=0.5, AL_acc_rate=0.5, seed=1, weights=False, w_factor=1, do_thres='tuned', tuned_threshold=None, AL_params=None, CLF_params=None, cost_matrix=None, dataset_name = None): #test_ratio = 0.3\n",
        "  #check validity and completeness of parameters\n",
        "  print('\\n\\n\\n---------------------------------------------------')\n",
        "  local_parameters = locals()\n",
        "  print('PARAMETERS:\\n')\n",
        "  for l_p in local_parameters:\n",
        "    if l_p not in ['data', 'y', 'kf_indices','AL_models', 'classifier']:\n",
        "      print(f'{l_p}: ', local_parameters[l_p])\n",
        "\n",
        "  ##########################################\n",
        "  # MISC LOOP SETUP\n",
        "  ##########################################\n",
        "  seed_everything(seed)\n",
        "  \n",
        "  ##############\n",
        "  # use train-test indices via StratifiedKFold object passed as argument\n",
        "  train_idx = kf_indices[iteration][0]\n",
        "  test_idx = kf_indices[iteration][1]\n",
        "\n",
        "  print('\\nsanity check of train-test splits:',np.shape(train_idx), np.shape(test_idx))\n",
        "\n",
        "  #draw a stratified sample as initial labeled data\n",
        "  sss = StratifiedShuffleSplit(n_splits=1, test_size=init_sample, random_state=seed)\n",
        "  for unlabel, label in sss.split(X=data[train_idx], y=y[train_idx]):\n",
        "    unlabel_idx, accept_idx = np.asarray(unlabel), np.asarray(label)    \n",
        "\n",
        "  unlabel_idx = IndexCollection(unlabel_idx)\n",
        "  accept_idx = IndexCollection(accept_idx)\n",
        "\n",
        "  #### APPLY ROBUST SCALER\n",
        "  # scale once for each round (not each generation within the round) using the initial labeled set\n",
        "  # in some cases this is a very small set to scale on...\n",
        "  scaler = RobustScaler(with_centering=True, with_scaling=True)\n",
        "  scaler.fit(data[accept_idx.index,:])\n",
        "  X_t = scaler.transform(np.array(data)) #scaled version of full dataset\n",
        "  \n",
        "\n",
        "  # calculate the actual number of generations that can be drawn from the data, in case max_gens * sample_size exceeds available training size\n",
        "  # num_gens = int((np.min(len(train_idx), len(test_idx)) - init_sample) / sample_size)\n",
        "  num_gens = int((len(train_idx) - init_sample) / sample_size)\n",
        "  num_gens = np.min([num_gens, max_gens])\n",
        "  print(\"strategy {}, round {}: running loop for {} generations\".format(AL_models[key][0], iteration, num_gens))\n",
        "\n",
        "  #initialize results dataframe\n",
        "  stats = pd.DataFrame({'generation':    range(num_gens+1),\n",
        "                    'n_accepts':         np.nan, #total number of accepts\n",
        "                    'n_rejects':         np.nan, #total number of rejects               \n",
        "                    'bad_ratio_accepts': np.nan,\n",
        "                    'bad_ratio_rejects': np.nan,\n",
        "                    'n_score_accepts':   np.nan,\n",
        "                    'n_AL_selects':      np.nan,\n",
        "                    'bad_ratio_score_accepts': np.nan,\n",
        "                    'bad_ratio_AL_selects': np.nan,\n",
        "\n",
        "                    'roc_auc':           np.nan,\n",
        "                    'brier':             np.nan,\n",
        "                    'h-measure':         np.nan,\n",
        "                    'average_precision': np.nan,\n",
        "                    'balanced_accuracy': np.nan,\n",
        "                    'pcc':               np.nan, # same as accuracy\n",
        "\n",
        "                    'precision':         np.nan,\n",
        "                    'recall':            np.nan,\n",
        "                    'f1':                np.nan, \n",
        "                    'fpr':               np.nan,\n",
        "                    'fnr':               np.nan,\n",
        "\n",
        "                    'tn':                np.nan,\n",
        "                    'fp':                np.nan,\n",
        "                    'fn':                np.nan,\n",
        "                    'tp':                np.nan\n",
        "                    #'time':             np.nan\n",
        "                       })\n",
        "\n",
        "  cost = pd.DataFrame({\n",
        "          'generation': range(num_gens+1),\n",
        "          #'initial_cost': np.nan,\n",
        "          #'initial_cpl': np.nan,\n",
        "          'gen_internal_cost': np.nan,\n",
        "          'gen_internal_cpl': np.nan,\n",
        "\n",
        "          'total_internal_cost': np.nan,\n",
        "          'total_internal_cpl':np.nan, #cost per loan\n",
        "\n",
        "          'model_internal_cost': np.nan,\n",
        "          'model_internal_cpl': np.nan,\n",
        "\n",
        "          'external_cost': np.nan,\n",
        "          'external_cpl': np.nan\n",
        "          }\n",
        "          )\n",
        "\n",
        "  # store indices that were drawn, needed mostly for debugging and plausibility checking\n",
        "  indices = {'test_idx': [],\n",
        "              'train_idx': [],\n",
        "              #'initial_idx': [],\n",
        "             \n",
        "              'applicants_idx': {},     # in each gen, new applicants are randomly drawn\n",
        "                         \n",
        "              'gen_score_accept_idx':  {},    # top n accepted cases based on prediction score\n",
        "              'gen_score_reject_idx':  {},    # rejected cases based on prediction score\n",
        "              'gen_AL_select_idx':  {},   # this gen's AL selection\n",
        "              'gen_AL_reject_idx':  {},   # not picked by either Score or \n",
        "              'gen_accept_idx': {},\n",
        "              \n",
        "              'total_score_accept_idx': {},\n",
        "              'total_AL_select_idx': {},  # all instances that were accepted via AL over all rounds\n",
        "              'total_reject_idx': {},    # \n",
        "              'accept_idx':  {},\n",
        "              }\n",
        "  \n",
        "  \n",
        "  indices['test_idx'] = test_idx\n",
        "  indices['train_idx'] = train_idx\n",
        "  #indices['initial_idx'] = accept_idx.index\n",
        "  indices['accept_idx'][0] = accept_idx.index\n",
        "  indices['gen_accept_idx'][0] = accept_idx.index\n",
        "\n",
        "  \n",
        "\n",
        "  ##########################################\n",
        "  # SET INITIAL PERFORMANCE POINT WITH INITIAL SAMPLE\n",
        "  ##########################################\n",
        "  clf = clone(classifier, safe=True) #clone in order to receive fresh classifier\n",
        "  clf.set_params(**CLF_params)\n",
        "  print('classifier: ',str(clf))\n",
        "\n",
        "  clf.fit(X=X_t[accept_idx.index,:], y=y[accept_idx.index]) ### first fit as basline, no AL selection was performed yet\n",
        "  \n",
        "  prediction = clf.predict_proba(X_t[test_idx, :])\n",
        "  print('mean of initial TEST prediction:', np.mean(prediction[:,1]))\n",
        "  print('y mean of TEST DATA:', np.mean(y[test_idx]))\n",
        "  print('y mean of INITIALLY ACCEPTED DATA:', np.mean(y[accept_idx]))\n",
        "\n",
        "  '''sns.set_style('whitegrid')\n",
        "  sns.kdeplot(prediction[:,1]).set_title('Distribution of initial TEST prediction')\n",
        "  plt.show()'''\n",
        "\n",
        "  skplt.metrics.plot_calibration_curve(y[test_idx],[prediction], ['LR'], title = 'Calibration plot of test prediction')\n",
        "  plt.show()\n",
        "\n",
        "  # compute threshold candidates\n",
        "  # roc curve is computed just once, in generation 0, so that thresholds are fixed for the entirety of the run\n",
        "  fpr, tpr, thresholds = roc_curve(y[test_idx],prediction[:,1],drop_intermediate=True)\n",
        "  threshold_dict = {'mean': np.round(np.mean(y[accept_idx.index]),6),\n",
        "                'roc': np.round(thresholds[np.argmin(np.abs(fpr+tpr-1))],6),\n",
        "                'youden': np.round(thresholds[np.argmax((tpr-fpr))],6),\n",
        "                'none': 0.5,\n",
        "                'tuned': np.round(tuned_threshold,6)\n",
        "                }\n",
        "\n",
        "  print('thresholds: ',threshold_dict)\n",
        "\n",
        "\n",
        "  ##########################################\n",
        "  # Calculate performance metrics for baseline, using (thresholded) prediction\n",
        "\n",
        "  print('threshold used: ',threshold_dict[do_thres])\n",
        "  pred_thres = (prediction[:,1] >= threshold_dict[do_thres]).astype(int)\n",
        "  print(\"mean of thresholded prediction: \", np.mean(pred_thres))\n",
        "  print(\"median of thresholded prediction: \", np.median(pred_thres))\n",
        "  print('percentage kept: ',1-np.mean(pred_thres))\n",
        "\n",
        "\n",
        "  # INITIAL COST\n",
        "  # Establish cost-matrix\n",
        "\n",
        "  if cost_matrix == None:\n",
        "    y_mean = np.mean(y)\n",
        "    fn_cost = (1-y_mean)/y_mean\n",
        "    cost_matrix = np.array([[0, 1],[fn_cost, 0]]) # [tn, fp], [fn, tp]\n",
        "  else:\n",
        "    fn_cost = cost_matrix[1][0]\n",
        "\n",
        "  severity = 1/fn_cost # used for h-measure\n",
        "  print('cost matrix [[tn, fp], [fn, tp]]: ', cost_matrix)\n",
        "  \n",
        "  # run cost function for initial data, gen = 0, set oracle = True because no rejects exist yet, same as when running the oracle model\n",
        "  cost = cost_function(y, pred_thres, indices, cost, cost_matrix, oracle=True)\n",
        "  #print('initial cost: ', cost)\n",
        "\n",
        "  # run the prediction analyser function for further performance metrices\n",
        "  stats = prediction_analyser(y[test_idx], prediction, pred_thres, stats, gen=0, severity_ratio=severity)\n",
        "  # save indices and bad ratios of initial round\n",
        "  # this is just the baseline; since initial accepts are randomly selected, the ratios between accepted and rejected cases will be identical here\n",
        "  indices['accept_idx'][0] = accept_idx.index  #store initially accepted indices\n",
        "  stats['bad_ratio_accepts'].loc[stats['generation'] == 0] = np.round(np.mean(y[accept_idx.index]), 4)\n",
        "  stats['bad_ratio_rejects'].loc[stats['generation'] == 0] = np.round(np.mean(y[unlabel_idx.index]), 4)\n",
        "\n",
        "\n",
        "\n",
        "  ########################################################################\n",
        "  # LOAD AL Strategies\n",
        "  ########################################################################\n",
        "\n",
        "  #name = AL_models[key][1]\n",
        "  if key in [\"random\", \"unc\", \"qbc\", \"eer\", \"dw\"]:\n",
        "    strategy = strategy_getter(X_t, y, **AL_params) #, **, al_hyperparameters\n",
        "\n",
        "  elif key == \"lal\":\n",
        "    reg_est = AL_params['reg_est']\n",
        "    reg_depth = AL_params['reg_depth']\n",
        "    reg_feat = AL_params['reg_feat']\n",
        "    cls_est = AL_params['cls_est']\n",
        "\n",
        "    strategy = strategy_getter(X_t, y, strategy_name= 'QueryInstanceLAL', mode='LAL_iterative', train_slt=False, cls_est = cls_est) #**param_dict\n",
        "    strategy.download_data()\n",
        "    strategy.train_selector_from_file(reg_est=reg_est, reg_depth=reg_depth, feat=reg_feat)\n",
        "  \n",
        "  elif key in [\"cors\", \"density\", \"quire\", \"bmdr\", \"spal\"]:\n",
        "    # these AL models need actual training indices and must hence be initialized in every generation of the loop, \n",
        "    # using the newest accepts and rejects from the current generation\n",
        "    pass\n",
        "\n",
        "\n",
        "  # timer\n",
        "  loop_start = time.time()\n",
        "  \n",
        "  ########################################################################\n",
        "  # ACCEPTANCE LOOP with AL\n",
        "  ########################################################################\n",
        "  for g in range(num_gens):\n",
        "    print('---------------------------------------------------')\n",
        "    print(f'dataset {dataset_name}, strategy {key}, AL ratio {AL_acc_rate}, cost matrix {cost_matrix}: starting round {iteration}, gen{g+1}')\n",
        "\n",
        "    seed_everything(seed)\n",
        "    \n",
        "    ##########################################\n",
        "    # DRAW APPLICANTS IN EACH GENERATION\n",
        "    # we draw a stratified sample from the unlabeled indices to avoid samples without minority class instances\n",
        "    # break condition prior to meeting num_gens when not enough unlabeled instances to draw a full sample containing both classes\n",
        "    # this is done because StratifiedShuffleSplit works with float test sizes\n",
        "    ##########################################\n",
        "\n",
        "    if sample_size < len(unlabel_idx.index) and 1 > np.mean(y[unlabel_idx.index]) > 0:\n",
        "      sss = StratifiedShuffleSplit(n_splits=1, test_size=sample_size, random_state=seed) # THESE ARE POSITIONS, NOT INDICES\n",
        "      \n",
        "      for dump_idx, use_idx in sss.split(X=np.zeros(len(unlabel_idx.index)), y=y[unlabel_idx.index]): #just splitting on y, hence X provided as zeros\n",
        "        sss_applicants_pos = np.asarray(use_idx) #Yields positions of applicants in UNLABEL INDEX\n",
        "        #sss_unlabel_pos, sss_applicants_pos = np.asarray(dump_idx), np.asarray(use_idx)\n",
        "      \n",
        "      applicants_idx = np.array(unlabel_idx.index)[sss_applicants_pos]\n",
        "      applicants_idx = IndexCollection(applicants_idx)\n",
        "      unlabel_idx.difference_update(applicants_idx) # delete applicants_idx from unlabel_idx\n",
        "    \n",
        "    else:\n",
        "      break #break loop if not enough instances are left or only instances of one class remain\n",
        "    \n",
        "    print(\"len applicants_idx:\", len(applicants_idx.index))\n",
        "\n",
        "    #add applicants to indices dictionary\n",
        "    #should be the same for every strategy & round >> just a plausibility check\n",
        "    indices['applicants_idx'][g+1] = applicants_idx.index \n",
        "\n",
        "    ##########################################\n",
        "    # SELECT AND SCORE APPLICANTS\n",
        "    ##########################################\n",
        "\n",
        "    ## PREDICTION ON APPLICANTS, NOT NEEDED FOR ORACLE EXCEPT FOR KDEPLOT\n",
        "    applicants_pred = clf.predict_proba(X_t[applicants_idx.index, :])\n",
        "    #print('applicants_pred', np.round(applicants_pred[:15], 4))\n",
        "    print('mean applicant pred col 1 (bad)', np.mean(applicants_pred[:,1]))\n",
        "    \n",
        "    # plot distribution of predictions\n",
        "    #sns.set_style('whitegrid')\n",
        "    '''sns.kdeplot(applicants_pred[:,1]).set_title('Distribution of applicant prediction')\n",
        "    plt.show()'''\n",
        "    \n",
        "    if key == 'oracle':\n",
        "      # accept all applicants for \"oracle\" model\n",
        "      accept_idx.update(applicants_idx)\n",
        "      indices['gen_accept_idx'][g+1] = applicants_idx.index\n",
        "\n",
        "      #save bad ratios of oracle\n",
        "      stats['bad_ratio_accepts'].loc[stats['generation'] == g+1] = np.round(np.mean(y[accept_idx.index]), 4)\n",
        "      #stats['bad_ratio_rejects'].loc[stats['generation'] == g+1] = np.round(np.mean(y[unlabel_idx.index]), 4) # not very accurate, since there are no rejects technically\n",
        "    \n",
        "\n",
        "    else:\n",
        "      # apply threshold\n",
        "      applicants_pred_thres = (applicants_pred[:,1] >= threshold_dict[do_thres]).astype(int) #set any prediction reaching or exceeding threshold to 1\n",
        "      print('mean of thresholded prediction: ', np.mean(applicants_pred_thres))\n",
        "      print('percentage of applicants kept (cutoff based on threshold): ',1-np.mean(applicants_pred_thres))\n",
        "      print('threshold used: ',threshold_dict[do_thres])\n",
        "\n",
        "      # apply cutoff – keep the good ones (remaining zeros in prediction after applying threshold)\n",
        "      cutoff = np.round(1-np.mean(applicants_pred_thres), 3) #percentage to keep\n",
        "      score_accept_cutoff = np.round(cutoff * score_acc_rate,3) # e.g. cutoff at 0.6, accept rate = 0.8 >> cutoff at 0.6 * 0.8 = keep the top 48%\n",
        "      score_accept_pos = np.where(applicants_pred[:,1] < np.quantile(applicants_pred[:, 1], score_accept_cutoff)) # gives positions (indices of index vector) of best n%\n",
        "      \n",
        "      #save indices\n",
        "      gen_score_accept_idx = np.asarray(applicants_idx.index)[score_accept_pos] # get actual indices based on positions\n",
        "      gen_score_accept_idx = IndexCollection(gen_score_accept_idx)\n",
        "      accept_idx.update(gen_score_accept_idx) # add top ranking applicants to overall accepted cases\n",
        "      print(\"number of score-accepted applicants: \",len(gen_score_accept_idx))\n",
        "\n",
        "      gen_score_reject_idx = IndexCollection(np.asarray(applicants_idx.index))\n",
        "      gen_score_reject_idx.difference_update(gen_score_accept_idx)\n",
        "      \n",
        "      print(\"length of gen_score_reject_idx:\", len(gen_score_reject_idx.index))\n",
        "      \n",
        "    print('model key: ', key)\n",
        "\n",
        "\n",
        "    ####################################################################################\n",
        "    # 2nd STEP: PERFORM AL-SELECTION OF APPLICANTS\n",
        "    ####################################################################################\n",
        "\n",
        "    if key != 'oracle': #, 'score'\n",
        "\n",
        "      ##########################################\n",
        "      # SETUP\n",
        "      ##########################################\n",
        "\n",
        "      # calculate AL-quota with cutoff\n",
        "      AL_accept_cutoff = np.round(cutoff * AL_acc_rate,3)\n",
        "      n_instances = int(sample_size*AL_accept_cutoff) #number of instances to be sampled via AL\n",
        "      print('AL_accept_cutoff', AL_accept_cutoff)\n",
        "      print('no. of AL-accepted instances:', n_instances)\n",
        "\n",
        "\n",
        "      if key in [\"cors\", \"density\", \"quire\"]:\n",
        "        al_idx=np.concatenate((np.asarray(accept_idx.index), np.asarray(gen_score_reject_idx.index)))\n",
        "        strategy = strategy_getter(X_t, y, train_idx=al_idx, **AL_params)\n",
        "\n",
        "      ##########################################\n",
        "      # MAKE SELECTION\n",
        "      ##########################################\n",
        "\n",
        "      if key == 'score':\n",
        "        # special case: instead of selecting all applicants at once in the score model, it is split up the same way as with the AL models\n",
        "        # this makes it easier to compare the performance (BAD ratios) of the score selections between models\n",
        "        print('AL_accept_cutoff', AL_accept_cutoff, 'score_accept_cutoff', score_accept_cutoff)\n",
        "        accept_pos = np.where( #give position\n",
        "            np.logical_and( #for which both conditions apply:\n",
        "                np.quantile(applicants_pred[:, 1], score_accept_cutoff) <= applicants_pred[:,1], # Prediction score (instance==BAD) is higher than those accepted as score selection\n",
        "                applicants_pred[:,1] <= np.quantile(applicants_pred[:, 1], cutoff)))[0]     #but not higher (==worse) than what is to be accepted by score+AL   \n",
        "        # this complicated line above selects the n next-best instances from the applicant sample\n",
        "        # so in case of the score-only model the quota reserved for AL selections is also filled with score-based selections...\n",
        "        #anything ranking between the \"score-accept-rate\"-th percentile and the \"al-accept-rate\"-th percentile is chosen in this step – score model only\n",
        "        \n",
        "        gen_AL_select_idx = np.array(applicants_idx.index)[accept_pos]\n",
        "        gen_AL_select_idx = IndexCollection(gen_AL_select_idx)\n",
        "      \n",
        "      elif key == 'quire':\n",
        "        print(strategy)\n",
        "        ## quire has no batch mode, instances are selected one by one >> so a loop is needed\n",
        "        gen_AL_select_idx = np.zeros(shape=n_instances, dtype=int) #empty array to be filled with selected indices\n",
        "        labeled_temp = IndexCollection(accept_idx.index)\n",
        "        unlabeled_temp = IndexCollection(gen_score_reject_idx.index)\n",
        "\n",
        "        print('starting QUIRE selection')\n",
        "        for n in range(n_instances):\n",
        "          selection = strategy.select(label_index=labeled_temp.index, unlabel_index=unlabeled_temp.index)[0] # returns a list of len one, hence the [0]\n",
        "          #print(np.shape(selection))\n",
        "          #print(selection)\n",
        "\n",
        "          labeled_temp.update(selection)\n",
        "          unlabeled_temp.difference_update(selection)\n",
        "\n",
        "          gen_AL_select_idx[n] = selection\n",
        "          if n%10 == 0:\n",
        "            print(f\"QUIRE selected {n} cases:\", gen_AL_select_idx)\n",
        "      \n",
        "        gen_AL_select_idx = IndexCollection(gen_AL_select_idx)\n",
        "\n",
        "\n",
        "      elif key in ['bmdr', 'spal']:\n",
        "        # select instances for this gen and AL Model\n",
        "        '''SPECIAL CASE: similar to QUIRE and some other techniques, BMDR and SPAL build distance matrices upon which their selections are calculated.\n",
        "        But unlike QUIRE etc., BMDR and SPAL do not take a training index as parameter, which would be used to subset the data and reduce the size of the distance matrix.\n",
        "        For large datasets, this means that the distance matrix for the rbf kernel (using the full dataset) would become very large, requiring hundreds of GB of RAM.\n",
        "        This workaround reduces the input data to only a relevant subset, builds proxy-indices (indices of indices), makes a selection, \n",
        "        and transates the selected indices of the subset to indices that work for the whole dataset.'''\n",
        "\n",
        "        al_idx=np.concatenate((np.asarray(accept_idx.index), np.asarray(gen_score_reject_idx.index)))\n",
        "        #print(\"al_idx\", al_idx)\n",
        "        #print(',X_t[gen_score_reject_idx,:10]',X_t[gen_score_reject_idx,:10])\n",
        "        strategy = strategy_getter(X_t[al_idx], y[al_idx], **AL_params)\n",
        "        \n",
        "        temp_accept_idx = list(range(len(accept_idx.index)))\n",
        "        temp_unlabel_idx = [x + len(temp_accept_idx) for x in list(range(len(gen_score_reject_idx.index)))]\n",
        "        #print('temp_unlabel_idx', temp_unlabel_idx)\n",
        "        #print('gen_score_reject_idx',gen_score_reject_idx)\n",
        "        #print('al_idx[temp_unlabel_idx]', al_idx[temp_unlabel_idx])\n",
        "\n",
        "        temp_gen_AL_select_idx = strategy.select(label_index=temp_accept_idx, unlabel_index=temp_unlabel_idx, batch_size=n_instances, model=clf, qp_solver = 'OSQP') \n",
        "        #print('temp_gen_AL_select_idx',temp_gen_AL_select_idx)\n",
        "        print('al_idx[temp_gen_AL_select_idx]',al_idx[temp_gen_AL_select_idx])\n",
        "        #print('X_t[al_idx][temp_gen_AL_select_idx]', X_t[al_idx][temp_gen_AL_select_idx])\n",
        "        gen_AL_select_idx = al_idx[temp_gen_AL_select_idx]\n",
        "        gen_AL_select_idx = IndexCollection(gen_AL_select_idx)\n",
        "        #print('X_t[gen_AL_select_idx]',X_t[gen_AL_select_idx])\n",
        "\n",
        "        '''################\n",
        "        strategy2 = strategy_getter(X_t, y, **AL_params) #, **, al_hyperparameters\n",
        "\n",
        "        # select instances for this gen and AL Model\n",
        "        gen_AL_select_idx2 = strategy2.select(label_index=accept_idx, unlabel_index=gen_score_reject_idx, batch_size=n_instances, model=clf, qp_solver = 'OSQP') \n",
        "        gen_AL_select_idx2 = IndexCollection(gen_AL_select_idx2)\n",
        "\n",
        "        print('gen_AL_select_idx2', gen_AL_select_idx2)'''\n",
        "\n",
        "      else:\n",
        "        # select instances for this gen and AL Model\n",
        "        gen_AL_select_idx = strategy.select(label_index=accept_idx, unlabel_index=gen_score_reject_idx, batch_size=n_instances, model=clf) \n",
        "        gen_AL_select_idx = IndexCollection(gen_AL_select_idx)\n",
        "\n",
        "      indices['gen_AL_select_idx'][g+1] = gen_AL_select_idx.index #Different for every model\n",
        "      indices['gen_accept_idx'][g+1] = np.concatenate((gen_AL_select_idx.index, gen_score_accept_idx.index))\n",
        "\n",
        "      \n",
        "      print('len gen_AL_select_idx', len(gen_AL_select_idx))\n",
        "      print('len accept_idx before round update', len(accept_idx.index))\n",
        "\n",
        "      ## add score and AL selected instances to accept_idx\n",
        "      gen_score_reject_idx.difference_update(gen_AL_select_idx) # after subtracting the AL selections, the remaining samples in the index are ultimately rejected\n",
        "      accept_idx.update(gen_score_accept_idx)\n",
        "      accept_idx.update(gen_AL_select_idx)\n",
        "      print('len accept_idx after round update', len(accept_idx.index))\n",
        "      print('len gen_score_reject_idx after round update', len(gen_score_reject_idx))\n",
        "\n",
        "      ####################################################################################\n",
        "      # Save rejected and accepted cases\n",
        "      ####################################################################################\n",
        "          \n",
        "      if g == 0:\n",
        "        total_score_accept_idx = IndexCollection(np.array(gen_score_accept_idx.index))\n",
        "        total_reject_idx = IndexCollection(np.array(gen_score_reject_idx.index)) # define total reject index list in initial generation, update after\n",
        "        total_AL_select_idx = IndexCollection(np.array(gen_AL_select_idx.index))\n",
        "\n",
        "      else:\n",
        "        total_score_accept_idx.update(gen_score_accept_idx)\n",
        "        total_reject_idx.update(gen_score_reject_idx)\n",
        "        total_AL_select_idx.update(gen_AL_select_idx)\n",
        "\n",
        "      indices['total_reject_idx'][g+1] = total_reject_idx.index \n",
        "      indices['total_score_accept_idx'][g+1] = total_score_accept_idx.index \n",
        "      indices['gen_score_accept_idx'][g+1] = gen_score_accept_idx.index \n",
        "      indices['gen_score_reject_idx'][g+1] = gen_score_reject_idx.index  \n",
        "\n",
        "      gen_score_reject_idx.difference_update(gen_AL_select_idx) ## remove AL selects from (score) rejected cases\n",
        "      indices['gen_AL_reject_idx'][g+1] = gen_score_reject_idx.index\n",
        "\n",
        "      #indent removed, because I actually do get gen_AL_select_idx from the score model, even though the naming is imprecise\n",
        "      indices['total_AL_select_idx'][g+1] = total_AL_select_idx.index #Different for every model\n",
        "      stats['n_AL_selects'].loc[stats['generation'] == g+1] = len(total_AL_select_idx.index)\n",
        "      stats['bad_ratio_AL_selects'][g+1] = np.round(np.mean(y[total_AL_select_idx.index]), 4) \n",
        "      stats['bad_ratio_rejects'].loc[stats['generation'] == g+1] = np.round(np.mean(y[total_reject_idx.index]), 4)\n",
        "      stats['bad_ratio_score_accepts'][g+1] = np.round(np.mean(y[total_score_accept_idx.index]), 4) # different to some degree      \n",
        "      stats['n_rejects'].loc[stats['generation'] == g+1] = len(total_reject_idx.index)\n",
        "      stats['n_score_accepts'].loc[stats['generation'] == g+1] = len(total_score_accept_idx.index)      \n",
        "\n",
        "    # same for all models\n",
        "    stats['n_accepts'].loc[stats['generation'] == g+1] = len(accept_idx.index)\n",
        "    indices['accept_idx'][g+1] = accept_idx.index\n",
        "    stats['bad_ratio_accepts'].loc[stats['generation'] == g+1] = np.round(np.mean(y[accept_idx.index]), 4)\n",
        "\n",
        "    ##########################################\n",
        "    # PREDICTION TEST SCORING OF SELECTED INSTANCES, GATHER PERFORMANCE METRICS\n",
        "    ##########################################\n",
        "\n",
        "    if weights and key not in ['oracle', 'score']:\n",
        "      # use sample weights in fitting\n",
        "      ### not too efficient to save \"total_AL_select_idx\" and re-initialize sample_weights every gen... \n",
        "      ### A smarter solution would just add correct weights dynamically\n",
        "\n",
        "      #find matches in AL select and accept indices and set weights for these rows\n",
        "      sample_weights=np.ones(len(accept_idx.index))\n",
        "      sample_weights[np.where(np.in1d(accept_idx.index, total_AL_select_idx.index, assume_unique=True, invert=False))] = w_factor \n",
        "      \n",
        "      clf.fit(X=X_t[accept_idx.index, :], y=y[accept_idx.index], sample_weight=sample_weights)\n",
        "    \n",
        "    else:\n",
        "      # fit clf again with all selected instances\n",
        "      clf.fit(X=X_t[accept_idx.index, :], y=y[accept_idx.index]) \n",
        "    \n",
        "    ##########################################\n",
        "    # predict on TEST\n",
        "    ##########################################\n",
        "    test_prediction = clf.predict_proba(X_t[test_idx, :])\n",
        "    print(f'mean of gen {g+1} TEST prediction:', np.mean(test_prediction[:,1]))\n",
        "\n",
        "    # plot prediction density for test prediction\n",
        "    '''sns.set_style('whitegrid')\n",
        "    sns.kdeplot(test_prediction[:,1]).set_title('Density Plot of end-of-gen test prediction')\n",
        "    plt.show()''' \n",
        "    \n",
        "    test_pred_thres = (test_prediction[:,1] >= threshold_dict[do_thres]).astype(int) #set any prediction reaching or exceeding threshold to 1\n",
        "\n",
        "\n",
        "    ### COMPUTE & COLLECT Test prediction performance AND INTERNAL & EXTERNAL COST for this generation\n",
        "    stats = prediction_analyser(y[test_idx], test_prediction, test_pred_thres, stats, gen=g+1, severity_ratio=severity)\n",
        "    \n",
        "    if key == 'oracle':\n",
        "      cost = cost_function(y, test_pred_thres, indices, cost, cost_matrix, gen = g+1, oracle=True)\n",
        "    else:\n",
        "      cost = cost_function(y, test_pred_thres, indices, cost, cost_matrix, gen = g+1, oracle=False)\n",
        "    \n",
        "    ################################################################################################################\n",
        "    # display some info at the end of every second round\n",
        "    ################################################################################################################\n",
        "\n",
        "  # if (g % 2 == 0):\n",
        "    if key == \"oracle\":\n",
        "      print(f'gen {g+1}/{num_gens} | # applicants = {len(applicants_idx.index)} | score accepts = NaN | AL selects = NaN | total accepts = {len(accept_idx.index)} | unlabelled = {len(unlabel_idx.index)} |  time = {((time.time() - loop_start) / 60):.2f} min')\n",
        "    elif key == 'score':\n",
        "      print(f'gen {g+1}/{num_gens} | # applicants = {len(applicants_idx.index)} | score accepts = {len(gen_score_accept_idx.index)} | AL selects = NaN | total accepts = {len(accept_idx.index)} | unlabelled = {len(unlabel_idx.index)} |  time = {((time.time() - loop_start) / 60):.2f} min')\n",
        "    else:\n",
        "      print(f'gen {g+1}/{num_gens} | # applicants = {len(applicants_idx.index)} | score accepts = {len(gen_score_accept_idx.index)} | AL selects = {len(gen_AL_select_idx.index)} | total accepts = {len(accept_idx.index)} | unlabelled = {len(unlabel_idx.index)} |  time = {((time.time() - loop_start) / 60):.2f} min')\n",
        "      #AL = {len(total_AL_select_idx.index)} | total rejects = {len(total_reject_idx.index)} |\n",
        "      #.format(g, num_gens, len(applicants_idx.index), len(accept_idx), len(unlabel_idx), (time.time() - loop_start) / 60))  # strategy{} >> re.search('query_labels.(.+?)\\'>', str(strategy.__class__)).group(1)\n",
        "  \n",
        "  print(cost)\n",
        "  print('\\n Finished {} generations in {:.2f} minutes'.format(num_gens, (time.time() - loop_start) / 60), '\\n \\n \\n -------------------------------------------------')\n",
        "\n",
        "  return stats, indices, cost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jKcyFLscV-Ru"
      },
      "source": [
        "# Prepare and run loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VsXIteojiXiU"
      },
      "outputs": [],
      "source": [
        "## available datasets\n",
        "\n",
        "    # OK gmsc          # shape:  (150000, 68)\n",
        "    # OK uk            # very low bad rate, shape:  (30000, 51), y mean:  0.04\n",
        "    # OK lendingclub   # loooow performance, X shape:  (41623, 114) y shape:  (41623,) y mean:  0.1331235134420873 \n",
        "    # pakdd           # shape:  (50000, 373), y mean:  0.26082\n",
        "   \n",
        "    \n",
        "    # OK bene2         # some learning, shape:  (7190, 28) >> 3 folds NO WASTE \n",
        "    # hmeq          # not promising either, shape:  (5960, 20) # 5 folds NO WASTE\n",
        "    # bene1_nobins  # shape:  (3123, 18) # 5 folds NO WASTE\n",
        "\n",
        "    # australian    # very small dataset shape:  (690, 42) >> 10 folds NO WASTE\n",
        "    # german        # shape:  (1000, 61) >> 10 folds NO WASTE\n",
        "    # thomas        # loooow performance shape:  (1225, 28) >> 10 folds NO WASTE\n",
        "\n",
        "#dataset_list = [\"bene2\", \"bene1_nobins\",\"gmsc\", \"uk\", \"lendingclub\", \"hmeq\", \"australian\", \"german\", \"thomas\", \"pakdd\"]\n",
        "\n",
        "#dataset_list = [\"german\", \"thomas\", \"australian\", \"hmeq\"]\n",
        "dataset_list = [\"bene1_nobins\"]\n",
        "ratios_list = [0.1,0.2] #,0.3,0.4,0.5\n",
        "cost_mat_list =[None, [[0,1],[5,0]]] #None, [[0,1],[2,0]],[[0,1],[5,0]],[[0,1],[10,0]]\n",
        "#cost_mat=[[0,1],[2,0]]\n",
        "#cost_mat = None\n",
        "\n",
        "### Training parameters\n",
        "rounds = 5 #do not change, always 5\n",
        "tuned = True # do not change, always run tuned data, exists just as a debugging option to make sure there is no issue with some AL_parameters\n",
        "wghts = False #weights\n",
        "server = 'google' #'google','cohen', 'springsteen', 'harrison', 'afflux'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYd5iHExayNI"
      },
      "outputs": [],
      "source": [
        "'''# TEST: load tuned parameters from JSON file via helper function\n",
        "parma = param_getter(tuned=True, dataset='thomas')\n",
        "#parma['eer'] = {'AL':{}, 'CLF':{}}\n",
        "print(len(parma.keys()))\n",
        "print(parma.keys())\n",
        "parma'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "28OdWODvVOCb",
        "outputId": "e8f5ebbb-3586-4a10-d3d6-c9991183dc63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X type:  <class 'numpy.ndarray'> X shape:  (3123, 18) y shape:  (3123,) y mean:  0.3333333333333333\n",
            "140484658952112 140484658977808\n",
            "tuned thresholds:  {'bene2': 0.334, 'bene1_nobins': 0.343, 'gmsc': 0.057, 'uk': 0.036, 'lendingclub': 0.135, 'hmeq': 0.241, 'australian': 0.428, 'german': 0.309, 'thomas': 0.254, 'pakdd': 0.275}\n",
            "available_data 2418\n",
            "241 217\n",
            "bene1_nobins__rounds-5_max_gens-10_AL_acc_rate-0-1_weights-False_cost_matrix-None_do_thres-tuned\n",
            "\n",
            "----------- STARTING LOOP FOR DATASET bene1_nobins ------------\n",
            "start time:  2022-03-31 15:34:44.492015\n",
            "{'strategy_name': 'QueryInstanceSPAL', 'kernel': 'rbf', 'mu': 0.01, 'gamma': 0.01, 'rho': 10, 'lambda_init': 0.01, 'lambda_pace': 0.1}\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------------------\n",
            "PARAMETERS:\n",
            "\n",
            "key:  spal\n",
            "rounds:  5\n",
            "iteration:  0\n",
            "max_gens:  10\n",
            "sample_size:  217\n",
            "init_sample:  241\n",
            "score_acc_rate:  0.9\n",
            "AL_acc_rate:  0.1\n",
            "seed:  30\n",
            "weights:  False\n",
            "w_factor:  9.0\n",
            "do_thres:  tuned\n",
            "tuned_threshold:  0.343\n",
            "AL_params:  {'strategy_name': 'QueryInstanceSPAL', 'kernel': 'rbf', 'mu': 0.01, 'gamma': 0.01, 'rho': 10, 'lambda_init': 0.01, 'lambda_pace': 0.1}\n",
            "CLF_params:  {'tol': 0.0001, 'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 50, 'C': 0.1}\n",
            "cost_matrix:  None\n",
            "dataset_name:  bene1_nobins\n",
            "\n",
            "sanity check of train-test splits: (2498,) (625,)\n",
            "strategy SPAL, round 0: running loop for 10 generations\n",
            "classifier:  LogisticRegression(C=0.1, max_iter=50, penalty='l1', random_state=30,\n",
            "                   solver='liblinear')\n",
            "mean of initial TEST prediction: 0.34469590255262045\n",
            "y mean of TEST DATA: 0.3344\n",
            "y mean of INITIALLY ACCEPTED DATA: 0.3236514522821577\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3gU5fbA8e8hoUtRQJCioYYmRRBEKVG6V0VBUQGVIly5WMBOlYugYqEJFlBBBKQIeFHhqvgDLAiKiFjBSO+9BIKQ5Pz+mEnuJqRsSDaTzZ7P88yTndkpZzbJnJ15Z84rqooxxpjQlc/rAIwxxnjLEoExxoQ4SwTGGBPiLBEYY0yIs0RgjDEhzhKBMcaEOEsEBhFREanmvn5DRIa7r6NEZFeAt91dRD4L5DbS2O4MERmdQ9u6TUR2ikiMiDTMiW16RUS2iUgb9/UQEXnrAtfzq4hEZWtwJk2WCPIIEekmIuvcg81eEVkmIs0zux5VfUBVnw1QjBFu0gn32d5sVW0XiO1lF99EeYFeBh5U1YtU9ccArD9xPSNFZFZW15NdVPU5Vb0/o/lSS8qqWkdVVwYsOJOMJYI8QEQeBSYAzwFlgcuB14BOORxHWE5uL4hcAfzqdRCZ5ZuwTR6nqjYE8QCUAGKAO9KZpwnwLXAM2AtMBgr4vK9ANff1DGC0+zoK2AUMAQ4B24DuPsvNAF4HlgKngDbAP4AfgRPATmCkz/w73G3FuEMzoCfwtc881wLfA8fdn9f6vLcSeBb4BjgJfAaUTmOf/Yl9tM94XyAaOAIsAcq70790Yz7lxnxnKtvKBwwDtgMHgJnu76Wgu0zi8n+lsmyq6wduAja4v7PVQD2fZZ4CdrufwSagNdABOAucc9fzUxqfyzZgMPAbcBSYDhRK8Zk9BewD3nP37WngL+AwMB+4xGd997j7fRgY6q6/jfveSGCWz7zN3X055v5t9AT6uTGfdeP+yCfOxPUUxPmis8cdJgAFU8T8mPvZ7wV6ef1/GWyD5wHYkMVfoHMAiAPC05mnEXANEA5EAL8DA33eTy8RxAHj3H/GVu4BK9Jn3uPAde4Bo5C7zJXueD1gP3CrO3+Eu61wn233xE0EwCXuwekeN9a73fFS7vsr3QNSDaCwO/5CGvvsT+yJ+3kDTrK4yp33VeDL1D6fNLbVGyeJVAEuAhYB72Vi+WTvAw3dg1pTIAy4zz0wFgQicQ6i5X0+06ru65H4HHjT2NY24Begkvt5f5PK73usu63CwCPAGqCiO+1N4H13/to4B++W7nvj3OXPSwQ4Z0Un3d9pfqAU0CDl7yJFnInrGeXGcClQBieZPJsi5lHuem8ETgMXe/2/GUyDXRoKfqWAQ6oal9YMqvqDqq5R1ThV3Ybzz9wqE9sYrqp/q+oq4BOgq897/1HVb1Q1QVXPqOpKVf3ZHd8IvJ+Jbf0D+FNV33NjfR/4A7jZZ57pqrpZVWNxvp02yELsiboD76jqelX9G+cbczMRifAz7u7AOFXdoqox7vJ3ZeHSSj/gTVVdq6rxqvou8DdOMo/HOejWFpH8qrpNVf/K5Ponq+pOVT0CjME5OCdKAJ5xP7NY4AFgqKrucj+bkcDt7r7dDnysql+67w13l09NN2C5qr6vqudU9bCqbvAz3u7AKFU9oKoHgX/jfFlIdM59/5yqLsVJTpF+rttgbQR5wWGgdHoHHRGpISIfi8g+ETmB05ZQ2s/1H1XVUz7j24HyPuM7U2yrqYisEJGDInIc50Di77bKu+v3tR2o4DO+z+f1aZxv4Bcae6rbdQ/mh1NsNz0p496Oc0ZT1s/lU7oCeExEjiUOON/gy6tqNDAQ54B8QETmikhq+5Qe399Zys/koKqeSRHLYp84fsdJRmXd5ZLW5X7Wh9PYZiWcs7kLkdrn6xvz4RRfhDL6uzApWCIIft/ifFu8NZ15Xsf5Zl1dVYvjXDcXP9d/sYgU9Rm/HOc6baKU5Wvn4Fxjr6SqJYA3fLaVUanbPTgHHl+X41wPvxAZxZ7qdt1lSmViuynjvhzncsX+TEX7PzuBMapa0mco4p4hoapzVLW5u03FuZQDGX++iSqliDW93+dOoGOKWAqp6m6c6/FJ6xKRIjifW1r7VDWN9zL7d5HW79FcIEsEQU5VjwMjgCkicquIFBGR/CLSUURedGcrhtN4GyMiNYH+mdzMv0WkgIi0wGnEXJDOvMWAI6p6RkSa4FwSSHQQ59JBlTSWXQrUcG+FDReRO3GuQ3+cyXgzG/v7QC8RaSAiBXHOmNa6l9HAOaCnFXPi8oNEpLKIXOQuPy+9y3UppFz/NOAB9+xKRKSoiPxDRIqJSKSI3ODGeQaI5X+XY/YDESKS0f/1ABGpKCKX4DTwzktn3jeAMSJyBYCIlBGRxLvRPgBuEpHmIlIA5zp9WtueDbQRka7u77aUiCRe1vPn8x3mbrs0zt97rrlNNi+wRJAHqOorwKM4d64cxPn29SDwoTvL4zgH5JM4B5n0/vFT2ofTYLsH55/5AVX9I535/wWMEpGTOP+w833iPI1zTfob91LDNSn24zDOwfoxnEsMTwI3qeqhTMSb6dhVdTnO9e2FON9yqwJ3+cwyEnjXjTm1NoZ3cO6w+RLYinOAfigTcSZbv6quw7mLabIbfzROozo47QMv4DRu78NpQB3svpeY5A6LyPp0tjcH546rLTiXa9J7sG4izhneZ+7vdA1OIzaq+iswwF3fXjfWVB9AVNUdOA25j+HcmbUBqO++/TZOm8cxEfkwlcVHA+uAjcDPwPoMYjaZJKrWMY3Je9ynUmepakWvY8lNRGQbcL+b/IwB7IzAGGNCniUCY4wJcXZpyBhjQpydERhjTIgLuqJSpUuX1oiICK/DMMaYoPLDDz8cUtUyqb0XdIkgIiKCdevWeR2GMcYEFRFJ+dR+Ers0ZIwxIc4SgTHGhDhLBMYYE+IsERhjTIizRGCMMSEuYIlARN4RkQMi8ksa74uITBKRaBHZKCJXBSoWY4wxaQvkGcEMnG4U09IRqO4O/XBq5htjjMlhAUsEqvolTrnZtHQCZqpjDVBSRC4LVDzGGBOsTp06xbZt2wK2fi/bCCqQvMu8XaTRNaCI9BORdSKy7uDBgzkSnDHG5BajR4+mc+fOJCSk1SV01gRFY7GqTlXVxqrauEyZVJ+QNsaYPOXYsWPs3u30lvrUU08xYcIE8uULzCHby0Swm+R9p1bkwvumNcaYPCM+Pp5rr72Wnj17AlCyZElatmwZsO15WWtoCfCgiMzF6fruuKru9TAeY4zx1IkTJyhevDhhYWGMGTOGSpUqZbxQNgjk7aPvA98CkSKyS0T6iMgDIvKAO8tSnD5To3H60f1XoGIxxpjcbuPGjVSpUoUlS5YAcNttt9G4ceMc2XbAzghU9e4M3lecjq+NMSZkqSoiQs2aNbnllluoVq1ajscQFI3FxhiTF73//vs0a9aMM2fOUKBAAd555x1q166d43FYIjDGGI9cfPHFFCtWjBMnTngaR9D1Wdy4cWO1jmmMMcEoISGBl19+mZIlS9KvXz/gf5eGAk1EflDVVBsd7IzAGGNyiIjwf//3f3z99dfJpnnNEoExxgTQ33//zZgxYzh8+DAiwqJFi3j33Xe9DisZSwTGGBNAf/75J//+979ZtGgRAEWKFMkVZwG+gq7zemOMye1iYmJYsWIFN998M3Xr1uWPP/6gSpUqXoeVJjsjMMaYbJZYJG7nTqeuZm5OAmCJwBhjssXRo0fZtWsXAIMHD2bFihU5ViIiq+zSkDHGZFF8fDzXXXcdFSpU4PPPP6dEiRI0b97c67D8ZonAGGMu0PHjxylRogRhYWE899xzXH755V6HdEHs0pAxxlyAjRs3UrlyZT788EMAbr31Vq66Kji7XrdEYIwxmZDYS1itWrXo0qULNWvW9DiirLNEYIwxfpo1axZNmzblzJkz5M+fn2nTplkiMMaYUFKmTBlKlSrFyZMnvQ4lW1ljsTHGpCE+Pp6XXnqJkiVL8sADD9C+fXvatWuX654Mzio7IzDGmDTky5ePVatW8e233yZNy2tJACwRGGNMMmfOnGHUqFG5ukhcdrNEYIwxPqKjoxk9enTSbaGFCxf2OKLAs0RgjAl5MTExSQf+unXrsmnTJvr06eNxVDnHEoExJuSNGTOGO+64I6lWUOXKlT2OKGdZIjDGhKQjR44kVQd9+umn+fLLL6lYsaLHUXnDEoExJuTEx8dz7bXX0rt3bwBKlChBs2bNPI7KO/YcgTEmZBw7doySJUsSFhbG2LFjueKKK7wOKVewMwJjTEj46aefkhWJ69SpEw0aNPA4qtzBEoExJk9LLBJXu3ZtunbtSu3atT2OKPexRGCMybPee+89mjRpklQk7s0336RGjRpeh5XrWCIwxuRZZcuW5dJLL81zReKymzUWG2PyjPj4eF544QUuueQS+vfvT7t27WjXrp3XYeV6lgiMMXlGvnz5+Oabb7jsssu8DiWo2KUhY0xQi42N5ZlnnuHQoUNJReLefvttr8MKKgFNBCLSQUQ2iUi0iDydyvuXi8gKEflRRDaKyI2BjMcYk/ds2bKF559/niVLlgBQqFAhjyMKPgFLBCISBkwBOgK1gbtFJOV9W8OA+araELgLeC1Q8Rhj8o4TJ06wePFiAOrUqcOff/6Z9JSwybxAnhE0AaJVdYuqngXmAp1SzKNAcfd1CWBPAOMxxuQRzz33HF27dk0qEmdPCGdNIBNBBWCnz/gud5qvkUAPEdkFLAUeSm1FItJPRNaJyLqDBw8GIlZjTC53+PBhduzYAcCQIUP46quvQrZIXHbzurH4bmCGqlYEbgTeE5HzYlLVqaraWFUblylTJseDNMZ4K7FIXGIfAcWLF+eaa67xOKq8I5C3j+4GKvmMV3Sn+eoDdABQ1W9FpBBQGjgQwLiMMUHiyJEjXHLJJYSFhfHyyy/bJaAACeQZwfdAdRGpLCIFcBqDl6SYZwfQGkBEagGFALv2Y4zhp59+okqVKixatAiAm2++mXr16nkcVd4UsESgqnHAg8CnwO84dwf9KiKjROQWd7bHgL4i8hPwPtBTVTVQMRljcr/4+HjAKRLXrVs3O/jnAAm2427jxo113bp1XodhjAmAd999l4kTJ7J69Wp7HiCbicgPqto4tfe8biw2xpgk5cuXp0KFCsTExHgdSkixWkPGGM/Ex8fz3HPPcckllzBgwADatm1L27ZtvQ4r5FgiMMZ4Jl++fKxZs8aKxHnMLg0ZY3LU6dOnGT58eLIicW+99ZbXYYU0SwTGmBy1detWXnzxRT766CMAChYs6HFEJsNEICLXiUhR93UPERknIvZUhzHGb8ePH+eDDz4AnCJx0dHR9OrVy+OoTCJ/zgheB06LSH2c+/7/AmYGNCpjTJ7y/PPP061bt6QicZUqVcpgCZOT/EkEce5DXp2Ayao6BSgW2LCMMcHu4MGDbN++HXCKxH3zzTdWJC6X8ueuoZMiMhi4B2jhFoXLH9iwjDHBLD4+nubNm1OpUiWWL19O8eLFufrqq70Oy6TBn0RwJ9AN6K2q+0TkcuClwIZljAlGhw8fplSpUoSFhfHKK68QERHhdUjGDxleGlLVfcBCILFp/xCwOJBBGWOCz4YNG5IVibvpppuoW7eux1EZf/hz11Bf4APgTXdSBeDDQAZljAkeiUXi6tSpQ48ePahfv77HEZnM8qexeABwHXACQFX/BC4NZFDGmOAwffp0GjduTGxsLPnz52fKlClUrVrV67BMJvmTCP52+xwGQETCcfoaNsaEuMsvv5wrrriC06dPex2KyQJ/GotXicgQoLCItAX+BXwU2LCMMblRfHw8o0aNokyZMjz44IO0bt2a1q1bex2WySJ/zgiexuk17GfgnzidzA8LZFDGmNwpX758/PDDD/z8889eh2KykT9nBLcCM1V1WqCDMcbkPqdOnWLMmDEMGjSIMmXKsGjRIgoUKOB1WCYb+XNGcDOwWUTeE5Gb3DYCY0yI2L59O+PGjeOTTz4BsCSQB/nzHEEvoBqwALgb+EtErGasMXnYsWPHmD9/PuD0HRwdHU3Pnj29DcoEjF9lqFX1HLAMmAv8gHO5yBiTR73wwgv06NEjqUic1QjK2/x5oKyjiMwA/gS6AG8B5QIclzEmhx04cIBt27YBMHToUL799ltLACHCn+v99wLzgH+q6t8BjscY44H4+Hiuu+46rrjiCpYvX06xYsVo1KiR12GZHJJhIlDVu3MiEGNMzjt48CBlypQhLCyMiRMnWpG4EJXmpSER+dr9eVJETvgMJ0XkRM6FaIwJhB9//JEqVaok9Rx24403Urt2bY+jMl5I84xAVZu7P60TGmPykLi4OMLDw7nyyivp3bu3XQIyfjUWv+fPNGNM7vf222/TqFEjYmNjCQ8PZ+LEiVSuXNnrsIzH/Ll9tI7viPtAmX2FMCYIRUREULVqVSsSZ5JJr41gsIicBOr5tg8A+4H/5FiExpgLFh8fz/Dhw5k0aRIArVu3ZtGiRZQqVcrjyExukl4bwfPA8yLyvKoOzsGYjDHZJF++fPz0009UqFDB61BMLpZmIhCRmqr6B7BARK5K+b6qrg9oZMaYCxITE8Po0aN59NFHufTSS/nggw+sPpBJV3ptBI+6P19JZXjZn5WLSAcR2SQi0SLydBrzdBWR30TkVxGZk4nYjTGp2LFjBxMmTGDZsmWAFYkzGUvv0lA/9+f1F7JiEQkDpgBtgV3A9yKyRFV/85mnOjAYuE5Vj4qIdYFpzAU4evQon376KXfddRe1a9dmy5YtlC9f3uuwQlK5cuXYv3//edPLli3Lvn37PIgoY/7cPnqHiBRzXw8TkUUi0tCPdTcBolV1i9vV5VygU4p5+gJTVPUogKoeyFz4xhiAsWPHct9997F7924ASwIeSi0JpDc9N/Dn9tHhqnpSRJoDbYC3gTf8WK4CsNNnfJc7zVcNoIaIfCMia0SkQ2orEpF+IrJORNYdPHjQj00bk/ft37+frVu3Ak6RuDVr1lijsLkg/iSCePfnP4CpqvoJkF0XHcOB6kAUTl8H00SkZMqZVHWqqjZW1cZlypTJpk0bE7zi4+Np3rw5ffv2BaBYsWI0bOjPibox5/On+uhuEXkT51r/WBEpiH8JZDdQyWe8ojvN1y5grdvfwVYR2YyTGL73Y/3GhJwDBw4kFYmbNGmSPRVssoU/B/SuwKdAe1U9BlwCPOHHct8D1UWksogUAO4ClqSY50OcswFEpDTOpaIt/oVuTGhZv359siJxHTt2pGbNmh5HZXx9/vnnXodwQfzpqvI08BfQXkQeBC5V1c/8WC4OeBAnifwOzFfVX0VklIjc4s72KXBYRH4DVgBPqOrhC9wXY/KkuLg4AOrVq8f9999PkyZNPI7IpOarr76iU6dOhIenfqGlbNmyORyR/0RV059B5BGcu3sWuZNuw2kreDXAsaWqcePGum7dOi82bUyOe+utt5g4cSLfffcdhQsX9jock4bvvvuONm3aUKFCBVatWsWll+a+O+FF5AdVbZzae/60EfQBmqrqKXdlY4FvAU8SgTGhpGrVqtSsWZPY2FhLBLnUxo0b6dChA2XKlGH58uW5MglkxJ9EIPzvziHc1xKYcIwJbfHx8QwbNoyyZcsycOBArr/+eq6//oKe6TQ54I8//qBNmzYULVqUL774Imhv3/UnEUwH1orIYpwE0AnnWQJjTDbLly8fv//+OydOWCeAud1ff/1F69atyZcvH1988UVQd/PpT5/F40RkJdAcUKCXqv4Y6MCMCRUnT57k2Wef5fHHH+fSSy9lwYIF5M+f3+uwTDp27txJ69atOXPmDKtWraJGjRpeh5Ql/tw+mkhS/DTGZINdu3bx6quv8t///hfAkkAut2/fPlq3bs3Ro0f57LPPqFu3rtchZZk/tYZGAO8CFwOlgekiMizQgRmTlx05coQ5c5xiu7Vq1WLLli3ce++9HkdlMnL48GHatm3Lnj17WLZsWZ7p79mfM4LuwNWqOlJVnwGuAe4JbFjG5G0vvfQSvXr1SioSd9lll3kckcnI8ePHad++PX/++SdLlizh2muv9TqkbONPItgDFPIZL8j5pSKMMRnYu3cvW7Y4D84PHTqU7777LmjvMgk1MTEx3HjjjWzcuJFFixZxww03eB1StvLnrqHjwK8i8jlOY3Fb4DsRmQSgqg8HMD5j8oT4+HhatGhBREQEy5cv56KLLqJ+/fpeh2X8EBsbS6dOnVizZg3z58/nxhtv9DqkbOdPIljsDolWBiYUY/Keffv2UbZsWcLCwpgyZYoViQsyZ8+e5fbbb2fFihXMnDmTLl26eB1SQPhz++i7ORGIMXnN+vXradGiBdOnT6dr1660b9/e65BMJsTFxdGtWzeWLl3Km2++SY8ePbwOKWAyc/uoMcYP586dA5wicQ888ADNmjXzOCKTWQkJCfTu3ZuFCxcyfvx4+vXr53VIAWWJwJhsNHXqVBo2bMjp06cJDw/nlVdeoVKlShkvaHINVaV///689957jB49moEDB3odUsClmQhE5D335yM5F44xwa169erUqVOHM2fOeB2KuQCqyqOPPsrUqVMZMmQIQ4cO9TqkHJFeG0EjESkP9BaRmaR4olhVjwQ0MmOCQHx8PEOGDKFcuXIMGjTIisQFueHDhzNhwgQeeeQRRo8e7XU4OSa9RPAG8AVQBfiB5IlA3enGhLR8+fKxefNmYmNjvQ7FZNHzzz/PmDFj6Nu3L+PHj0ckdKrp+NMxzeuq2j+H4smQdUxjvHbixAn+/e9/8+STT1K2bFni4uLS7JXKBIeJEycycOBAunfvzrvvvktYWJjXIWW79Dqm8aeryv4iUl9EHnSHetkfojHBY8+ePbz++utJ/dNaEghub731FgMHDqRz587MmDEjTyaBjPhTdO5hYDZwqTvMFpGHAh2YMbnJoUOHmDVrFgA1a9Zk69atefq+8lAxe/Zs+vXrR8eOHXn//fdDNqn7c/vo/ThdVY5Q1RE4Ref6BjYsY3KXl19+mfvvv589e/YAubsjcuOfxYsXc9999xEVFcXChQspUKCA1yF5xp9EYF1VmpC0Z8+epCJxw4YN4/vvv6d8+fIeR2Wyw7Jly7jzzjtp0qQJS5YsCfn+oDPbVSXArVhXlSaPi4+Pp2XLlsmKxF155ZVeh2UuULly5di/f/950//66y8uuugiDyLKXTLbVSVYV5UmD9u7dy/lypUjLCyM1157jSpV7C7pvCC1JABw4MCBHI4kd/KrxISqrlfVSe5gScDkSevWraNq1aosWLAAgHbt2lGtWjWPozIm8EKzidwYH+fOnSN//vw0aNCAAQMG5Kmep0JdbGws06ZN8zqMXM+KzpmQ9sYbb1C/fv2kInEvvfQSFStW9Dosk0WxsbFMmjSJqlWr8sgjVi4tI5YITEirVasWDRs25O+///Y6FJMNUiaAyMhIVqxY4XVYuV6Gl4ZEpDMwFudhMnEHVdXiAY7NmGwXFxfH4MGDKVeuHI899hitWrWiVatWXodlsijxEtALL7zA3r17iYqKYs6cOURFRQHOcx+pNRjb8yAOf9oIXgRuVtXfAx2MMYEWFhZGdHR0UucxJrilTACtWrVKlgAS7du3z5sAg4Q/l4b2WxIwwez48eMMGjSI/fv3IyIsWLCACRMmeB2W58qVK4eInDeUK1fO69AydObMGV599dWkS0A1atRgxYoVrFy58rwkYDLmzxnBOhGZB3wIJF1IVdVFAYvKmGy0d+9epk6dSuPGjenevXvI1pNJKa1769OanhucOXOGadOm8fzzz7N3715atmyZ6hmAyRx/zgiKA6eBdsDN7nCTPysXkQ4isklEokXk6XTm6yIiKiKplkg1JrMOHjzIzJkzAadI3LZt2+jevbvHUZkLlXgGUKVKFR5++GGqV6/OihUrWLVqlSWBbODPk8W9LmTFIhIGTAHaAruA70Vkiar+lmK+YsAjwNoL2Y4xqRk3bhzjx4+nTZs2lC9fnjJlyngdUlC57bbbiIyMpEaNGklDmTJlcryzlsQzgBdeeIE9e/bYGUCA+HPXUEXgVeA6d9JXwCOquiuDRZsA0aq6xV3PXKAT8FuK+Z7FuSvpiUzEbcx5du3axd9//03VqlUZOnQoPXr0sCJxF2jTpk188sknyRrVS5YsmSwxJCaK6tWrU7Ro0WzdfmoJYNasWURFRYVUz2E5xd+ic3OAO9zxHu60thksVwHY6TO+C2jqO4OIXAVUUtVPRCTNRCAi/YB+AJdffrkfIZtQExcXR6tWrahcuXJSkbg6dep4HVauldFzE7/99htxcXHs2LGDTZs2sXnz5qRh1apVSX0zJKpQocJ5ZxCRkZFERESk2SaTViG4YsWKUaxYMfbs2UOLFi0sAeQAfxJBGVWd7jM+Q0QGZnXDIpIPGAf0zGheVZ0KTAWnq8qsbtvkHbt376Z8+fKEh4fz5ptvWpE4P6gqAwYMSPP9xHvrw8PDqVKlClWqVKFjx47J5jl9+jTR0dFs3rw5WaKYN28eR48eTZovPDycqlWrJjuDSBzSapQ+efIkDRo0sASQg/xJBIdFpAfwvjt+N3DYj+V2A5V8xiu60xIVA+oCK91fdDlgiYjcoqrWKbHJ0Lp162jRogXTp0/nrrvuok2bNl6HFBSmTp3K22+/zZAhQxgzZswFraNIkSLUq1ePevWS91yrqhw+fDgpMfgmic8++8zvJ7hXrVplCSAH+dN5/RU4bQTNAAVWAw+r6o4MlgsHNgOtcRLA90A3Vf01jflXAo9nlASs83pz9uxZChQoQHx8PEOGDOGRRx6xtgA/ffvtt7Rq1YrWrVvz8ccf52j/vPHx8ezcuTMpMTz0UNo93mZ0XDKZl17n9Rkmgixu+EZgAhAGvKOqY0RkFLBOVZekmHcllghMBl577TVeffVV1q1bl+0NlHnd3r17adSoEYULF2bdunVcfPHFnsaT3jd+SwTZL71EkOalIRF5UlVfFJFXcc4EklHVhzPasKouBZammDYijXmjMlqfMXXr1qVx48ZWIiKTzp49y+23387x48f59NNPPU8CJndJr40gsayEff02nomLiyrB6pQAACAASURBVOPJJ5+kfPnyPP7447Rs2ZKWLVt6HVbQGThwIKtXr2bevHm5pstNKwSXe6SZCFT1I/flaVVd4PueiNyRyiLGZLuwsDC2b99uDYdZ8Pbbb/P666/zxBNP0LVrV6/DSWKF4HIPfxqL16vqVRlNyynWRpD3HTt2jBEjRjBkyBDKlStHfHx8jjZq5iXfffcdLVq0oGXLlixbtszqLIWwC20j6AjcCFQQkUk+bxUH4rI3RGP+Z//+/bzzzjtcc801dOvWzZLABdq/fz+dO3emfPnyzJ0715KASVN6fxl7cNoHbgF+8Jl+EhgUyKBM6Nm/fz/Lli2jZ8+eREZGsm3bNkqXLu11WEHr3Llz3HHHHRw5coTVq1dTqlQpr0MyuVh6bQQ/AT+JyGLglKrGQ1IxuYI5FJ8JEePHj2fixIm0a9eO8uXLWxLIoscff5yvvvqK2bNn06BBA6/DMbmcP2WoPwMK+4wXBpYHJhwTSnbu3El0dDQAw4YNY/369fZgWDaYOXMmkyZNYtCgQXTr1s3rcEwQ8OeiYSFVjUkcUdUYESkSwJhMCEgsElelSpWkInG1atXyOqyg98MPP/DPf/6TqKgoXnzxRa/DMUHCn0RwSkSuUtX1ACLSCIgNbFgmr9q1axcVKlQgPDycadOmWZG4bHTw4EE6d+5MmTJlmD9/vjUOG7/5c2loILBARL4Ska+BecCDgQ3L5EXff/891apVY968eQC0bt2aypUrexxV3hAXF8edd97J/v37Wbx4sXXEYzLFnx7KvheRmkCkO2mTqtrz/cZvf//9NwULFuSqq67i0UcfpVWrVl6HlOc89dRTrFixgunTp9OoUSOvwzFBxp8zAnCSQG3gKuBuEbk3cCGZvGTy5MnUq1ePU6dOERYWxnPPPcdll13mdVh5ypw5cxg3bhwPPvggPXv29DocE4T86aryGSAKJxEsBToCXwMzAxqZyRPq1atHs2bNiIuzZxAD4aeffuL++++nRYsWjBs3zutwTJDyp8TEz0B94EdVrS8iZYFZqppRV5UBYSUmcre4uDgee+wxKlSowJNPPul1OHna4cOHufrqqzl79iw//PCDFWsz6bqgEhM+YlU1QUTiRKQ4cIDkPY8ZkyQ8PJw9e/aQP39+r0PJ0+Lj47n77rvZvXs3X375pSUBkyX+tBGsE5GSwDScUhPrgW8DGpUJKkePHmXAgAFJ1STnzp3Lyy+/7HFUedvQoUP5/PPPee2112jatKnX4Zggl24iEKf27/OqekxV3wDaAvepaq8cic4EhQMHDjBz5kxWrFgBYEXiAmz+/PmMHTuWBx54gD59+ngdjskD/GojUNXc0ZMF1kaQW+zbt4+lS5fSu3dvwLlebYXNAu/nn3/mmmuuoUGDBqxYsYICBQp4HZIJEum1EfhzaWi9iFydzTGZIDdx4kQGDBjAnj17ACwJ5ICjR49y2223Ubx4cRYsWGBJwGQbfxJBU2CNiPwlIhtF5GcR2RjowEzus23bNjZv3gw4ReI2bNhgReJySHx8PN27d2fHjh188MEH9rmbbJVexzSXq+oOoH0OxmNyqbi4OK6//nqqVKnCF198QdGiRYmMjMx4QZMtRo4cybJly3j99de57rrrvA7H5DHp3T76IXCVqm4XkYWq2iWngjK5x44dO6hUqRLh4eG88847ViTOA4sXL2b06NH06dOHf/7zn16HY/Kg9C4N+fYWbv/9Iej777+nRo0azJ07F4Drr7+eK664wuOoQsvvv//OvffeS5MmTZg8eTLOjXzGZK/0EoGm8drkcWfOnAHgqquu4vHHH+eGG27wOKLQdPz4cW699VaKFCnCwoULKVSokNchmTwqvURQX0ROiMhJoJ77+oSInBSREzkVoMlZkyZNSlYkbvTo0fbUqgcSEhK455572LJlCwsWLKBixYpeh2TysDQTgaqGqWpxVS2mquHu68Tx4jkZpMk5DRs2pHnz5p4UiStXrhwict5Qrly5HI/Fa88++ywfffQR48ePp2XLll6HY/K4DB8oy23sgbLsFRcXx8CBA6lUqRJPPfWUp7Gkd/072P5Os2LJkiV06tSJ++67j+nTp1u7gMkWWS06Z/Kw8PBwDh48yEUXXeR1KAbYtGkT99xzD1dddRWvv/66JQGTIywRhKDDhw8zdOhQRowYQfny5Xn//ffJl8/fPopMdipXrhz79+8/b/qOHTsoXLiwBxGZUGT//SHo8OHDzJkzh6+++gogVySB+Pj4dN8fMGAAO3bsyKFock5qSQDg0KFDORyJCWUBPQKISAcR2SQi0SLydCrvPyoiv7mlK74QEbtJPUD27t3LW2+9BUCNGjXYvn07d955p8dRORISEujbt2+680ybNo1q1arxz3/+k23btuVMYMaEiIAlAhEJA6bgdG1ZG6ev49opZvsRaKyq9YAPgBcDFU+omzhxIg8//DB79+4F4OKLL/Y4IkdCQgIPPPAA06dPp2jRoqnOU7ZsWaKjo7n//vuZMWMG1atXp0+fPvz11185HG32iImJ4YMPPqBbt25eh2KMQ1UDMgDNgE99xgcDg9OZvyHwTUbrbdSokRr/bNmyRTdt2qSqqjExMbp582aPI0ouISFB+/fvr4AOGTJEExISMlxm586d+tBDD2nBggU1LCxM77vvvqR9zM2OHDmiM2fO1E6dOmmhQoUU0NKlSyvOw5qpDsZkJ2CdpnX8TeuNrA7A7cBbPuP3AJPTmX8yMCyN9/oB64B1l19+ecA+qLzk3LlzGhERoa1bt/Y6lFQlJCToQw89pIA++eSTfiUBX3v27NFBgwZp4cKFNV++fNq9e3f97bffAhTthdm3b5+++eab2q5dOw0PD1dAK1SooA899JCuXLlSz507Z4nA5JhcnwiAHsAaoGBG67UzgvRt3bo16aC6YsUK3bFjh8cRnS8hIUEHDRqkgD766KOZTgK+9u3bp0888YQWKVJERUTvvPNO/fnnn7Mx2szZvn27jh8/Xlu0aKEiooBWq1ZNn3rqKV27dq3Gx8cnm79s2bKpJoGyZct6tAcmr/IqEfh1aQhoA/wOXOrPei0RpG3t2rVaoEABnT17ttehpCkhIUGfeOIJBfThhx/OUhLwdeDAAR08eLBedNFFCujtt9+uGzZsyJZ1Z+SPP/7Q5557Ths3bpx0IK9Xr56OHDlSN27cmG37aExWeJUIwoEtQGWgAPATUCfFPA2Bv4Dq/q7XEsH5Tp8+raqqcXFxOmLECN2/f7/HEaUuISFBhwwZooD2798/IAfIQ4cO6bBhw7R48eIK6K233qo//PBDtm4jISFBf/zxRx0+fLjWqVMn6eDftGlTHTt2bK5rizFG1aNE4GyXG4HN7sF+qDttFHCL+3o5sB/Y4A5LMlqnJYLkJkyYoFWrVtWTJ096HUqGnnnmGQW0b9++510iyW5HjhzRkSNHasmSJRXQm266SdeuXXvB64uPj9fVq1fr448/rlWqVFFA8+XLp1FRUTpp0qRceQnOGF+eJYJADJYIHInfpr/++mvt27evHj9+3OOI0jdq1CgFtFevXgFPAr6OHTumo0eP1ksuuUQB7dChg65evdqvZc+dO6dffPGFDhgwQMuXL6+A5s+fXzt27KjTpk3TAwcOBDh6Y7KPJYI85Ny5c9q/f399/vnnvQ7Fb88//7wCeu+992pcXJwnMZw4cUJfeOGFpFs227Rpk5QcUg4lS5bU3r17a6lSpRTQwoULa+fOnXX27Nl67NgxT+I3JqvSSwTe1xYwmRIeHs7Ro0c5cSI4uoR4+eWXGTx4MN26deOdd94hLCzMkziKFSvGU089xdatW3nppZfYuHEjR44cSXXeY8eOsXDhQjp27MiiRYs4dOgQCxcupFu3bpQoUSKHIzcm8KwMdRA4dOgQQ4YMYeTIkZQvX56EhIRcUR8oIxMmTGDQoEF07dqV2bNnEx6ee2ocnj59Os0nmQH+/vtvChQokIMRGRNY6ZWhzv1HE8PRo0eZN28eX3/9NZA7isRlZMqUKQwaNIguXbowa9asXJUEAIoUKZLu+5YETCjJ/UeUELV7926mTp0KQPXq1dm+fTtdu3b1OCr/vPHGGzz44IN06tSJOXPmkD9/fq9DMsakwxJBLjV58mQGDRqUVCSuZMmSHkfkn7feeov+/ftz0003MX/+fPtmbUwQsDaCXOSvv/7i3Llz1KxZk9OnT7N3716qVq3qdVh+mzFjBr1796Z9+/Z8+OGHFCxY0OuQ0pVWpzBly5Zl3759HkR0Yc6dO8euXbs4c+aM16GYXKBQoUJUrFjxvDNx66oyCMTFxdG6dWuqVavG8uXLKVKkSFAlgVmzZtG7d2/atGnDokWLcn0SAILqYJ+eXbt2UaxYMSIiIqxryxCnqhw+fJhdu3ZRuXJlv5ezROCxrVu3EhERQXh4OO+++25QHfwTzZ07l/vuu4+oqCg+/PBD62Ixh505c8aSgAFARChVqhQHDx7M1HLWRuCh7777jsjISObMmQNAq1atqFixosdRZc6CBQvo0aMHzZs356OPPsrwbhwTGJYETKIL+VuwROCB2NhYABo1asTQoUNp166dxxFdmMWLF9OtWzeuueYaPvnkk3TvyzfG5F6WCHLYhAkTqFu3LjExMYSFhfHMM89QpkwZr8PKtCVLltC1a1caN27MsmXLuOiii7wOyXgoLCyMBg0aULduXe644w5Onz6dqeWfeOIJ6tSpwxNPPJHpbT/33HPJxgP5tzhy5EhefvllAEaMGMHy5csBiIiI4NChQxe83g0bNrB06dJMLxcVFUV23DxjiSCHJN6d1aRJE9q2bUuw3a3la+nSpdx+++00bNiQ//73vxQrVszrkIzHChcuzIYNG/jll18oUKAAb7zxhl/LxcXFATB16lQ2btzISy+9lOltp0wEOWXUqFG0adPG7/kT9zU1F5oIsoslggCLi4ujX79+vPDCCwBce+21vPHGG0F78Pz000/p3LkzV155JZ999pnV3smFoqKimDFjBuDcWhoVFcWsWbMAp7RGVFQU8+bNA+D48eNERUWxaNEiwClnEhUVxUcffQRc2J1VLVq0IDo6mlOnTtG7d2+aNGlCw4YN+c9//gM4txnfcsst3HDDDbRu3ZpbbrmFmJgYGjVqxLx58zh48CBdunTh6quv5uqrr+abb74BICYmhl69enHllVdSr149Fi5cyNNPP01sbCwNGjSge/fuyeK49957+fDDD5PGu3fvnhSDr7Fjx3LllVdSv359nn76aQCmTZvG1VdfTf369enSpUuqZzg9e/bkgw8+SBp/8cUXufLKK2nSpAnR0dFJ8zzwwAM0bdqUJ598ku+++45mzZrRsGFDrr32WjZt2sTZs2cZMWIE8+bNo0GDBsybNy/Nzy42Npa77rqLWrVqcdtttyVdZs4qu2sowMLDw4mJieHUqVNeh5JlX3zxBbfeeiu1atXi888/D5qH3EzOiYuLY9myZXTo0IExY8Zwww038M4773Ds2DGaNGmS9A16/fr1bNy4kUsuuQRwLuds2LABgG7dujFo0CCaN2/Ojh07aN++Pb///jvPPvssJUqU4Oeffwac0itdunRh8uTJScv66tOnD+PHj+fWW2/l+PHjrF69mnfffTfZPMuWLeM///kPa9eupUiRIkmFCDt37kzfvn0BGDZsGG+//TYPPfRQuvueGNvMmTMZOHAgH3/8MeDc3rt69WrCwsI4ceIEX331FeHh4SxfvpwhQ4awcOFCRo0axbp165g8eTIAQ4YMSfWze/PNNylSpAi///47Gzdu5Kqrrrqg31NKlggC4ODBgzz99NOMGjWKChUqMHv27KC/q2PlypXcfPPNVK9enc8//zzpH9jkPitXrkx6nT9//mTjRYoUSTZeokSJZOOlS5dONl6uXDm/tpn4rRycM4I+ffpw7bXXsmTJkqRr6mfOnGHHjh0AtG3bNs2/oeXLl/Pbb78ljZ84cYKYmBiWL1/O3Llzk6ZffPHF6cbUqlUr/vWvf3Hw4EEWLlxIly5dzqt5tXz5cnr16pV0t1tiTL/88gvDhg3j2LFjxMTE0L59+ww/g7vvvjvp56BBg5Km33HHHUlVd48fP859993Hn3/+iYhw7ty5VNf12WefpfrZffnllzz88MMA1KtXj3r16mUYlz8sEQTA8ePHWbhwIR06dOCOO+4I+iTw1Vdf8Y9//IPKlSuzfPlySpcu7XVIJpdJbCPwpaosXLiQyMjIZNPXrl2b7h1mCQkJrFmzhkKFCmU5rnvvvZdZs2Yxd+5cpk+f7vdyPXv25MMPP6R+/frMmDEjWXJMi+//ue9r330dPnw4119/PYsXL2bbtm1ERUWluq60PrtAsTaCbLJr166kBrJq1aqxfft27rjjDo+jujDlypVDRJKGli1bcvr0aQ4ePMill17qdXgmSLRv355XX3016caIH3/80a/l2rVrx6uvvpo0nphg2rZty5QpU5KmHz16FHDOetL6Zt2zZ08mTJgAQO3atc97v23btkyfPj2pDSDx0tDJkye57LLLOHfuHLNnz/Yr7sR2l3nz5tGsWbNU5zl+/DgVKlQASGrHAae/jJMnTyaNp/XZtWzZMum5o19++YWNGzf6FVtGLBFkkylTpvDYY48lFYkLRCNqygN04uDv6bu/Uqu/A2T6aUUT2oYPH865c+eoV68ederUYfjw4X4tN2nSJNatW0e9evWoXbt20hesYcOGcfToUerWrUv9+vVZsWIFAP369aNevXrnNRaDUzeqVq1a9OrVK9VtdejQgVtuuYXGjRvToEGDpEsxzz77LE2bNuW6666jZs2afsV99OhR6tWrx8SJExk/fnyq8zz55JMMHjyYhg0bJruL6Prrr+e3335LaixO67Pr378/MTEx1KpVixEjRtCoUSO/YsuIFZ3LgujoaOLi4pKKxO3bt48qVaoEbHvpXWJav349sbGxxMbGcvr06aTXKcf9eW/Tpk1pbifY/l5Cwe+//06tWrW8DiNXOn36NFdeeSXr168PqTvcUvubsKJzARAXF0ebNm2SGk+LFCmSrUlAVTl06BCbNm1KGtLjz90D+fLlo0iRIhQuXJjChQsne124cGFKlSpF4cKFM9yWMcFg+fLl9OnTh0GDBoVUErgQlggyKTo6mqpVqxIeHs57772X5SJxZ86cITo6OtkBf9OmTWzevDnpGihk3GPW4sWLUz24+47nz5/fr4br+fPnZ2mfjMkN2rRpw/bt270OIyhYIsiEtWvX0rx5c6ZPn06PHj1o0aKFX8upKrt37071YL9t27Zkl1sqVKhAZGQkd955J5GRkUnDFVdckW53j7feemuW988YE5osEfjh1KlTFC1alKuvvppnnnmGjh07pjrfyZMn2bx583kH+82bNyd7oKxo0aLUqFGDpk2bcu+99yYd7GvUqJEravaULVs2zQ5bjDF5jyWCDLzyyitMmTKFn376iWLFijF48GC2bdvG2rVrkw70iQf9PXv2JC0nIkRERBAZGUnLli2TfbsvX778BT1bkFMH6LzSYYsxxj8hlQgy0zXhoUOH2Lx5M0eOHKFkyZJ0796dv/76i+joaM6ePZs038UXX0xkZCRt27ZN9s2+WrVq2fJAjC87QBtjAiGkEkFa98fv37+fsWPHsmnTJv744w/Wr1/P33//nfR+/vz5iY2NJTIykptuuinpYB8ZGUnp0qWD/slhEzoC1U/zRRddRExMTLJpI0eOZNq0aZQpU4azZ88yfPjwpDIMJncJqUSQnqeffppy5coRGRlJpUqViIiI4JFHHiEyMpLKlSun21BrTLBI78tQIAwaNIjHH3+cP//8k0aNGnH77bef16m68Z4d3VzdunXjxRdfpEKFCqiqfcs3QWngwIGpVuL0R1p1bxo0aJBUpuFCVa9enSJFinD06FErU5ILWYkJ19KlS1mzZg1g/b8ak93Wr19P9erVLQnkUnZG4NqxY0fQdhZjTKKMvrmn9yXHnwqbmTV+/HimT5/O5s2bkzq7MblPQM8IRKSDiGwSkWgReTqV9wuKyDz3/bUiEhHIeNK6zbJs2bKWBIwJgEGDBvHrr7+ycOFC+vTpw5kzZ7wOyaQiYIlARMKAKUBHoDZwt4ikrAPbBziqqtWA8cDYQMUDsGrVKn799VdUldOnT7N161ZU1W7LNCEjvS9DgZRY4TNlD2EmdwjkGUETIFpVt6jqWWAu0CnFPJ2AxL+MD4DWEqAL9HFxcbRv3z6pd5/ChQsTERERiE0Zk2vt27cPVT1vyOqXodOnT1OxYsWkYdy4cefNM2LECMaNG0dCQkKWtmWyXyDbCCoAO33GdwFN05pHVeNE5DhQCjjkO5OI9AP6AVx++eUXFEx4eDizZs3KcpE4Y8z5/Dm4N2rUyCrb5lJBcdeQqk5V1caq2rhMmTIXvJ7mzZtz2WWXZWNkxhgT/AKZCHYDlXzGK7rTUp1HRMKBEsDhAMZkjDEmhUAmgu+B6iJSWUQKAHcBS1LMswS4z319O/B/al1gGZNp9m9jEl3I30LAEoGqxgEPAp8CvwPzVfVXERklIre4s70NlBKRaOBR4LxbTI0x6StUqBCHDx+2ZGBQVQ4fPpzpgpfWZ7ExQe7cuXPs2rXL7tE3gPPFoGLFiufVdLI+i43Jw/Lnz0/lypW9DsMEsaC4a8gYY0zgWCIwxpgQZ4nAGGNCXNA1FovIQWD7BS5emhRPLYcA2+fQYPscGrKyz1eoaqpP5AZdIsgKEVmXVqt5XmX7HBpsn0NDoPbZLg0ZY0yIs0RgjDEhLtQSwVSvA/CA7XNosH0ODQHZ55BqIzDGGHO+UDsjMMYYk4IlAmOMCXF5MhGISAcR2SQi0SJyXkVTESkoIvPc99eKSETOR5m9/NjnR0XkNxHZKCJfiMgVXsSZnTLaZ5/5uoiIikjQ32rozz6LSFf3d/2riMzJ6Rizmx9/25eLyAoR+dH9+77Rizizi4i8IyIHROSXNN4XEZnkfh4bReSqLG80tf5Lg3kAwoC/gCpAAeAnoHaKef4FvOG+vguY53XcObDP1wNF3Nf9Q2Gf3fmKAV8Ca4DGXsedA7/n6sCPwMXu+KVex50D+zwV6O++rg1s8zruLO5zS+Aq4Jc03r8RWAYIcA2wNqvbzItnBE2AaFXdoqpngblApxTzdALedV9/ALQWEcnBGLNbhvusqitU9bQ7uganx7hg5s/vGeBZYCyQF2o0+7PPfYEpqnoUQFUP5HCM2c2ffVaguPu6BLAnB+PLdqr6JXAknVk6ATPVsQYoKSJZ6oM3LyaCCsBOn/Fd7rRU51GnA53jQKkciS4w/NlnX31wvlEEswz32T1lrqSqn+RkYAHkz++5BlBDRL4RkTUi0iHHogsMf/Z5JNBDRHYBS4GHciY0z2T2/z1D1h9BiBGRHkBjoJXXsQSSiOQDxgE9PQ4lp4XjXB6Kwjnr+1JErlTVY55GFVh3AzNU9RURaQa8JyJ1VTXB68CCRV48I9gNVPIZr+hOS3UeEQnHOZ08nCPRBYY/+4yItAGGAreo6t85FFugZLTPxYC6wEoR2YZzLXVJkDcY+/N73gUsUdVzqroV2IyTGIKVP/vcB5gPoKrfAoVwirPlVX79v2dGXkwE3wPVRaSyiBTAaQxekmKeJcB97uvbgf9TtxUmSGW4zyLSEHgTJwkE+3VjyGCfVfW4qpZW1QhVjcBpF7lFVYO5n1N//rY/xDkbQERK41wq2pKTQWYzf/Z5B9AaQERq4SSCgzkaZc5aAtzr3j10DXBcVfdmZYV57tKQqsaJyIPApzh3HLyjqr+KyChgnaouAd7GOX2MxmmUucu7iLPOz31+CbgIWOC2i+9Q1Vs8CzqL/NznPMXPff4UaCcivwHxwBOqGrRnu37u82PANBEZhNNw3DOYv9iJyPs4yby02+7xDJAfQFXfwGkHuRGIBk4DvbK8zSD+vIwxxmSDvHhpyBhjTCZYIjDGmBBnicAYY0KcJQJjjAlxlgiMMSbEWSIwnnCrgc7yGQ8XkYMi8rGXcWWWiGxz79dHRFZnMG9PESmfyfVHpFWF0ov1mLzJEoHxyimgrogUdsfbksWnI7OL+7R5pqnqtRnM0hPIVCIwJidYIjBeWgr8w319N/B+4hsiUtSty/6dW2e+kzs9QkS+EpH17nCtOz1KRFaKyAci8oeIzE6toqw7z0QR2SAiv4hIE3f6SBF5T0S+wXnYsIyILBSR793hOne+UiLymVvr/y2cUsCJ647xef2UiPwsIj+JyAsicjtOjafZ7rYLi0gjEVklIj+IyKeJFSTd6T+JyE/AgNQ+OBGZKyL/8BmfISK3p/X5pFi2p4hM9hn/WESi3NftRORbd9kFInJRur9Bkzd4XXvbhtAcgBigHk4Z8ELABpynKT92338O6OG+LolTM6coUAQo5E6vjvN0Ke6yx3HqruQDvgWap7LdlcA093VL3JrvOBUsfwAKu+NzEpcHLgd+d19PAka4r/+B8yRr6cR9cn92BFbzv/4fLvHZdmP3dX53njLu+J04T80CbARauq9fIpW69MBtwLvu6wI41SgLp/P5RPjsa09gss+6PnY/v9I4fTcUdac/lbivNuTtIc+VmDDBQ1U3itM73N04Zwe+2gG3iMjj7nghnAPyHmCyiDTAKaFQw2eZ71R1F4CIbMA5+H2dyqbfd7f/pYgUF5GS7vQlqhrrvm4D1PY5qSjufjtuCXR2l/9ERI6msv42wHR1+39Q1dRqy0fiFMX73N1GGLDXjaWkOjXpAd7DSSwpLQMmikhBoAPwparGikiJdD6fjFyD07HLN25MBXASqsnjLBEYry0BXsb5RurbJ4QAXVR1k+/MIjIS2A/Ux/nm79vhjG9F1XjS/vtOWVclcfyUz7R8wDWqmqxDm1SuNl0oAX5V1WYp1l8yjfmTUdUzIrISaI9zNjHXfWsQaX8+bF7v2QAAAX1JREFUieJIflm4kE9Mn6vq3X7ug8kjrI3AeO0d4N+q+nOK6Z8CDyVe5xeneio4JcP3qlNr/h6cb9KZdae7zuY4lRuPpzLPZ/h0cOJ+wwbn0kk3d1pH4OJUlv0c6CUiRdz5LnGnn8Qpjw2wCSgjTv18RCS/iNRRp9+AY25sAN3T2Y95OAXHWgD/daf58/lsAxqISD4RqYTTCxg4FVqvE5FqbkxFRSQzZxQmSFkiMJ5S1V2qOimVt57FuY6+UUR+dccBXgPucxtSa5L8W7y/zojIj8AbOLXsU/Mw0FiczsF/Ax5wp/8baOnG1BmnBHLKffovzpnOOvcSVeLlrRnAG+60MJwS6GPdfdkAJDbs9gKmuPOldwryGU4HQ8vV6cYR/Pt8vgG2Ar/htHmsd+M+iNN+8L6IbMS5LFQzne2bPMKqj5qQ4l5OeVyDu18CY7KVnREYY0yIszMCY4wJcXZGYIwxIc4SgTHGhDhLBMYYE+IsERhjTIizRGCMMSHu/wEzgJ7ejpbkKgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thresholds:  {'mean': 0.323651, 'roc': 0.433214, 'youden': 0.345693, 'none': 0.5, 'tuned': 0.343}\n",
            "threshold used:  0.343\n",
            "mean of thresholded prediction:  0.6688\n",
            "median of thresholded prediction:  1.0\n",
            "percentage kept:  0.33120000000000005\n",
            "cost matrix [[tn, fp], [fn, tp]]:  [[0. 1.]\n",
            " [2. 0.]]\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen1\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.3296588649113277\n",
            "mean of thresholded prediction:  0.6451612903225806\n",
            "percentage of applicants kept (cutoff based on threshold):  0.3548387096774194\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  70\n",
            "length of gen_score_reject_idx: 147\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.036\n",
            "no. of AL-accepted instances: 7\n",
            "al_idx[temp_gen_AL_select_idx] [1688 1095 1007 2081 1586  621 1991]\n",
            "gen_AL_select_idx2 [621, 1095, 1145, 1688, 1991, 2081, 2111]\n",
            "len gen_AL_select_idx 7\n",
            "len accept_idx before round update 311\n",
            "len accept_idx after round update 318\n",
            "len gen_score_reject_idx after round update 140\n",
            "mean of gen 1 TEST prediction: 0.3496997064414164\n",
            "gen 1/10 | # applicants = 217 | score accepts = 70 | AL selects = 7 | total accepts = 318 | unlabelled = 2040 |  time = 0.27 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen2\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.34544574620418755\n",
            "mean of thresholded prediction:  0.6774193548387096\n",
            "percentage of applicants kept (cutoff based on threshold):  0.32258064516129037\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  63\n",
            "length of gen_score_reject_idx: 154\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.032\n",
            "no. of AL-accepted instances: 6\n",
            "al_idx[temp_gen_AL_select_idx] [ 340 2077 1564 1608  575   14]\n",
            "gen_AL_select_idx2 [14, 340, 575, 1564, 1608, 2077]\n",
            "len gen_AL_select_idx 6\n",
            "len accept_idx before round update 381\n",
            "len accept_idx after round update 387\n",
            "len gen_score_reject_idx after round update 148\n",
            "mean of gen 2 TEST prediction: 0.3433576246556933\n",
            "gen 2/10 | # applicants = 217 | score accepts = 63 | AL selects = 6 | total accepts = 387 | unlabelled = 1823 |  time = 0.30 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen3\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.33775220915633986\n",
            "mean of thresholded prediction:  0.631336405529954\n",
            "percentage of applicants kept (cutoff based on threshold):  0.36866359447004604\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  72\n",
            "length of gen_score_reject_idx: 145\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.037\n",
            "no. of AL-accepted instances: 8\n",
            "al_idx[temp_gen_AL_select_idx] [1767  510  449  980  380 1876 1917 1241]\n",
            "gen_AL_select_idx2 [380, 449, 510, 980, 1241, 1767, 1876, 1917]\n",
            "len gen_AL_select_idx 8\n",
            "len accept_idx before round update 459\n",
            "len accept_idx after round update 467\n",
            "len gen_score_reject_idx after round update 137\n",
            "mean of gen 3 TEST prediction: 0.338037531464626\n",
            "gen 3/10 | # applicants = 217 | score accepts = 72 | AL selects = 8 | total accepts = 467 | unlabelled = 1606 |  time = 0.33 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen4\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.3392835636320821\n",
            "mean of thresholded prediction:  0.6036866359447005\n",
            "percentage of applicants kept (cutoff based on threshold):  0.3963133640552995\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  77\n",
            "length of gen_score_reject_idx: 140\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.04\n",
            "no. of AL-accepted instances: 8\n",
            "al_idx[temp_gen_AL_select_idx] [1616  286 2117  702 2140 1483  874   12]\n",
            "gen_AL_select_idx2 [12, 286, 702, 874, 1483, 1616, 2117, 2140]\n",
            "len gen_AL_select_idx 8\n",
            "len accept_idx before round update 544\n",
            "len accept_idx after round update 552\n",
            "len gen_score_reject_idx after round update 132\n",
            "mean of gen 4 TEST prediction: 0.32932745103381\n",
            "gen 4/10 | # applicants = 217 | score accepts = 77 | AL selects = 8 | total accepts = 552 | unlabelled = 1389 |  time = 0.45 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen5\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.3334475618523663\n",
            "mean of thresholded prediction:  0.6129032258064516\n",
            "percentage of applicants kept (cutoff based on threshold):  0.3870967741935484\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  76\n",
            "length of gen_score_reject_idx: 141\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.039\n",
            "no. of AL-accepted instances: 8\n",
            "al_idx[temp_gen_AL_select_idx] [1163 1150 1068 1021 1016  929  708   13]\n",
            "gen_AL_select_idx2 [13, 708, 929, 1016, 1021, 1068, 1150, 1163]\n",
            "len gen_AL_select_idx 8\n",
            "len accept_idx before round update 628\n",
            "len accept_idx after round update 636\n",
            "len gen_score_reject_idx after round update 133\n",
            "mean of gen 5 TEST prediction: 0.3225585143842687\n",
            "gen 5/10 | # applicants = 217 | score accepts = 76 | AL selects = 8 | total accepts = 636 | unlabelled = 1172 |  time = 0.48 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen6\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.3376419899109271\n",
            "mean of thresholded prediction:  0.5714285714285714\n",
            "percentage of applicants kept (cutoff based on threshold):  0.4285714285714286\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  84\n",
            "length of gen_score_reject_idx: 133\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.043\n",
            "no. of AL-accepted instances: 9\n",
            "al_idx[temp_gen_AL_select_idx] [2091 2293 1553 1213  313 1386  599 1363 2121]\n",
            "gen_AL_select_idx2 [297, 313, 599, 1386, 2091, 2096, 2101, 2121, 2293]\n",
            "len gen_AL_select_idx 9\n",
            "len accept_idx before round update 720\n",
            "len accept_idx after round update 729\n",
            "len gen_score_reject_idx after round update 124\n",
            "mean of gen 6 TEST prediction: 0.31088248703329346\n",
            "gen 6/10 | # applicants = 217 | score accepts = 84 | AL selects = 9 | total accepts = 729 | unlabelled = 955 |  time = 0.66 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen7\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.33389832886978377\n",
            "mean of thresholded prediction:  0.5668202764976958\n",
            "percentage of applicants kept (cutoff based on threshold):  0.43317972350230416\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  85\n",
            "length of gen_score_reject_idx: 132\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.043\n",
            "no. of AL-accepted instances: 9\n",
            "al_idx[temp_gen_AL_select_idx] [1387 2261 1030 2233 2211 1922  914 2334 1715]\n",
            "gen_AL_select_idx2 [914, 1030, 1387, 1715, 1922, 2211, 2233, 2261, 2334]\n",
            "len gen_AL_select_idx 9\n",
            "len accept_idx before round update 814\n",
            "len accept_idx after round update 823\n",
            "len gen_score_reject_idx after round update 123\n",
            "mean of gen 7 TEST prediction: 0.30397005600176946\n",
            "gen 7/10 | # applicants = 217 | score accepts = 85 | AL selects = 9 | total accepts = 823 | unlabelled = 738 |  time = 1.16 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen8\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.3015527949977545\n",
            "mean of thresholded prediction:  0.4377880184331797\n",
            "percentage of applicants kept (cutoff based on threshold):  0.5622119815668203\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  110\n",
            "length of gen_score_reject_idx: 107\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.056\n",
            "no. of AL-accepted instances: 12\n",
            "al_idx[temp_gen_AL_select_idx] [2122  678 2443 1192 2345  545 1601 1908 1701  647 2202 1283]\n",
            "gen_AL_select_idx2 [356, 545, 647, 678, 1192, 1283, 1601, 1701, 1908, 2122, 2202, 2443]\n",
            "len gen_AL_select_idx 12\n",
            "len accept_idx before round update 933\n",
            "len accept_idx after round update 945\n",
            "len gen_score_reject_idx after round update 95\n",
            "mean of gen 8 TEST prediction: 0.307371875679403\n",
            "gen 8/10 | # applicants = 217 | score accepts = 110 | AL selects = 12 | total accepts = 945 | unlabelled = 521 |  time = 1.36 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen9\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.2961393431555562\n",
            "mean of thresholded prediction:  0.4377880184331797\n",
            "percentage of applicants kept (cutoff based on threshold):  0.5622119815668203\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  110\n",
            "length of gen_score_reject_idx: 107\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.056\n",
            "no. of AL-accepted instances: 12\n",
            "al_idx[temp_gen_AL_select_idx] [ 401 1831 2137  455  546 1974  759 1649 1769  658  368 1099]\n",
            "gen_AL_select_idx2 [368, 401, 455, 546, 658, 759, 1099, 1649, 1769, 1831, 1974, 2137]\n",
            "len gen_AL_select_idx 12\n",
            "len accept_idx before round update 1055\n",
            "len accept_idx after round update 1067\n",
            "len gen_score_reject_idx after round update 95\n",
            "mean of gen 9 TEST prediction: 0.3024302514980675\n",
            "gen 9/10 | # applicants = 217 | score accepts = 110 | AL selects = 12 | total accepts = 1067 | unlabelled = 304 |  time = 1.79 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 0, gen10\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.29691279075834875\n",
            "mean of thresholded prediction:  0.43317972350230416\n",
            "percentage of applicants kept (cutoff based on threshold):  0.5668202764976958\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  111\n",
            "length of gen_score_reject_idx: 106\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.057\n",
            "no. of AL-accepted instances: 12\n",
            "al_idx[temp_gen_AL_select_idx] [ 799  849 2419 2478 1952 1569 1593 1611 1650 1834 1329   32]\n",
            "gen_AL_select_idx2 [32, 799, 849, 1329, 1569, 1593, 1611, 1650, 1834, 1952, 2419, 2478]\n",
            "len gen_AL_select_idx 12\n",
            "len accept_idx before round update 1178\n",
            "len accept_idx after round update 1190\n",
            "len gen_score_reject_idx after round update 94\n",
            "mean of gen 10 TEST prediction: 0.29684551252140173\n",
            "gen 10/10 | # applicants = 217 | score accepts = 111 | AL selects = 12 | total accepts = 1190 | unlabelled = 87 |  time = 1.89 min\n",
            "    generation  gen_internal_cost  gen_internal_cpl  total_internal_cost  \\\n",
            "0            0              156.0          0.647303                156.0   \n",
            "1            1               94.0          0.433180                250.0   \n",
            "2            2               96.0          0.442396                346.0   \n",
            "3            3              103.0          0.474654                449.0   \n",
            "4            4               92.0          0.423963                541.0   \n",
            "5            5               99.0          0.456221                640.0   \n",
            "6            6               93.0          0.428571                733.0   \n",
            "7            7               90.0          0.414747                823.0   \n",
            "8            8              109.0          0.502304                932.0   \n",
            "9            9               83.0          0.382488               1015.0   \n",
            "10          10               84.0          0.387097               1099.0   \n",
            "\n",
            "    total_internal_cpl  model_internal_cost  model_internal_cpl  \\\n",
            "0             0.647303                  0.0                 NaN   \n",
            "1             0.545852                 94.0            0.433180   \n",
            "2             0.512593                190.0            0.437788   \n",
            "3             0.503363                293.0            0.450077   \n",
            "4             0.487827                385.0            0.443548   \n",
            "5             0.482655                484.0            0.446083   \n",
            "6             0.475049                577.0            0.443164   \n",
            "7             0.467614                667.0            0.439105   \n",
            "8             0.471421                776.0            0.447005   \n",
            "9             0.462625                859.0            0.439836   \n",
            "10            0.455827                943.0            0.434562   \n",
            "\n",
            "    external_cost  external_cpl  \n",
            "0           269.0        0.4304  \n",
            "1           275.0        0.4400  \n",
            "2           301.0        0.4816  \n",
            "3           282.0        0.4512  \n",
            "4           275.0        0.4400  \n",
            "5           267.0        0.4272  \n",
            "6           255.0        0.4080  \n",
            "7           264.0        0.4224  \n",
            "8           255.0        0.4080  \n",
            "9           244.0        0.3904  \n",
            "10          244.0        0.3904  \n",
            "\n",
            " Finished 10 generations in 1.89 minutes \n",
            " \n",
            " \n",
            " -------------------------------------------------\n",
            "\n",
            "\n",
            "\n",
            "---------------------------------------------------\n",
            "PARAMETERS:\n",
            "\n",
            "key:  spal\n",
            "rounds:  5\n",
            "iteration:  1\n",
            "max_gens:  10\n",
            "sample_size:  217\n",
            "init_sample:  241\n",
            "score_acc_rate:  0.9\n",
            "AL_acc_rate:  0.1\n",
            "seed:  30\n",
            "weights:  False\n",
            "w_factor:  9.0\n",
            "do_thres:  tuned\n",
            "tuned_threshold:  0.343\n",
            "AL_params:  {'strategy_name': 'QueryInstanceSPAL', 'kernel': 'rbf', 'mu': 0.01, 'gamma': 0.01, 'rho': 10, 'lambda_init': 0.01, 'lambda_pace': 0.1}\n",
            "CLF_params:  {'tol': 0.0001, 'solver': 'liblinear', 'penalty': 'l1', 'max_iter': 50, 'C': 0.1}\n",
            "cost_matrix:  None\n",
            "dataset_name:  bene1_nobins\n",
            "\n",
            "sanity check of train-test splits: (2498,) (625,)\n",
            "strategy SPAL, round 1: running loop for 10 generations\n",
            "classifier:  LogisticRegression(C=0.1, max_iter=50, penalty='l1', random_state=30,\n",
            "                   solver='liblinear')\n",
            "mean of initial TEST prediction: 0.402765727921693\n",
            "y mean of TEST DATA: 0.3328\n",
            "y mean of INITIALLY ACCEPTED DATA: 0.3651452282157676\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZyNdf/H8ddnxjLIEsbIFrKTPSI3SkRlaVNJpbpJ8QsRWrTSniJKlCUqkiWl1V2qW5GxVoqGLMNgbIOZwSyf3x/nzNxjzHLGnDPXnHM+z8fjesxZruV9nZm5Pue6vtf1vURVMcYYE7xCnA5gjDHGWVYIjDEmyFkhMMaYIGeFwBhjgpwVAmOMCXJWCIwxJshZITCIiIpIHffjaSIyzv24s4hE+3jZd4jIN75cRjbLnS0i4wtoWTeIyB4ROSkiLQpimU4RkZ0icrX78WMi8u55zucPEens1XAmW1YIAoSI9BORSPfGJkZEvhSRDnmdj6oOVtXnfJSxprvoFMmwvA9UtZsvluctGQvleXoVGKqqF6jqBh/MP20+T4vIvPzOx1tU9XlV/Xdu42VVlFW1saqu9Fk4cxYrBAFARB4G3gCeByKAGsBbQO8CzhFakMvzIxcDfzgdIq8yFmwT4FTVBj8egLLASeCWHMZpA/wCHANigClAsQzvK1DH/Xg2MN79uDMQDTwGHAJ2AndkmG428DbwBRAPXA1cB2wAjgN7gKczjL/bvayT7qEdMAD4b4Zx2gNrgTj3z/YZ3lsJPAesAk4A3wAVs1lnT7KPz/B8IBAFHAGWAVXcr//ozhzvznxrFssKAZ4AdgEHgffdv5fi7mnSpt+exbRZzh+4Htjo/p39DDTNMM0YYK/7M9gKdAG6A2eAJPd8NmXzuewEHgW2AEeBWUBYps9sDLAfmOtet7HAduAw8DFQPsP87nSv92Hgcff8r3a/9zQwL8O4Hdzrcsz9tzEAGOTOfMad+7MMOdPmUxzXF5197uENoHimzCPdn30McI/T/5f+NjgewIZ8/gJdG4BkoEgO47QCLgeKADWBP4HhGd7PqRAkAxPd/4yd3Bus+hnGjQOucG8wwtzTXOp+3hQ4APRxj1/TvawiGZY9AHchAMq7N053urPe7n5ewf3+SvcGqR5Qwv38xWzW2ZPsaet5Fa5i0dI97pvAj1l9Ptks615cRaQ2cAGwGJibh+nPeh9o4d6otQVCgbvdG8biQH1cG9EqGT7TS9yPnybDhjebZe0Efgequz/vVVn8vl9yL6sEMAxYDVRzv/YO8JF7/Ea4Nt4d3e9NdE9/TiHAtVd0wv07LQpUAJpn/l1kypk2n2fdGSoB4biKyXOZMj/rnu+1QAJwodP/m/402KEh/1cBOKSqydmNoKrrVHW1qiar6k5c/8yd8rCMcap6WlV/AJYDfTO896mqrlLVVFU9paorVfU39/PNwEd5WNZ1wN+qOted9SPgL6BnhnFmqeo2VU3E9e20eT6yp7kDmKmq61X1NK5vzO1EpKaHue8AJqrqDlU96Z7+tnwcWhkEvKOqa1Q1RVXnAKdxFfMUXBvdRiJSVFV3qur2PM5/iqruUdUjwARcG+c0qcBT7s8sERgMPK6q0e7P5mngZve63Qx8rqo/ut8b554+K/2AFar6kaomqephVd3oYd47gGdV9aCqxgLP4PqykCbJ/X6Sqn6BqzjV93DeBmsjCASHgYo5bXREpJ6IfC4i+0XkOK62hIoezv+oqsZneL4LqJLh+Z5My2orIt+LSKyIxOHakHi6rCru+We0C6ia4fn+DI8TcH0DP9/sWS7XvTE/nGm5OcmcexeuPZoID6fP7GJgpIgcSxtwfYOvoqpRwHBcG+SDIjJfRLJap5xk/J1l/kxiVfVUpixLMuT4E1cxinBPlz4v92d9OJtlVse1N3c+svp8M2Y+nOmLUG5/FyYTKwT+7xdc3xb75DDO27i+WddV1TK4jpuLh/O/UERKZXheA9dx2jSZu6/9ENcx9uqqWhaYlmFZuXV1uw/XhiejGriOh5+P3LJnuVz3NBXysNzMuWvgOlxxIE9p/2cPMEFVy2UYSrr3kFDVD1W1g3uZiutQDuT++aapnilrTr/PPUCPTFnCVHUvruPx6fMSkZK4Prfs1umSbN7L699Fdr9Hc56sEPg5VY0DngSmikgfESkpIkVFpIeIvOwerTSuxtuTItIAeCCPi3lGRIqJyL9wNWIuzGHc0sARVT0lIm1wHRJIE4vr0EHtbKb9AqjnPhW2iIjcius49Od5zJvX7B8B94hIcxEpjmuPaY37MBq4NujZZU6bfoSI1BKRC9zTL8jpcF0mmec/Axjs3rsSESklIteJSGkRqS8iV7lzngIS+d/hmANATRHJ7f96iIhUE5HyuBp4F+Qw7jRggohcDCAi4SKSdjbaJ8D1ItJBRIrhOk6f3bI/AK4Wkb7u320FEUk7rOfJ5/uEe9kVcf29F5rTZAOBFYIAoKqvAQ/jOnMlFte3r6HAUvcoo3BtkE/g2sjk9I+f2X5cDbb7cP0zD1bVv3IY/0HgWRE5gesf9uMMORNwHZNe5T7UcHmm9TiMa2M9EtchhtHA9ap6KA9585xdVVfgOr69CNe33EuA2zKM8jQwx505qzaGmbjOsPkR+AfXBvr/8pDzrPmraiSus5imuPNH4WpUB1f7wIu4Grf342pAfdT9XlqROywi63NY3oe4zrjagetwTU4X1k3CtYf3jft3uhpXIzaq+gcwxD2/GHfWLC9AVNXduBpyR+I6M2sj0Mz99nu42jyOicjSLCYfD0QCm4HfgPW5ZDZ5JKp2YxoTeNxXpc5T1WpOZylMRGQn8G938TMGsD0CY4wJelYIjDEmyNmhIWOMCXK2R2CMMUHO7zqVqlixotasWdPpGMYY41fWrVt3SFXDs3rP7wpBzZo1iYyMdDqGMcb4FRHJfNV+Ojs0ZIwxQc4KgTHGBDkrBMYYE+SsEBhjTJCzQmCMMUHOZ4VARGaKyEER+T2b90VEJotIlIhsFpGWvspijDEme77cI5iN6zaK2ekB1HUPg3D1mW+MMaaA+awQqOqPuLqbzU5v4H11WQ2UE5GLfJXHGGP8VXx8PDt37vTZ/J1sI6jK2bfMiyabWwOKyCARiRSRyNjY2AIJZ4wxhcX48eO58cYbSU3N7pbQ+eMXjcWqOl1VW6tq6/DwLK+QNsaYgHLs2DH27nXdLXXMmDG88cYbhIT4ZpPtZCHYy9n3Tq3G+d+b1hhjAkZKSgrt27dnwIABAJQrV46OHTv6bHlO9jW0DBgqIvNx3fouTlVjHMxjjDGOOn78OGXKlCE0NJQJEyZQvXr13CfyAl+ePvoR8AtQX0SiReQ+ERksIoPdo3yB656pUbjuo/ugr7IYY0xht3nzZmrXrs2yZcsAuOGGG2jdunWBLNtnewSqensu7yuuG18bY0zQUlVEhAYNGtCrVy/q1KlT4Bn8orHYGGMC0UcffUS7du04deoUxYoVY+bMmTRq1KjAc1ghMMYYh1x44YWULl2a48ePO5rD7+5Z3Lp1a7Ub0xhj/FFqaiqvvvoq5cqVY9CgQcD/Dg35moisU9UsGx1sj8AYYwqIiPDdd9/x3//+96zXnGaFwBhjfOj06dNMmDCBw4cPIyIsXryYOXPmOB3rLFYIjDHGh/7++2+eeeYZFi9eDEDJkiULxV5ARn5383pjjCnsTp48yffff0/Pnj1p0qQJf/31F7Vr13Y6VrZsj8AYY7wsrZO4PXtc/WoW5iIAVgiMMcYrjh49SnR0NACPPvoo33//fYF1EZFfdmjIGGPyKSUlhSuuuIKqVavy7bffUrZsWTp06OB0LI9ZITDGmPMUFxdH2bJlCQ0N5fnnn6dGjRpORzovdmjIGGPOw+bNm6lVqxZLly4FoE+fPrRs6Z+3XrdCYIwxeZB2l7CGDRty00030aBBA4cT5Z8VAmOM8dC8efNo27Ytp06domjRosyYMcMKgTHGBJPw8HAqVKjAiRMnnI7iVdZYbIwx2UhJSeGVV16hXLlyDB48mGuuuYZu3boVuiuD88v2CIwxJhshISH88MMP/PLLL+mvBVoRACsExhhzllOnTvHss88W6k7ivM0KgTHGZBAVFcX48ePTTwstUaKEw4l8zwqBMSbonTx5Mn3D36RJE7Zu3cp9993ncKqCY4XAGBP0JkyYwC233JLeV1CtWrUcTlSwrBAYY4LSkSNH0nsHHTt2LD/++CPVqlVzOJUzrBAYY4JOSkoK7du359577wWgbNmytGvXzuFUzrHrCIwxQePYsWOUK1eO0NBQXnrpJS6++GKnIxUKtkdgjAkKmzZtOquTuN69e9O8eXOHUxUOVgiMMQEtrZO4Ro0a0bdvXxo1auRwosLHCoExJmDNnTuXNm3apHcS984771CvXj2nYxU6VgiMMQErIiKCSpUqBVwncd5mjcXGmICRkpLCiy++SPny5XnggQfo1q0b3bp1czpWoWeFwBgTMEJCQli1ahUXXXSR01H8ih0aMsb4tcTERJ566ikOHTqU3knce++953Qsv+LTQiAi3UVkq4hEicjYLN6vISLfi8gGEdksItf6Mo8xJvDs2LGDF154gWXLlgEQFhbmcCL/47NCICKhwFSgB9AIuF1EMp+39QTwsaq2AG4D3vJVHmNM4Dh+/DhLliwBoHHjxvz999/pVwmbvPPlHkEbIEpVd6jqGWA+0DvTOAqUcT8uC+zzYR5jTIB4/vnn6du3b3oncXaFcP74shBUBfZkeB7tfi2jp4H+IhINfAH8X1YzEpFBIhIpIpGxsbG+yGqMKeQOHz7M7t27AXjsscf46aefgraTOG9zurH4dmC2qlYDrgXmisg5mVR1uqq2VtXW4eHhBR7SGOOstE7i0u4RUKZMGS6//HKHUwUOX54+uheonuF5NfdrGd0HdAdQ1V9EJAyoCBz0YS5jjJ84cuQI5cuXJzQ0lFdffdUOAfmIL/cI1gJ1RaSWiBTD1Ri8LNM4u4EuACLSEAgD7NiPMYZNmzZRu3ZtFi9eDEDPnj1p2rSpw6kCk88KgaomA0OBr4E/cZ0d9IeIPCsivdyjjQQGisgm4CNggKqqrzIZYwq/lJQUwNVJXL9+/WzjXwDE37a7rVu31sjISKdjGGN8YM6cOUyaNImff/7ZrgfwMhFZp6qts3rP6cZiY4xJV6VKFapWrcrJkyedjhJUrK8hY4xjUlJSeP755ylfvjxDhgyha9eudO3a1elYQccKgTHGMSEhIaxevdo6iXOYHRoyxhSohIQExo0bd1Ynce+++67TsYKaFQJjTIH6559/ePnll/nss88AKF68uMOJTK6FQESuEJFS7sf9RWSiiNhVHcYYj8XFxfHJJ58Ark7ioqKiuOeeexxOZdJ4skfwNpAgIs1wnfe/HXjfp6mMMQHlhRdeoF+/fumdxFWvXj2XKUxB8qQQJLsv8uoNTFHVqUBp38Yyxvi72NhYdu3aBbg6iVu1apV1EldIeXLW0AkReRS4E/iXu1O4or6NZYzxZykpKXTo0IHq1auzYsUKypQpw2WXXeZ0LJMNTwrBrUA/4F5V3S8iNYBXfBvLGOOPDh8+TIUKFQgNDeW1116jZs2aTkcyHsj10JCq7gcWAWlN+4eAJb4MZYzxPxs3bjyrk7jrr7+eJk2aOJzKeMKTs4YGAp8A77hfqgos9WUoY4z/SOskrnHjxvTv359mzZo5nMjklSeNxUOAK4DjAKr6N1DJl6GMMf5h1qxZtG7dmsTERIoWLcrUqVO55JJLnI5l8siTQnDafc9hAESkCK57DRtjglyNGjW4+OKLSUhIcDqKyQdPGot/EJHHgBIi0hV4EPjMt7GMMYVRSkoKzz77LOHh4QwdOpQuXbrQpUsXp2OZfPJkj2AsrruG/Qbcj+sm80/4MpQxpnAKCQlh3bp1/Pbbb05HMV7kyR5BH+B9VZ3h6zDGmMInPj6eCRMmMGLECMLDw1m8eDHFihVzOpbxIk/2CHoC20Rkrohc724jMMYEiV27djFx4kSWL18OYEUgAHlyHcE9QB1gIXA7sF1ErM9YYwLYsWPH+PjjjwHXvYOjoqIYMGCAs6GMz3jUDbWqJgFfAvOBdbgOFxljAtSLL75I//790zuJsz6CApsnF5T1EJHZwN/ATcC7QGUf5zLGFLCDBw+yc+dOAB5//HF++eUXKwBBwpPj/XcBC4D7VfW0j/MYYxyQkpLCFVdcwcUXX8yKFSsoXbo0rVq1cjqWKSC5FgJVvb0gghhjCl5sbCzh4eGEhoYyadIk6yQuSGV7aEhE/uv+eUJEjmcYTojI8YKLaIzxhQ0bNlC7du30O4dde+21NGrUyOFUxgnZ7hGoagf3T7sJjTEBJDk5mSJFinDppZdy77332iEg41Fj8VxPXjPGFH7vvfcerVq1IjExkSJFijBp0iRq1arldCzjME9OH22c8Yn7gjL7CmGMH6pZsyaXXHKJdRJnzpJTG8GjInICaJqxfQA4AHxaYAmNMectJSWFcePGMXnyZAC6dOnC4sWLqVChgsPJTGGSUxvBC8ALIvKCqj5agJmMMV4SEhLCpk2bqFq1qtNRTCGWbSEQkQaq+hewUERaZn5fVdf7NJkx5rycPHmS8ePH8/DDD1OpUiU++eQT6x/I5CinNoKH3T9fy2J41ZOZi0h3EdkqIlEiMjabcfqKyBYR+UNEPsxDdmNMFnbv3s0bb7zBl19+CTjbSVzlypURkXOGypWtc4LCRFR9c7MxEQkFtgFdgWhgLXC7qm7JME5d4GPgKlU9KiKVVPVgTvNt3bq1RkZG+iSzMf7q6NGjfP3119x2220A7Nu3jypVqjicCkQk2/d8te0xWRORdaraOqv3PDl99BYRKe1+/ISILBaRFh4stw0Qpao73Le6nA/0zjTOQGCqqh4FyK0IGGOy9tJLL3H33Xezd+9egEJRBIz/8OT00XGqekJEOgBXA+8B0zyYriqwJ8PzaPdrGdUD6onIKhFZLSLds5qRiAwSkUgRiYyNjfVg0cYEvgMHDvDPP/8Ark7iVq9ebY3C5rx4UghS3D+vA6ar6nLAWwcdiwB1gc647nUwQ0TKZR5JVaeramtVbR0eHu6lRRvjv1JSUujQoQMDBw4EoHTp0rRo4cmOujHn8qT30b0i8g6uY/0viUhxPCsge4HqGZ5Xc7+WUTSwxn2/g39EZBuuwrDWg/kbE3QOHjyY3knc5MmTC/VVwampqTm+f/DgQSpVqlRAaUxOPNmg9wW+Bq5R1WNAeeARD6ZbC9QVkVoiUgy4DViWaZyluPYGEJGKuA4V7fAsujHBZf369Wd1EtejRw8aNGjgcKrsvfpqzicX9unTh1OnThVQGpMTT25VmQBsB64RkaFAJVX9xoPpkoGhuIrIn8DHqvqHiDwrIr3co30NHBaRLcD3wCOqevg818WYgJScnAxA06ZN+fe//02bNm0cTpS7//73vzz22GPccsstpKamoqpnDQsXLuSXX37hvvvus7OHCoPMv6DMAzAM+B141j38BvxfbtP5amjVqpUaEyxmzJihTZo00YSEBKejeOzgwYNatWpVrVOnjsbFxWU73oQJExTQ5557rgDTBS8gUrPZrnrSRnAf0FZV4wFE5CXgF+BNH9QlY0wGl1xyCQ0aNCAxMZESJUo4HSdXqamp3HnnnRw6dIjVq1dTpkyZbMd99NFH+euvvxg3bhz169fnlltuKcCkJiNPCoHwvzOHcD/O/ioRY8x5S0lJ4YknniAiIoLhw4dz5ZVXcuWVVzody2MvvPACX3/9Ne+88w7NmzfPcVwRYcaMGWzfvp277rqLmjVrctlllxVQUpORJ43Fs4A1IvK0iDwDrMZ1LYExxstCQkL4888/+fvvv52OkmcrV67kySefpF+/fumnteamePHiLFmyhMqVK9OrVy/27NmT+0TG6zzqYsLd6VwHQIH/quoGXwfLjnUxYQLNiRMneO655xg1ahSVKlUiKSmJokWLOh0rTw4cOEDz5s0pW7YskZGRXHDBBXma/o8//qBdu3Zccskl/PTTT3me3uQuX11MZJxPpp/GGC+Ijo7mzTff5KuvvgLwuyKQkpJCv379iIuLY+HChee1EW/cuDELFixg8+bN3Hnnnbleg2C8y5O+hp4E5gAXAhWBWSLyhK+DGRPIjhw5wocfujrbbdiwITt27OCuu+5yONX5ee655/juu++YOnUql1566XnPp0ePHrz++ussXbqUxx57zIsJTa6yO50obQC2AmEZnpcAtuY2na8GO33UBIKxY8dqsWLFNDo62uko+fLNN9+oiOjdd9/tlfmlpqbqAw88oIDOmjXLK/M0LuRw+miubQQi8j1wg7quKsbdF9BiVb3Kh/UpW9ZGYPxVTEwMiYmJ1K5dm5MnT7J9+3aaNWvmdKzztm/fPpo3b06lSpVYs2YNpUqV8sp8k5KSuPbaa/nhhx9YsWIFHTt29Mp8g11ObQSeFIKlwGXAt7gai7sCv+LqJwhVfciraXNhhcD4o5SUFOrXr0/NmjVZsWKF03HyLTk5mS5durBu3TrWrl1Lw4YNvTr/Y8eOcfnllxMbG8uvv/7KJZdc4tX5B6OcCoEn1xEscQ9pVnojlDHBYP/+/URERBAaGsrUqVMLdSdxefHkk0/y448/MnfuXK8XAYBy5crx+eef07ZtW66//np++eUXypU7p2Ni4y3ZHTMqrIO1ERh/sW7dOi1ZsqQuWLDA6She9cUXXyig//73v32+rJUrV2rRokW1a9eumpSU5PPlBTJyaCPIy+mjxhgPJCUlAa5O4gYPHky7du0cTuQ9e/bs4c4776Rp06ZMnjzZ58vr1KkT77zzDt9++y3Dhg3z+fKClRUCY7xo+vTptGjRgoSEBIoUKcJrr71G9erVc5/QDyQlJXHrrbdy+vRpFi5cWGB9H91zzz088sgjvPXWW0yZMqVAlhlssi0EIjLX/dPKsDEeqlu3Lo0bNw7IfvYfe+wxfvnlF959913q1atXoMt+4YUX6NWrF8OGDUu/8M54T7ZnDbnvEXA18CWum8ecdUWxqh7xdbis2FlDpjBJSUnhscceo3LlyowYMcLpOD6zbNkyevfuzYMPPsjUqVMdyXDy5Ek6dOjAP//8w88//0zjxo0dyeGvzreLiWnAf4AGwLpMg22JjcHVSdy2bdvSbyIfiHbu3Mndd99Ny5YtmThxomM5LrjgAj777DNKlixJz549iY2NdSxLoMm2EKjqZFVtCMxU1dqqWivDULsAMxpTqBw/fpyRI0dy4MABRISFCxcWSMOpE86cOcOtt95KamoqH3/8McWLF3c0T/Xq1fn000+JiYnhhhtu4PTp047mCRSe3KryARFpJiJD3UPTgghmTGG1b98+3n77bb799lsAihTx5HIc/zR69Gh+/fVXZs2aVWgu6mrTpg1z5sxh1apVDBw4kOwObxvPedLp3EPAB0Al9/CBiPyfr4MZU5gcOnSIefPmAdCgQQP++ecf+vfv73Aq31q0aBGTJk1i2LBh3HjjjU7HOUvfvn159tlnmTt3Li+++KLTcfxfdhcYpA3AZqBUhuelgM25TeerwS4oM04YM2aMFi9eXPfu3et0lAIRFRWlZcqU0TZt2ujp06edjpOl1NRU7devnwK6aNEip+MUeuTzgjK7VaUJSvv27WPHjh0APPHEE6xdu5YqVao4nMr3Tp06Rd++fQkNDWXBggUUK1bM6UhZEhHee+89Lr/8cvr378+6deucjuS38nqryqexW1WaIJCSkkLHjh0ZNGgQ4DpjJT997fuThx9+mPXr1zNnzhxq1qzpdJwchYWFsXTpUsLDw+nVqxd79+51OpJf8qSxeCJwD3DEPdyjqm/4OpgxToiJiUFVCQ0N5a233mLatGlORypQCxYs4O2332bUqFH07NnT6TgeiYiI4PPPP+f48eP06tWL+Ph4pyP5HY/uWVyY2AVlxlciIyPp2LEjs2fPpm/fvk7HKXDbtm2jVatWNG3alJUrV/rdLTOXL19Or1696NOnDwsXLiQkxHrQychb9yw2JiCldRLXvHlzhgwZQvv27R1OVPASExO55ZZbKF68OPPnz/e7IgBw3XXX8eqrr7J48WJCQ0MRkbOGypUrOx2x0LI9AhPUpk2bxuTJk4mMjKRkyZJOxykwlStX5sCBA+e8Xq5cOY4ePepAIu9Q1Rz3BPxte+dNtkdgTDYaNmxIixYtgu4K1ayKALjuDObPROyExvPhyQVlN4rI3yISJyLHReSEiBwviHDGeFtycjKPPPIIr732GuDq7/6DDz7gwgsvdDiZMc7x5Nr4l4Geqvqnr8MY42uhoaFERUWltwsYYzw7NHTAioDxZ3FxcYwYMeKsTuLeeMPOgDYmjSeFIFJEFojI7e7DRDeKSOHqeMSYHMTExDB9+nRWrFgBBHYncZ4I9L2hiIiIPL1uPCsEZYAEoBvQ0z1c78nMRaS7iGwVkSgRGZvDeDeJiIpIli3axuRVbGws77//PuDqJG7nzp3ccccdDqdynqoycODAbN8PhI3l/v370/vQ2blzJ6GhoQwfPpz9+/c7Ha3wyq4TovwOQCiwHagNFAM2AY2yGK808COurita5zZf63TOeGLs2LFB1Umcpx577DEF9JlnnnE6SoHp37+/lipVSg8fPux0FEeRn07nRKSaiCwRkYPuYZGIVPOgxrQBolR1h6qeAeYDvbMY7zngJSDwbvJqClR0dDTbt28H4PHHH2fdunVB0Umcp9566y2ef/55Bg4cyLhx45yOU2BGjx5NfHy8Y7fY9Aeedjq3DKjiHj5zv5abqsCeDM+j3a+lE5GWQHVVXZ7TjERkkIhEikik3Z7OZCU5OZlOnTpx//33A65O4uyetv+zZMkShg4dyvXXX89bb70VVOfbX3rppVx33XVMnjyZhIQEp+MUSp4UgnBVnaWqye5hNhCe3wWLSAgwERiZ27iqOl1VW6tq6/DwfC/aBJC9e/eiqhQpUoR33nmH6dOnOx2p0Fm1ahX9+vWjTZs2zJ8/Pygby8eMGcOhQ4eYOXOm01EKJU8KwWER6S8ioe6hP3DYg+n2AtUzPK/mfi1NaaAJsFJEdgKXA8uswQ2c87kAACAASURBVNh4KjIykjp16rBgwQIArr76amrXtttpZ/Tnn3/Ss2dPqlevzueff06pUqWcjuSIDh060K5dO1577TWSk5OdjlPoeFII7gX6AvuBGOBmXN1S52YtUFdEaolIMeA2XIeYAFDVOFWtqKo1VbUmrsbiXqpqHQmZHJ05cwaAFi1a8NBDD9GxY0eHExVO+/bto3v37hQrVoyvvvqKihUrOh3JMSLCmDFj2LlzJx9//LHTcQodT+5HsEtVe6lquKpWUtU+qrrbg+mSgaHA18CfwMeq+oeIPCsivfIf3QSjt956i2bNmhEfH09oaCgvvfSSNQhn4fjx41x77bUcOXKEL774wvaUgJ49e9KwYUNefvnloO58LivZHiwUkdGq+rKIvAmc86mp6kO5zVxVvwC+yPTak9mM2znXtCboNWnShNatWwf8RVH5cebMGW688Ub++OMPli9fTsuWLZ2OVCiEhITwyCOPcO+99/L111/TvXt3pyMVGtl2Qy0iPVX1MxG5O6v3VXWOT5Nlw7qhDi7JycmMHj2aKlWqMGrUKKfjFHqpqancddddfPDBB8yZM4e77rrL6UiFypkzZ6hduzZ169bl+++/dzpOgTqvbqhV9TP3wwRVnZNxwHWlsTE+Fxoayq5du4iJiXE6il949NFH+eCDD3j++eetCGShWLFiPPzww6xcuZI1a9Y4HafwyO5KM/3flb/rPXmtoAa7sjjwHT16VP/v//5PY2JiVFU1OTnZ4UT+YdKkSQrogw8+qKmpqU7HKbSOHz+u5cqV0xtuuMHpKAWK87myWER6uNsHqorI5AzDbMDOvzI+c+DAAWbOnMl3330HuPYKTM4WLlzI8OHDueGGG5g8eXJQXTCWV6VLl2bIkCEsXbqUrVu3Oh2nUMjprKF9QCSurh/WZRiWAdf4PpoJJgcOHGD27NkA1K9fn507d9KvXz9nQ/mJH374gf79+9O+fXs++OADK5weeOihhyhevDivvPKK01EKhVzvWSwiZYB4VU1xPw8FiquqI+0E1lgcmMaOHcukSZPYvn27nQ6aB7///jsdOnTgoosuYtWqVZQvX97pSH7jwQcf5L333uOff/4Jir+5/N6z+BugRIbnJYAV3ghmgtuePXuIiooC4IknnmD9+vVB8Q/pLdHR0fTo0YOSJUvy1VdfWRHIo1GjRpGcnGw3KcKzQhCmqifTnrgfl/RdJBMM0jqJGzx4MODqJK5hw4YOp/Ifx44do0ePHsTFxfHll19y8cUXOx3J79SuXZu+ffsybdo0jh075nQcR3lSCOLdvYQCICKtgETfRTKBLDo6Or2TuBkzZjBjxgynI/md06dP06dPH7Zu3cqSJUto1qyZ05H81ujRozlx4gRvv/2201Ec5UkhGA4sFJGfROS/wAJcXUcYkydr1649q5O4Ll26UKtWLYdT+Ze0C8Z++OEHZs+eTZcuXZyO5NdatGhBt27dmDRpEqdOBe8tUTzpa2gt0AB4ABgMNFTVdb4OZgLH6dOnAWjZsiUPP/wwnTp1cjiR/xo1ahQff/wxr7zyip1V5SVjxozhwIEDzJnjSGcJhUKuZw0BiEgToBEQlvaaqr7vw1zZsrOG/MuUKVN48803Wb9+fdB2gewtEydOZOTIkQwbNozXX3/drhXwElWlTZs2HDt2jL/++itgT7/N11lDIvIU8KZ7uBJ4GbDeQ41HmjZtSrt27awP+HyaP38+I0eO5JZbbmHixIlWBLworYvqqKgoFi9e7HQcR3hyHcFvQDNgg6o2E5EIYJ6qdi2IgJnZHkHhlpyczMiRI6latSqjR492Ok5A+O677+jevTvt27fnq6++IiwsLPeJTJ6kpKTQsGFDypQpw9q1awOy0Ob3OoJEVU0Fkt0Xlx3k7DuPGZOuSJEi7Nu3j4MHDzodJSBs2rSJG264gXr16rF06VIrAj4SGhrKI488wrp169K7NgkmnhSCSBEpB8zA1cXEeuAXn6YyfuXo0aMMGTKE/fv3A67DGK+++qrDqfzfrl276NGjB2XKlOGrr76iXLlyTkcKaHfeeSeVK1fmxRdfdDpKgcuxEIhr/+gFVT2mqtOArsDdqurJrSpNkDh48CDvv/9+ev/ugdrYVpCOHDlCjx49SEhI4Msvv6RatWpORwp4YWFhDB8+nBUrVrBuXXCdGJljIXB3XfpFhuc7VXWzz1OZQm///v3MnDkT+F8ncbfffrvDqQJDYmIivXr1Yvv27Xz66ac0adLE6UhBY/DgwZQpU4aXX37Z6SgFypNDQ+tF5DKfJzF+ZdKkSQwZMoR9+/YBUKFCBYcTBYaUlBT69+/Pzz//zNy5c+2aiwJWtmxZBg8ezCeffML27dudjlNgPDlr6C+gLrATiAcE185CU5+ny4KdNeScnTt3cubMGerVq0d8fDzR0dHUr1/f6Vh+r3Llyhw4cOCc1yMiItLbXUzBiYmJoWbNmtx7770B1fVETmcN5XTP4hqqultEsuzNSlV3eTGjx6wQOCM5OZm6detSu3Zt/vOf/zgdJ6DkdKqiJxd8Gu8bNGgQ77//Prt27SIiIsLpOF5xvqePLoX0Df5EVd2VcfBFUFP47N69O72TuJkzZ6a3CxgTyEaNGsWZM2eYPHmy01EKRE6FIOPXlNq+DmIKn7Vr11KvXj3mz58PwJVXXmndHXtRYmIi48ePdzqGyUK9evW48cYbmTp1KsePH3c6js/lVAg0m8cmwKX1wtiyZUtGjRrFVVdd5XCiwKKqLF68mEaNGjFu3Din45hsjBkzhri4OKZPn+50FJ/LqRA0E5HjInICaOp+fFxETohI4JfIIDV58mSaNm1KfHw8oaGhjB8/PmCOkRYGv/32G1dffTU33XQTF1xwgbW3FGKXXXYZV155Ja+//np6D7qBKttCoKqhqlpGVUurahH347TnZQoypCk4LVq0oEOHDtZJnJcdOXKEoUOH0rx5czZs2MCUKVPYsGEDV111VbaF1gqw88aMGcO+ffv44IMPnI7iUx51Q12Y2FlD3pWcnMzw4cOpXr06Y8aMcTpOwElOTmb69OmMGzeOY8eOMXjwYJ599lm77sJPqCotW7YkMTGRLVu2EBLiyaVXhVN+O50zAaxIkSLExsZy9OhRp6MEnJUrV9KqVSuGDBlC06ZN2bBhA1OnTrUi4EfSuqjeunUry5YtczqOz1ghCEKHDx9m8ODB6VcFf/TRR0HZ0Zav7Nq1i759+3LllVcSFxfHwoUL+e6772ja1JFrME0+3XzzzdSqVYsXX3wxYK/rsEIQhA4fPsyHH37ITz/9BODXu7uFSUJCAk8//TQNGjTg888/55lnnuHPP//k5ptvDsj+7YNFkSJFGDVqFGvWrOHHH390Oo5vqKrPBqA7sBWIAsZm8f7DwBZgM/Af4OLc5tmqVSs1ebdv3z6dMWNG+vMjR444mCawpKam6oIFC7R69eoK6K233qq7du1yOpbxooSEBA0PD9cePXo4HeW8AZGazXbVZ18FRSQUmAr0wHW/49tFpFGm0TYArdXVb9EnuG6DaXxg0qRJPPTQQ8TExABw4YUXOpwoMGzcuJHOnTtz6623Ur58eX744Qfmz59PjRo1nI5mvKhEiRI89NBDfPnll2zeHHgdMPvymEAbIEpVd6jqGWA+0DvjCKr6vaomuJ+uBqzTdS/6559/2LZtGwDjxo1j06ZNXHTRRQ6nCgyHDh1i8ODBtGrVij/++INp06axbt06Onbs6HQ04yMPPvggpUqVCsguqn1ZCKoCezI8j3a/lp37gC+zekNEBolIpIhExsbGejFi4EpOTuaqq67iwQcfBKBUqVLUrVvX4VT+LykpicmTJ1O3bl3effddhg4dyt9//839999vN+QJcOXLl+f+++9n/vz57Ny50+k4XlUoWglFpD/QGnglq/dVdbqqtlbV1uHh4QUbzs/s3LkzvZO4WbNmMWvWLI+nrVy5MiJyzlC5cmUfJi58svscypcvT/PmzRk2bBitW7dm06ZNTJo0yQ6zBZERI0YQEhLCxIkTnY7iVb4sBHs5+yb31dyvnUVErgYeB3qpamBfx+1jv/76K/Xr1+ejjz4CoHPnzlSvXj2Xqf4nqz7xc3o9UGW3vkePHiUxMZElS5bwzTff0Lhx4wJOZpxWrVo17rjjDt59910OHTrkdByv8WUhWAvUFZFaIlIMuA0464oMEWkBvIOrCBz0YZaAlpiYCECrVq0YO3YsV199tcOJAteWLVvo06ePnQ4axEaPHk1iYiJvvvmm01G8xmeFQFWTgaHA18CfwMeq+oeIPCsivdyjvQJcACwUkY0iEriX7vnIpEmTuPTSSzl58iShoaE888wzVKpUyelYASssLMzpCMZhDRs2pFevXkyZMoX4+Hin43iFT9sIVPULVa2nqpeo6gT3a0+q6jL346tVNUJVm7uHXjnP0aRR9xWOrVu35qqrriI1NTVf81u+fLk3Yvm9HTt2OB3B+IExY8Zw5MgR3n33XaejeEWhaCw2nktOTubBBx/kpZdeAuCKK65g+vTplClzfh3Cnjp1ioceeojrr78+x/GGDRsW0P0RnT59mvHjx9txf+OR9u3b869//YuJEyeSlJTkdJx8s0LgZ4oUKcLRo0e9ctek33//ncsuu4w333yTESNGZNvtcYkSJZgyZQp169Zl2rRppKSk5HvZhcmKFSto2rQp48aNo2fPnmR3Zpp1C20yGjNmDLt3706/g59fy+6S48I6BGMXE7GxsTpw4EDdu3evqqqmpKTka36pqan65ptvavHixTUiIkK//PLLXKfZuHGjduzYUQFt1qyZ/vDDD/nKUBjs3btXb731VgW0Tp06+tVXXzkdyfiR1NRUbdKkiTZp0kRTU1OdjpMrcuhiwvENe16HYCwE27Zt0zJlyuiCBQvyPa8DBw7oddddp4Bee+21euDAAY+nzdynTt++fXXnzp35zlTQkpKS9I033tDSpUtr8eLF9ZlnntHExESnYxk/9P777yugn3/+udNRcmWFwA9FR0frO++8k/786NGj+Z7nV199pREREVq8eHGdPHnyeX+LiY+P16efflpLlCihYWFh+tRTT2l8fHy+8xWEn3/+WZs1a6aAdu/eXaOiopyOZPzYmTNntEaNGtqhQweno+TKCoEfGjt2rJYsWVL37duX73mdOnVKhw8froA2btxYN2/e7IWEqrt27dK+ffsqoDVq1NAFCxYU2l3kQ4cO6b///W8FtFq1arpo0aJCm9X4lzfeeEMBXbVqldNRcpRTIbBbVRYi27dvJykpiQYNGpCQkEBMTAyXXHJJvua5ZcsW+vXrx6ZNmxg6dCgvv/wyJUqU8FJilx9++IFhw4axadMmOnbsyKRJk2jevLlXl3G+UlNTmTVrFmPGjCEuLo4RI0bw5JNPcsEFFzgdzWuSkpKIjo7m1KlTTkcJSqmpqezdu5fixYsXimt4wsLCqFatGkWLFj3r9ZxuVen4N/y8DoG6R5CUlKQXX3yxdunSxSvzS01N1bffflvDwsK0YsWK+tlnn3llvtlJTk7WadOmaYUKFTQkJETvv/9+jY2N9ekyc7Nx40Zt166dAtqhQwf97bffHM3jKzt27NDY2Fjbw3HQ3r17de3atZqQkOBojtTUVI2NjdUdO3ac8x52aKjw2rFjR/o/8MqVK3XPnj35nmdsbKz27t1bAe3WrZvGxMTke56eOnLkiD700EMaGhqq5cqV00mTJumZM2cKbPmqqnFxcTp8+HANCQnR8PBwnT17dkBvJLds2RLQ6+cPzpw5o+vWrctyA1zQUlNTdcuWLee8boWgkFqzZo0WLVpU582b57V5fvvtt3rRRRdpsWLFdOLEifk+1fR8/f7773r11VcroI0aNdJvv/3W58tMO6upSpUqKiI6ePBgPXz4sM+X67Ss/ulNwdu1a5dGRkbq6dOnnY6S50JgF5Q5IGMncY8//jjdunXL9zzPnDnD6NGj6dq1K2XLlmXNmjXpXeY6oXHjxnzzzTcsWbKExMREunbtSp8+fXzWhcO2bdu45ppruPXWW6lcuTKrV6/m7bffpnz58j5ZnjGZRUREoKr+2VtvdhWisA7+vkfw+uuva+3atfXEiRNem+dff/2lLVu2VEAHDx5c6E7lTExM1Oeff15LlSqlxYoV00cffdRr65+QkKDjxo3TYsWKadmyZXXKlCmanJzslXn7i8KwRxASEqLNmjXTxo0b680335znv8FRo0Zpo0aNdNSoUXle9oQJE856XqpUqTzPw1NPPfWUvvLKK6qqOm7cuPQ93YsvvlhjY2N1+/btum7dOk1KSsrTfDds2KDLly/Pc55OnTrp2rVrz3ndDg0VUmnHcFetWqX333+/Hj9+3CvznDFjhpYsWVLLly+vS5Ysyfc8fSk6Olr79++vgFapUkXnzp2br2Pby5cv11q1aimg/fv3L9C2kMKkMBSCjBvffv366WuvvebRdGkbzDJlypx3Ac+84S+oQpBRWiGIj4/XtWvXpvcCkFFOxWHWrFk6ZMiQPOfxViGwQ0M+lpyczKBBg3jxxRcBV2dV06ZNo3Tp0vma75EjR7j55psZOHAgl19+OZs3b6ZPnz7eiOwzVatWZe7cufz8889UqVKFO++8kyuuuIK1a9fmaT67d+/mxhtv5LrrriMsLIzvv/+euXPnBt2d1LLTuXNnZs+eDbhOLe3cuTPz5s0DICEhgc6dO7NgwQIA4uLi6Ny5M4sXLwZc92Lu3Lkzn332GQD79+/P8/L/9a9/ERUVRXx8PPfeey9t2rShRYsWfPrppwDMnj2bXr16cdVVV9GlSxd69erFyZMnadWqFQsWLCA2NpabbrqJyy67jMsuu4xVq1YBcPLkSe655x4uvfRSmjZtyqJFixg7diyJiYk0b96cO+6446wcd911F0uXLk1/fscdd6RnyOill17i0ksvpVmzZowdOxaAGTNmcNlll9GsWTNuuukmEhISzpluwIABfPLJJ+nPX375Zdq2bcu9995LZGQkqampDBgwgMGDB9O2bVtGjx7Nr7/+Srt27WjRogXt27dn69atnDlzhieffJIFCxbQvHlzFixYkO1nl5iYyG233UbDhg254YYb0g8z51t2FaKwDv64R3D77bfr448/fl7TRkREKHDOEBISokWKFNGXX37ZsQbh/EhJSdGZM2emr98999yT6zf606dP60svvaQlS5bUkiVL6osvvlgoGuaclvnbX6dOnXTWrFmq6jqbpVOnTjp37lxVdV0V3qlTJ50/f76qqh47dkw7deqkixYtUlXXGWedOnXSZcuWqap6vJeV9i08KSlJe/XqpW+99ZY++uij6cs9evSo1q1bV0+ePKmzZs3SqlWrntWQn/Fb/O23364//fSTqroaYBs0aKCqqqNHj9Zhw4alj3fkyJFzps34fOXKldq7d+/09axZs+Y538q/+OILbdeuXfqhrLRMhw4dSh/n8ccf18mTJ6vq2XsEd999ty5cuFBVXXsE48ePV1XVadOmaYcOHfTAgQN6991363XXXZe+txMXF5ee4dtvv9Ubb7xRVc/dI8jus3vttdf0nnvuUVXVTZs2aWhoqB0aKqwOHjyo9957r0ZHR6uq5uvwR1ZFIG2IjIz0VmTHxMXF6ahRo7Ro0aJaunRpfeWVV7ItfqGhoQponz59/LKPI18pDIeG0toImjVrpkOHDtXTp09rq1attHHjxumvV69eXbds2aKzZs3SAQMGnDV9xo15eHh4+jTNmjXTKlWq6IkTJ7Rly5a6bdu2c5ad06GhRo0a6cGDB/Xtt9/WkSNHnjPtww8/rNOnTz/n9ZUrV2qHDh20SZMmWrNmTb3//vtVNedCsH37dlV1fWEpW7asbt68We+++26dPXt2+nx3796tffr00caNG2uTJk20fv36qnpuIcjus+vdu7f+5z//SR+vRYsWXikERbyzX2EyiouLY9GiRXTv3p1bbrnFZ7c1bNWqlU/mW5DKlCnDK6+8wsCBAxkxYgSPPPJItuOmpKTw2Wef5XrvBFPwSpQowcaNG896TVVZtGgR9evXP+v1NWvWUKpUqWznlZqayurVq71yN7i77rqLefPmMX/+fGbNmuXxdAMGDGDp0qU0a9aM2bNns3LlylynSfs/FxFCQ0M5ffo0p0+fPmtdx40bx5VXXsmSJUvYuXMnnTt3znJe2X12vmJtBF4SHR3NtGnTAKhTpw67du3illtu8Xj6o0eP8uuvvzJv3jyeeuopbr/99oDY0HuqXr16LF++PNc7pVkR8B/XXHMNb775puvQA7BhwwaPpuvWrdtZ9wNOKzBdu3Zl6tSp6a+n3SipaNGi2d4cZsCAAbzxxhsANGrU6Jz3u3btyqxZs9LbAI4cOQLAiRMnuOiii0hKSuKDDz7wKHdau8uCBQto3749YWFhJCYmpq8/uL4kVq1aFSC9HQegdOnSnDhxIv15dp9dx44d+fDDDwHX/UQ2b97sUbbcWCHwkqlTpzJy5EhiYmIAKFu27DnjxMfHs3HjRhYuXMiECRMYMGAA7du3Jzw8nPLly9O2bVvuvPNOnnvuOdasWUPFihULejUcd+211zodwXjJuHHjSEpKomnTpjRu3Jhx48Z5NN3kyZOJjIykadOmNGrUKP0L1hNPPMHRo0dp0qQJzZo14/vvvwdg0KBBNG3a9JzGYnCd29+wYUPuueeeLJfVvXt3evXqRevWrWnevDmvvvoqAM899xxt27bliiuuoEGDBh7lPnr0KE2bNmXSpEm8/vrrVK5cmeTk5LMadEePHs2jjz5KixYtSE5OTn/9yiuvZMuWLemNxdl9dg888AAnT56kYcOGPPnkk177smidzuVDVFQUycnJ6Z3E7d+/n6pVq7Jjxw7+/vtvtm3bdtbPvXv3njV9lSpVqFu3LvXq1aNevXrpj2vXrk3x4sUBcjys5G+/O08F4zrnx59//knDhg2djlEoJSQkcOmll7J+/fosv5z5UmpqKr/99hthYWEFdognTVZ/Ezl1OmdtBBlUrlw5y6sCIyIizjqNLiUlhe3bt/Ovf/2LcuXK0a1bt/QN/q5du866kXyFChWoV68eXbp0OWtjX6dOHY96wIyIiMg2kzEmeytWrOC+++5jxIgRBV4EAEJCQoiIiCA6Opr4+Pgc20WcZoUgg+wuDT9w4ACjRo3i77//5vfffyc6OpozZ84AcPDgQWJiYqhbty5t27alf//+6Rv8unXr5ruLg/M5j9vfWfEz3nD11Veza9cuRzOEh4cTExPD/v37892lvC9ZIfDQlClTqFatGv/88w/XXXcdffr0Sd/gR0RE+OzMoGAUjMXPBKbQ0FAqVapETEwMp06d8sqZUL5ghcBDaWcVPP/88zzwwANUqFDB4UTGGH9QqVIl9u/fz/79+6lZs6bTcbJkZw15KD4+npCQEJ544gkrAsYYjxUtWpSKFSty+PDh9EPKhY0VAg/ZoR9jzPlK66L64MGDTkfJkhWCDLJrjIyIiAioe9ya4FW5cmVE5Jwhvx32ZfX/8fTTT1O1alWaN29Oo0aN+Oijj/K1DH8WFhbGhRdeSGxs7FnXDxQWVggy2L9/P6rKHXfcwbhx49L74bDGSxMocjozzhdGjBjBxo0b+fTTT7n//vuzvQI4GFx00UWkpKQQGxvrdJRzWGMxrlNAx4wZw/jx49O7SrZDQcYfDR8+/Jw+fzyVXb83zZs3T++m4XzVrVuXkiVLcvToUSpVqpSvefmrkiVLUqZMGQ4ePEhERIRjdw/MSuFJ4qDjx4+zdOlSVq9eDVh7gDHetn79eurWrRu0RSBN5cqVSUpK4vDhw05HOUvQ7hHs3r2bzz77jCFDhlCnTh12796d75vFGOO03L655/Qlx5MeNvPq9ddfZ9asWWzbti39ZjfBrHTp0pQsWZL9+/dTsWLFQvOl06d7BCLSXUS2ikiUiIzN4v3iIrLA/f4aEanpyzwZvfPOO4wdOzb9+L8VAWO8b8SIEfzxxx8sWrSI++67j1OnTjkdyVEikt499bp164iMjEwfNm3a5FgunxUCEQkFpgI9gEbA7SKSuR/Y+4CjqloHeB14yVd5ALZu3cqWLVsAV0+Gv/32m93e0ASVnM6M86W0Hj7nzJnj0+X4g5SUlCxfd7Ih3Zd7BG2AKFXdoapngPlA70zj9AbS/jI+AbqIj/aVkpOTueaaa3jooYcA1400CutVfsb4StqZcZmH/J4Zl5CQQLVq1dKHiRMnnjPOk08+ycSJE8/qlNEUDr5sI6gK7MnwPBpom904qposInFABeBQxpFEZBAwCKBGjRrnFaZIkSLMmzevUHf8ZIy/8mTj3qpVK7Zu3VoAaUxe+cVZQ6o6XVVbq2rr8PDw855Phw4duOiii7yYzBhj/J8vC8FeoHqG59Xcr2U5jogUAcoCheu8KmOMCXC+LARrgboiUktEigG3AcsyjbMMuNv9+GbgO7VbUBmTZ/Zv4z+KFi2ap9fz6nz+FnzWRuA+5j8U+BoIBWaq6h8i8iwQqarLgPeAuSISBRzBVSyMMXkQFhbG4cOHqVChQqE5L91kr1mzZj6bt6py+PDhPN/3wO5ZbIyfS0pKIjo6OujP0TcuYWFhVKtW7Zw9DLtnsTEBrGjRotSqVcvpGMaP+cVZQ8YYY3zHCoExxgQ5KwTGGBPk/K6xWERigV3nOXlFMl21HARsnYODrXNwyM86X6yqWV6R63eFID9EJDK7VvNAZescHGydg4Ov1tkODRljTJCzQmCMMUEu2ArBdKcDOMDWOTjYOgcHn6xzULURGGOMOVew7REYY4zJxAqBMcYEuYAsBCLSXUS2ikiUiIzN4v3iIrLA/f4aEalZ8Cm9y4N1flhEtojIZhH5j4hc7EROb8ptnTOMd5OIqIj4/amGnqyziPR1/67/EJEPCzqjt3nwt11DRL4XkQ3uv+9rncjpxqGc1wAAB2ZJREFULSIyU0QOisjv2bwvIjLZ/XlsFpGW+V5oVvcv9ecBV5fX24HaQDFgE9Ao0zgPAtPcj28DFjiduwDW+UqgpPvxA8Gwzu7xSgM/AquB1k7nLoDfc11gA3Ch+3klp3MXwDpPBx5wP24E7HQ6dz7XuSPQEvg9m/evBb4EBLgcWJPfZQbiHkEbIEpVd6jqGWA+0DvTOL2BOe7HnwBdxL87cs91nVX1e1VNcD9djeuOcf7Mk98zwHPAS0Ag9NHsyToPBKaq6lEAVT1YwBm9zZN1VqCM+3FZYF8B5vM6Vf0R1/1ZstMbeF9dVgPlRCRf9+ANxEJQFdiT4Xm0+7Usx1HVZCAOqFAg6XzDk3XO6D5c3yj8Wa7r7N5lrq6qywsymA958nuuB9QTkVUislpEuhdYOt/wZJ2fBvqLSDTwBfB/BRPNMXn9f8+V3Y8gyIhIf6A10MnpLL4kIiHARGCAw1EKWhFch4c649rr+1FELlXVY46m8q3bgdmq+pqItMN118MmqprqdDB/EYh7BHuB6hmeV3O/luU4IlIE1+7k4QJJ5xuerDMicjXwONBLVU8XUDZfyW2dSwNNgJUishPXsdRlft5g7MnvORpYpqpJqvoPsA1XYfBXnqzzfcDHAKr6CxCGq3O2QOXR/3teBGIhWAvUFZFaIlIMV2PwskzjLAPudj++GfhO3a0wfirXdRaRFsA7uIqAvx83hlzWWVXjVLWiqtZU1Zq42kV6qao/3+fUk7/tpbj2BhCRirgOFe0oyJBe5sk67wa6AIhIQ1yFILZAUxasZcBd7rOHLgfiVDUmPzMMuENDqposIkOBr3GdcTBTVf8QkWeBSFVdBryHa/cxClejzG3OJc4/D9f5FeACYKG7XXy3qvZyLHQ+ebjOAcXDdf4a6CYiW4AU4BFV9du9XQ/XeSQwQ0RG4Go4HuDPX+xE5CNcxbyiu93jKaAogKpOw9UOci0QBSQA9+R7mX78eRljjPGCQDw0ZIwxJg+sEBhjTJCzQmCMMUHOCoExxgQ5KwTGGBPkrBAYR7h7A52X4XkREYkVkc+dzJVXIrLTfb4+IvJzLuMOEJEqeZx/zex6oXRiPiYwWSEwTokHmohICffzruTz6khvcV9tnmeq2j6XUQYAeSoExhQEKwTGSV8A17kf3w58lPaGiJRy98v+q7uf+d7u12uKyE8ist49tHe/3llEVorIJyLyl4h8kFWPsu5xJonIRhH5//bOLsSqKorjv/+IOuOEjR/zWPRgJQkmJGKoQw9iiRBkgphG+hZEgSD4JkogRr0okwhFKlIa4YtMpDM9mDQJ+TUzfuFTPUgiPZhk6EOyeljr0JnLmdsowTD3rB9s7t7r7H32x73c/XE4/3VF0pKw75R0RNIg/rJht6Tjks5FWBb55kjqD63/z3Ep4OLe90rx7ZIuSxqWtEfSOlzj6cuou0PSS5J+kHRB0qlCQTLsw5KGgfeqBk7SMUlrSulDktaNNT4NZTdL6i2l+yS9EvFVks5G2W8kPdH0G0xag4nW3s5QzwDcAxbiMuDtwBD+NmVfXN8NbIp4F66Z0wnMANrD/iz+dilR9i6uu9IGnAWWV9R7Gvgs4j2E5juuYHkB6Ij0V0V54GngesT3ATsivgZ/k3Vu0af4XA38xL/+H2aX6l4c8amRpzvS6/G3ZgFGgJ6If0yFLj3wBnA44tNwNcqOJuPzTKmvm4He0r36Yvzm4r4bOsO+vehrhtYOLScxkUwezGxE7h1uA747KLMKeF3Stki343/IvwG9khbhEgrPlcr8bGY3ASQN4X9+P1ZUfTTqPyNppqSusJ8ws/sRXwm8UNpUzIzVcQ+wNsp/K+lOxf1XAgct/D+YWZW2/PO4KN5A1DEFuBVt6TLXpAc4gk8sjXwH7JU0HXgNOGNm9yU92WR8/ouluGOXwWjTNHxCTVqcnAiSieYE8Am+Ii37hBDwppndKGeWtBO4DbyIr/zLDmfKiqoPGfv33airUqT/KtnagKVmNsqhTcVp0+Mi4KqZvdxw/64x8o/CzB5IOg28iu8mjsWlrYw9PgV/M/pYuL3UpgEz2zDOPiQtQj4jSCaaL4BdZna5wX4KeL8455erp4JLht8y15p/G19JPyrr457LceXGuxV5+ik5OIkVNvjRyVthWw3Mqig7AGyRNCPyzQ77n7g8NsANoFuun4+kqZIWmPsN+CPaBrCxST++xgXHVgAnwzae8fkVWCSpTdJTuBcwcIXWZZLmRZs6JT3KjiKZpOREkEwoZnbTzPZVXPoQP0cfkXQ10gD7gXfiQep8Rq/ix8sDSZeAA7iWfRUfAIvlzsGvAe+GfRfQE21ai0sgN/bpJL7TOR9HVMXx1iHgQNim4BLoH0VfhoDiwe4W4NPI12wL0o87GPre3I0jjG98BoFfgGv4M4+L0e7f8ecHRyWN4MdC85vUn7QIqT6a1Io4Ttlmk9svQZL8r+SOIEmSpObkjiBJkqTm5I4gSZKk5uREkCRJUnNyIkiSJKk5OREkSZLUnJwIkiRJas4/GvF+hReTzm4AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "thresholds:  {'mean': 0.365145, 'roc': 0.446917, 'youden': 0.448707, 'none': 0.5, 'tuned': 0.343}\n",
            "threshold used:  0.343\n",
            "mean of thresholded prediction:  0.7536\n",
            "median of thresholded prediction:  1.0\n",
            "percentage kept:  0.24639999999999995\n",
            "cost matrix [[tn, fp], [fn, tp]]:  [[0. 1.]\n",
            " [2. 0.]]\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 1, gen1\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.3873160290680114\n",
            "mean of thresholded prediction:  0.7096774193548387\n",
            "percentage of applicants kept (cutoff based on threshold):  0.29032258064516125\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  57\n",
            "length of gen_score_reject_idx: 160\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.029\n",
            "no. of AL-accepted instances: 6\n",
            "al_idx[temp_gen_AL_select_idx] [1922 1030  980  942 1876  937]\n",
            "gen_AL_select_idx2 [937, 942, 980, 1030, 1876, 1922]\n",
            "len gen_AL_select_idx 6\n",
            "len accept_idx before round update 298\n",
            "len accept_idx after round update 304\n",
            "len gen_score_reject_idx after round update 154\n",
            "mean of gen 1 TEST prediction: 0.3998067844791331\n",
            "gen 1/10 | # applicants = 217 | score accepts = 57 | AL selects = 6 | total accepts = 304 | unlabelled = 2040 |  time = 0.07 min\n",
            "---------------------------------------------------\n",
            "dataset bene1_nobins, strategy spal, AL ratio 0.1, cost matrix [[0. 1.]\n",
            " [2. 0.]]: starting round 1, gen2\n",
            "len applicants_idx: 217\n",
            "mean applicant pred col 1 (bad) 0.3992350042811811\n",
            "mean of thresholded prediction:  0.7327188940092166\n",
            "percentage of applicants kept (cutoff based on threshold):  0.26728110599078336\n",
            "threshold used:  0.343\n",
            "number of score-accepted applicants:  52\n",
            "length of gen_score_reject_idx: 165\n",
            "model key:  spal\n",
            "AL_accept_cutoff 0.027\n",
            "no. of AL-accepted instances: 5\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-e7799497a4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m                                         \u001b[0mAL_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mAL_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                                         \u001b[0mCLF_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCLF_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                                         **LOOP_params)\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m           \u001b[0mAL_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-885c6fe38264>\u001b[0m in \u001b[0;36mAL_loop\u001b[0;34m(data, y, kf_indices, key, AL_models, rounds, classifier, iteration, max_gens, sample_size, init_sample, score_acc_rate, AL_acc_rate, seed, weights, w_factor, do_thres, tuned_threshold, AL_params, CLF_params, cost_matrix, dataset_name)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m#print('al_idx[temp_unlabel_idx]', al_idx[temp_unlabel_idx])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m         \u001b[0mtemp_gen_AL_select_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_accept_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munlabel_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtemp_unlabel_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_instances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqp_solver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'OSQP'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;31m#print('temp_gen_AL_select_idx',temp_gen_AL_select_idx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'al_idx[temp_gen_AL_select_idx]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mal_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp_gen_AL_select_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/alipy/query_strategy/query_labels.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, label_index, unlabel_index, batch_size, qp_solver, **kwargs)\u001b[0m\n\u001b[1;32m   1506\u001b[0m                 \u001b[0;31m# The optimal objective value is returned by `prob.solve()`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1507\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1508\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOSQP\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mqp_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'OSQP'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mECOS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1509\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDCPError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOSQP\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mqp_solver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'OSQP'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mECOS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0msolve_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mProblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_solve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msolve_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cvxpy/problems/problem.py\u001b[0m in \u001b[0;36m_solve\u001b[0;34m(self, solver, warm_start, verbose, parallel, gp, qcp, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_chains\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         data, solving_inverse_data = self._solving_chain.apply(\n\u001b[0;32m--> 570\u001b[0;31m             self._intermediate_problem)\n\u001b[0m\u001b[1;32m    571\u001b[0m         solution = self._solving_chain.solve_via_data(\n\u001b[1;32m    572\u001b[0m             self, data, warm_start, verbose, kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cvxpy/reductions/chain.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, problem)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0minverse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreductions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0minverse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minverse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cvxpy/reductions/matrix_stuffing.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, problem)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;31m# Form the constraints\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoeffExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mnew_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstuffed_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0minverse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# Lower equality and inequality to Zero and NonPos.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cvxpy/reductions/qp2quad_form/qp_matrix_stuffing.py\u001b[0m in \u001b[0;36mstuffed_objective\u001b[0;34m(self, problem, extractor)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;31m# extract to x.T * P * x + q.T * x + r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mexpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquad_form\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# concatenate all variables in one vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cvxpy/utilities/coeff_extractor.py\u001b[0m in \u001b[0;36mquad_form\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0;31m# the coefficients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         coeffs, constant = self.extract_quadratic_coeffs(root.args[0],\n\u001b[0;32m--> 145\u001b[0;31m                                                          quad_forms)\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Restore expression.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mrestore_quad_forms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquad_forms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cvxpy/utilities/coeff_extractor.py\u001b[0m in \u001b[0;36mextract_quadratic_coeffs\u001b[0;34m(self, affine_expr, quad_forms)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0maffine_var_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maffine_inverse_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mextractor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoeffExtractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffine_inverse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffine_problem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# Combine affine data with quadforms.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/cvxpy/utilities/coeff_extractor.py\u001b[0m in \u001b[0;36maffine\u001b[0;34m(self, expr)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mop_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanonical_form\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexpr_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcanonInterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_problem_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mJ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0;31m# (data, ij) format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m                     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcoo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcoo_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m                     \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoo_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0marg1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_self\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36masformat\u001b[0;34m(self, format, copy)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0;31m# Forward the copy kwarg, if it's accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/coo.py\u001b[0m in \u001b[0;36mtocsr\u001b[0;34m(self, copy)\u001b[0m\n\u001b[1;32m    409\u001b[0m                       indptr, indices, data)\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_canonical_format\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_check\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/scipy/sparse/compressed.py\u001b[0m in \u001b[0;36mcheck_format\u001b[0;34m(self, full_check)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \"\"\"\n\u001b[1;32m    147\u001b[0m         \u001b[0;31m# use _swap to determine proper bounds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m         \u001b[0mmajor_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'row'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'column'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mmajor_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminor_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "############################################\n",
        "# RUN LOOP THAT ITERATES OVER ALL AL MODELS\n",
        "############################################\n",
        "\n",
        "#### SET UP MODEL\n",
        "classifier=LogisticRegression(solver=\"liblinear\", penalty=\"l1\", random_state=seed) \n",
        "\n",
        "AL_models = dict(\n",
        "    oracle=['Oracle', 'Oracle'],\n",
        "    score=['Score','Score'],\n",
        "    random=['Random','QueryInstanceRandom'],\n",
        "    \n",
        "    unc=['Uncertainty','QueryInstanceUncertainty'],\n",
        "    qbc=['QBC','QueryInstanceQBC'],\n",
        "    dw=['Dens.Weigh.', 'QueryInstanceDensityWeighted'],\n",
        "    cors=['Coreset','QueryInstanceCoresetGreedy'],\n",
        "\n",
        "    density=['Density','QueryInstanceGraphDensity'],\n",
        "    eer = ['EER', 'QueryExpectedErrorReduction'],\n",
        "    lal= ['LAL','QueryInstanceLAL'],\n",
        "    quire=['QUIRE','QueryInstanceQUIRE'], ### slow, no batch mode, samples need to be picked one by one with a for-loop\n",
        "    bmdr=['BMDR','QueryInstanceBMDR'], #only if cvxpy installed\n",
        "    spal=['SPAL','QueryInstanceSPAL'] #only if cvxpy installed\n",
        "    )\n",
        "\n",
        "AL_models = dict(\n",
        "    #bmdr=['BMDR','QueryInstanceBMDR'], #only if cvxpy installed\n",
        "    spal=['SPAL','QueryInstanceSPAL'] #only if cvxpy installed\n",
        "    )\n",
        "\n",
        "#store different filenames after each run, making loading of results easier\n",
        "filename_list=[]\n",
        "\n",
        "######################################\n",
        "# run loop for all datasets, AL-score ratios and models; save results, create averages for all metrics (generation-wise averages over all rounds)\n",
        "######################################\n",
        "\n",
        "for dataset in dataset_list:\n",
        "\n",
        "  X,y = data_loader(dataset)\n",
        "\n",
        "  # intialize result dictionaries\n",
        "  AL_stats={}\n",
        "  AL_indices={}\n",
        "  AL_cost={}\n",
        "\n",
        "  ######################################\n",
        "  # generate Kfold data splits\n",
        "  # this generates the data for each round!\n",
        "  # note: test set is different in each round!\n",
        "\n",
        "  skf = StratifiedKFold(n_splits=rounds, shuffle = True, random_state=seed)\n",
        "  idx = []\n",
        "  for train_index, test_index in skf.split(X, y):\n",
        "    idx.append((train_index, test_index))\n",
        "\n",
        "\n",
        "  # load tuned parameters, check that parameters for all models are set\n",
        "  param_dict = param_getter(tuned=tuned, dataset = dataset)\n",
        "  try:\n",
        "    assert len(param_dict)==13\n",
        "  except AssertionError:\n",
        "    print(\"parameters incomplete\")\n",
        "\n",
        "  # for each dataset, loop over differen AL-score ratios\n",
        "  for cost_mat in cost_mat_list:\n",
        "\n",
        "    #filename to load tuned thresholds based on cost matrix\n",
        "    fname = 'tuned_thresholds'\n",
        "    if cost_mat is not None:\n",
        "      #augment filename to load the correct file\n",
        "      fname += f\"_cost-{cost_mat[0][1]}-{cost_mat[1][0]}\"\n",
        "\n",
        "    infile = open(fname,'rb')\n",
        "    tuned_thresholds = pickle.load(infile)\n",
        "    infile.close()\n",
        "    print('tuned thresholds: ',tuned_thresholds)\n",
        "    \n",
        "    for r_AL in ratios_list:\n",
        "\n",
        "      LOOP_params = loop_param_setter(X, total_rounds = rounds, AL_acc_rate = r_AL, cost_mat=cost_mat, dataset=dataset, weights=wghts)\n",
        "\n",
        "      ### Build filename for saving results\n",
        "      filename = \"{}_\".format(dataset)\n",
        "\n",
        "      for key in LOOP_params:\n",
        "        if key not in [\"init_sample\", \"sample_size\", \"score_acc_rate\", \"w_factor\", \"tuned_threshold\", \"dataset_name\"]:\n",
        "          #if LOOP_params[key] == False:\n",
        "          #  filename += f\"_{key}-F\"\n",
        "          #elif LOOP_params[key] == True:\n",
        "          #  filename += f\"_{key}-T\"\n",
        "          #else:\n",
        "          filename += f\"_{key}-{LOOP_params[key]}\"\n",
        "\n",
        "      filename = filename.replace('.', '-')\n",
        "      filename = filename.replace('[', '')\n",
        "      filename = filename.replace(']', '')\n",
        "      filename = filename.replace(',', '-')\n",
        "      filename = filename.replace(' ', '')\n",
        "      print(filename)\n",
        "\n",
        "      print(f'\\n----------- STARTING LOOP FOR DATASET {dataset} ------------')\n",
        "      start = time.time()\n",
        "      starttime = datetime.datetime.now()\n",
        "      print(\"start time: \", starttime)\n",
        "\n",
        "      for key in AL_models:\n",
        "        AL_stats[\"{}\".format(key)] = {}\n",
        "        AL_indices[\"{}\".format(key)] = {}\n",
        "        AL_cost[\"{}\".format(key)] = {}\n",
        "\n",
        "        AL_params = param_dict[key]['AL']\n",
        "        AL_params.pop('skip', None) #Baseline models oracle, random & score skip the sampler-step in the CV loop and only tune the clf. skip parameter is not needed here\n",
        "        AL_params.pop('with_scaling', None) #Baseline models oracle, random & score skip the sampler-step in the CV loop and only tune the clf. skip parameter is not needed here\n",
        "        CLF_params=param_dict[key]['CLF']\n",
        "      \n",
        "        print(AL_params)\n",
        "        \n",
        "        for rnd in range(rounds):\n",
        "          stats, indices, cost = AL_loop(data=X, \n",
        "                                        y=y, \n",
        "                                        kf_indices=idx, \n",
        "                                        key = key,\n",
        "                                        seed=seed, \n",
        "                                        classifier = classifier, \n",
        "                                        AL_models=AL_models, \n",
        "                                        iteration = rnd, \n",
        "                                        AL_params=AL_params, \n",
        "                                        CLF_params=CLF_params, \n",
        "                                        **LOOP_params)\n",
        "\n",
        "          AL_stats[\"{}\".format(key)][rnd] = copy.deepcopy(stats)\n",
        "          AL_cost[\"{}\".format(key)][rnd] = copy.deepcopy(cost)\n",
        "          AL_indices[\"{}\".format(key)][rnd] = copy.deepcopy(indices)\n",
        "          \n",
        "        AL_stats[\"{}\".format(key)]['average'] = pd.concat(AL_stats[\"{}\".format(key)]).groupby(level=1).mean()\n",
        "        AL_cost[\"{}\".format(key)]['average'] = pd.concat(AL_cost[\"{}\".format(key)]).groupby(level=1).mean()\n",
        "\n",
        "        with open(f'model_results/{dataset}/{filename}', 'wb') as a_file:\n",
        "          pickle.dump(AL_stats, a_file)\n",
        "\n",
        "        with open(f'model_indices/{dataset}/{filename}', 'wb') as b_file:\n",
        "          pickle.dump(AL_indices, b_file)\n",
        "        \n",
        "        with open(f'model_cost/{dataset}/{filename}', 'wb') as c_file:\n",
        "          pickle.dump(AL_cost, c_file)\n",
        "\n",
        "      a_file.close()\n",
        "      b_file.close()\n",
        "      c_file.close()\n",
        "\n",
        "      # after finishing store filenames for later use when loading data\n",
        "      filename_list.append(filename)\n",
        "      with open(f'{server}_filename_collection', 'wb') as filename_collection:\n",
        "        pickle.dump(filename_list, filename_collection)\n",
        "      filename_collection.close()\n",
        "\n",
        "\n",
        "      ######################################\n",
        "      # print and save results\n",
        "      ######################################\n",
        "\n",
        "      print(' \\n \\n ################################## \\n ------------- RESULTS ------------- \\n ################################## \\n \\n')\n",
        "      for key in AL_models:\n",
        "        print(\"\\n \\n dataset {}: average results for strategy {}: \\n \\n\".format(dataset, AL_models[key][0]), (AL_stats[\"{}\".format(key)][\"average\"]), \"\\n \\n \\n\")\n",
        "\n",
        "      print(filename)\n",
        "      stats_plotter(AL_stats)\n",
        "      plt.savefig(f'result_images/{dataset}/STATS_{filename}.png', bbox_inches='tight')\n",
        "\n",
        "      print(' \\n \\n ################################## \\n ------------- COST ------------- \\n ################################## \\n \\n')\n",
        "      for key in AL_models:\n",
        "        print(\"\\n \\n dataset {}: average cost for strategy {}: \\n \\n\".format(dataset, AL_models[key][0]), (AL_cost[\"{}\".format(key)][\"average\"]), \"\\n \\n \\n\")\n",
        "\n",
        "      cost_metrics = [\"external_cost\", \"external_cpl\", \"total_internal_cpl\", \"gen_internal_cost\", \"gen_internal_cpl\", \"model_internal_cpl\"] #\"total_internal_cost\",\"model_internal_cost\",\n",
        "      print(filename)\n",
        "      cost_plotter(AL_cost, cost_metrics)\n",
        "      plt.savefig(f'result_images/{dataset}/COST_{filename}.png', bbox_inches='tight')\n",
        "\n",
        "      end = time.time()\n",
        "      endtime = datetime.datetime.now()\n",
        "      print(f\"start at {starttime}, end at {endtime}\")\n",
        "      print('total time (hours): ', (start-end)/360)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_a = np.array([1,2,3,4,5,6,7])\n",
        "list_b = np.array([3,4,6])\n",
        "list_b\n",
        "pos = np.where(np.in1d(list_a, list_b, assume_unique=True, invert=False))\n",
        "#print(type(pos))\n",
        "#print(pos)\n",
        "print(list_a[np.in1d(list_a, list_b, assume_unique=True, invert=False)])\n",
        "#print(list_b)\n",
        "np.in1d(list_a, list_b, assume_unique=True, invert=False)\n",
        "np.where(np.in1d(list_a, list_b, assume_unique=True, invert=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "qqSsY2iUB7h4",
        "outputId": "f4d2a266-1eda-4a9e-ce08-8025fd732422"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-df1a371507e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlist_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlist_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mlist_b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#print(type(pos))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list_a = np.array([10,22,33,44,5,66,77,88,99]) #data\n",
        "list_b = np.array([3,0,6,2]) #accept_idx\n",
        "list_c = np.array([4,8,7]) #unlabel_idx\n",
        "\n",
        "list_d = np.concatenate((list_b, list_c)) #al_idx\n",
        "subset = list_a[list_d] #training data for al model\n",
        "\n",
        "\n",
        "indices_temp_b = list(range(len(list_b))) # subset-indices of accepted data\n",
        "indices_temp_c = [x + len(list_b) for x in list(range(len(list_c)))] #subset indices of unlabeled data\n",
        "\n",
        "indices_temp_select = np.array([4,6]) #selected positions\n",
        "\n",
        "\n",
        "print('data',list_a)\n",
        "print('accepted data instances:', list_a[list_b])\n",
        "print('unlabeled data instances:', list_a[list_c])\n",
        "print('combined indices as input to AL',list_d)\n",
        "print('subset of data as input to AL',subset)\n",
        "\n",
        "print('indices_temp_b: indices of accepts in subset',indices_temp_b) #indices of accepts in subset\n",
        "print(\"indices_temp_c: indices of unlabeled in subset\", indices_temp_c) #indices of unlabeled in subset\n",
        "print(\"indices_temp_select\", indices_temp_select) #indices of unlabeled in subset\n",
        "\n",
        "print('compare data at selected indices in subset :', subset[indices_temp_select])\n",
        "print('to data at selected indices of indices in full data:', list_a[list_d][indices_temp_select])\n",
        "\n",
        "# end-product: subset of unlabeled index referencing whole dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGKzi2p-4nFG",
        "outputId": "9373ce79-7c1e-453e-eafb-5416b72c460d"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data [10 22 33 44  5 66 77 88 99]\n",
            "accepted data instances: [44 10 77 33]\n",
            "unlabeled data instances: [ 5 99 88]\n",
            "combined indices as input to AL [3 0 6 2 4 8 7]\n",
            "subset of data as input to AL [44 10 77 33  5 99 88]\n",
            "indices_temp_b: indices of accepts in subset [0, 1, 2, 3]\n",
            "indices_temp_c: indices of unlabeled in subset [4, 5, 6]\n",
            "indices_temp_select [4 6]\n",
            "compare data at selected indices in subset : [ 5 88]\n",
            "to data at selected indices of indices in full data: [ 5 88]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "elif key in ['bmdr', 'spal']:\n",
        "  # select instances for this gen and AL Model\n",
        "  al_idx=np.concatenate((np.asarray(accept_idx.index), np.asarray(gen_score_reject_idx.index)))\n",
        "  strategy = strategy_getter(X_t[al_idx], y[al_idx], **AL_params)\n",
        "  \n",
        "  gen_AL_select_idx = strategy.select(label_index=accept_idx, unlabel_index=gen_score_reject_idx, batch_size=n_instances, model=clf, qp_solver = 'OSQP') \n",
        "  gen_AL_select_idx = IndexCollection(gen_AL_select_idx)\n"
      ],
      "metadata": {
        "id": "3ut4YBQ-5kCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwqTUDL-NIPY"
      },
      "outputs": [],
      "source": [
        "'''#bene1_nobins_tuned-True_rounds-5_max_gens-10_init_sample-241_sample_size-217_score_acc_rate-0.7_AL_acc_rate-0.3_weights-F_w_factor-2.3333333333333335_do_thres-tuned_tuned_threshold-0.343\n",
        "\n",
        "cost_metrics = [\"external_cost\", \"external_cpl\", \"total_internal_cpl\", \"gen_internal_cost\", \"gen_internal_cpl\", \"model_internal_cpl\"]\n",
        "cost_plotter(AL_cost, cost_metrics)\n",
        "#plt.savefig(f'result_images/{dataset}/COST_{filename}.png', bbox_inches='tight')'''"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "eP-kbRQHWTbB"
      ],
      "name": "AL_CreditScoring.ipynb",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}